# 强化学习

# 一、强化学习核心概念

![../_images/rl_diagram_transparent_bg.png](https://spinningup.readthedocs.io/zh_CN/latest/_images/rl_diagram_transparent_bg.png)

智能体和环境的循环作用

强化学习的主要角色是 **智能体** 和 **环境**,环境是智能体存在和互动的世界。智能体在每一步的交互中，都会获得对于所处环境状态的观察（有可能只是一部分），然后决定下一步要执行的动作。环境会因为智能体对它的动作而改变，也可能自己改变。

智能体也会从环境中感知到 **奖励** 信号，一个表明当前状态好坏的数字。智能体的目标是最大化累计奖励，也就是 **回报**。强化学习就是智能体通过学习来完成目标的方法。

基本术语：

- 状态和观察(states and observations)
- 动作空间(action spaces)
- 策略(policies)
- 行动轨迹(trajectories)
- 不同的回报公式(formulations of return)
- 强化学习优化问题(the RL optimization problem)
- 值函数(value functions)

## 强化学习建模

### **基本元素**

- 环境状态 $S$ ,$t$ 时刻环境的状态$S_t$是它的环境状态集中某一个状态
- 个体动作$A$, $t$ 时刻个体采取的动作$A_t$是它的动作集中某一个动作
- 环境的奖励$R$,$t$ 时刻个体在状态$S_t$采取的动作$A_t$对应的奖励$R_{t+1}$会在$t+1$时刻得到。
- 个体策略 $π$,  $π(a|s)=P(A_t=a|S_t=s)$ ,在状态$S$时采取动作$a$的概率
- 个体在策略$π$和状态$S$时，采取行动后的价值（value）$v_π(s)$ 

​                                                                                    $v_π(s)=E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s)$

- $γ$是即奖励衰减因子，在[0，1]

- 状态转化模型   $ P^a_{ss′}$,   在状态$s$下采取动作$a$,转到下一个状态$s′$的概率

### 环境转换模型

假设转化到下一个状态$s′$的概率仅与上一个状态$s$有关，与之前的状态无关。用公式表示就是：

​																						$P^a_{ss′}=E(S_{t+1=s′}|S_t=s,A_t=a)$

假设在状态$s$时采取动作$a$的概率仅与当前状态$s$有关，与其他的要素无关。用公式表示就是:

​																							$π(a|s)=P(A_t=a|S_t=s)$



### 价值函数

$v_π(s)$： 状态价值函数

​																						$v_π(s)=E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s)$

$q_π(s,a)$:   动作价值函数

​														$q_π(s,a)=E_π(G_t|S_t=s,A_t=a)=E_π(R_{t+1}+γR_{t+2}+γ^2R_{t+3}+...|S_t=s,A_t=a)$

$v_π(s)$ 与 $q_π(s,a)$ 之间的转换关系：



![image-20221111153105853](C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221111153105853.png)

> ==状态价值和动作价值的解释==
>
> 状态价值函数是在一个确定的策略$π$下，在当前状态下采取行动后的价值，不考虑采取哪个确定的动作，是在此策略下此状态下所有动作产生的价值期望
>
> 动作价值函数是在一个确定的策略$π$下，在当前状态下采取某个确定的动作后的价值，由在当前状态下采取某个确定的动作后会得到一个价值即$R^a_s$, 此时状态发生改变，状态会根据环境状态转移策略转换为不同的状态，将转换后所有的状态价值求期望再加上$R^a_s$就得到了动作价值

### 最优价值函数：

寻找较优策略可以通过寻找较优的价值函数来完成。可以定义最优状态价值函数是所有策略下产生的众多状态价值函数中的最大者，即：

​																									$v∗(s)=\mathop{max}\limits_{π}v_π(s)$

同理也可以定义最优动作价值函数是所有策略下产生的众多动作状态价值函数中的最大者，即：

​																							  	$q_∗(s,a)=\mathop{max}\limits_{π}qπ(s,a)$

只要我们找到了最大的状态价值函数或者动作价值函数，那么对应的策略$π^∗$就是我们强化学习问题的解。同时，利用状态价值函数和动作价值函数之间的关系，我们也可以得到:

​																							  	$v_∗(s)=\mathop{max}\limits_{a}q∗(s,a)$

反过来的最优价值函数关系也很容易得到：

​																								$q_∗(s,a)=R^a_s+γ\sum\limits_{s′∈S}P^a_{ss′}v_∗(s′)$

## 状态和观察

一个 **状态** ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) 是一个关于这个世界状态的完整描述。这个世界除了状态以外没有别的信息。**观察** ![o](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg) 是对于一个状态的部分描述，可能会漏掉一些信息。

在深度强化学习中，我们一般用 [实数向量、矩阵或者更高阶的张量（tensor）](https://en.wikipedia.org/wiki/Real_coordinate_space) 表示状态和观察。比如说，视觉上的 **观察** 可以用RGB矩阵的方式表示其像素值；机器人的 **状态** 可以通过关节角度和速度来表示。

如果智能体观察到环境的全部状态，我们通常说环境是被 **全面观察** 的。如果智能体只能观察到一部分，我们称之为 **部分观察**。

> 强化学习有时候用这个符号 ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) 代表状态 , 有些地方也会写作观察符号 ![o](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg). 尤其是，当智能体在决定采取什么动作的时候，符号上的表示按理动作是基于状态的，但实际上，动作是基于观察的，因为智能体并不能知道状态（只能通过观察了解状态)。

## 动作空间

不同的环境有不同的动作。所有有效动作的集合称之为 **动作空间**。有些环境，比如说 Atari 游戏和围棋，属于 **离散动作空间**，这种情况下智能体只能采取有限的动作。其他的一些环境，比如智能体在物理世界中控制机器人，属于 **连续动作空间**。在连续动作空间中，动作是实数向量。

这种区别对于深度强化学习来说，影响深远。有些种类的算法只能直接用在某些案例上，如果需要用在别的地方，可能就需要大量重写代码。

## 马尔可夫决策

**马尔科夫决策过程** (Markov Decision Processes, MDPs)。MDP是一个5元组 ![\langle S, A, R, P, \rho_0 \rangle](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/8856e84a582f7587b47f6cc8c7846da6994492e9.svg)，其中

- ![S](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/54f67ffbc0534f8d941160590017216926db1975.svg) 是所有有效状态的集合,
- ![A](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/e03dd1414bcec5b3baac929fbed8ba0ef00b2d0b.svg) 是所有有效动作的集合,
- ![R : S \times A \times S \to \mathbb{R}](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/d424f8df005c1a370f1be27a7d7827895b36451c.svg) 是奖励函数，其中 ![r_t = R(s_t, a_t, s_{t+1})](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/ecc7ba14305238ee709bb0e6bd888c500f7f6c5b.svg),
- ![P : S \times A \to \mathcal{P}(S)](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/32d7cfdd3132ef22cbabbf8c37729375368a755a.svg) 是转态转移的规则，其中 ![P(s'|s,a)](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/7b7f50f2f22d884bf4bfaab439ad6f4e95d7de85.svg) 是在状态 ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) 下 采取动作 ![a](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg) 转移到状态 ![s'](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/2767335e46fe0770449b884e214f8b3df8958031.svg) 的概率。
- ![\rho_0](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/3607328515af96dafab2766b882383d20739b1c0.svg) 是开始状态的分布。

如果按照真实的环境转化过程看，转化到下一个状态s′的概率既与上一个状态s有关，还与上上个状态，以及上上上个状态有关。这一会导致我们的环境转化模型非常复杂，复杂到难以建模。因此我们需要对强化学习的环境转化模型进行简化。简化的方法就是假设状态转化的马尔科夫性，也就是假设转化到下一个状态s′的概率仅与上一个状态s有关，与之前的状态无关。

# 二、强化学习算法概述

## 强化学习算法分类

![../_images/rl_algorithms_9_15.svg](https://spinningup.readthedocs.io/zh_CN/latest/_images/rl_algorithms_9_15.svg)

> 更加前沿的内容，例如探索学习（exploration），迁移学习（transfer learning），元学习（meta learning）等

## 免模型学习（Model-Free） vs 有模型学习（Model-Based）





# 三、算法实现

> 博客参考：[0084. 强化学习 - 随笔分类 - 刘建平Pinard - 博客园 (cnblogs.com)](https://www.cnblogs.com/pinard/category/1254674.html)
>
> 代码参考：https://github.com/rexrex9/reinforcement_torch_pfrl
>
> ​				   https://github.com/lansinuote/Simple_Reinforcement_Learning

## SARSA 算法

### 算法原理

![img](https://pic4.zhimg.com/80/v2-69cd564673788eb073d8498612b4800b_1440w.png)

SARSA算法，属于在线控制这一类，即一直使用一个**确定的策略**来更新价值函数和选择新的动作，由S,A,R,S,A几个字母组成的。而S,A,R分别代表状态（State），动作(Action),奖励(Reward)，

<img src="https://images2018.cnblogs.com/blog/1042406/201809/1042406-20180909173602306-477774715.jpg" alt="img" style="zoom:50%;" />

在迭代的时候，我们首先基于ϵ-贪婪法在当前状态$S$选择一个动作$A$，这样系统会转到一个新的状态$S′$, 同时给我们一个即时奖励$R$, 在新的状态$S^′$，我们会基于ϵ−贪婪法在状态$S^′$选择一个动作$A^′$，但是注意这时候我们并不执行这个动作$A^′$，只是用来更新的我们的价值函数，价值函数的更新公式是：

​																							$$Q(S,A)=Q(S,A)+α(R+γQ(S^′,A^′)−Q(S,A))$$

用下一步的Q值来更新上一步的Q值

> ε-贪婪法的意思是说，我们有 1 − ε 的概率会按照 Q 函数来决定动作，通常 ε 就设一个很小的值，1 − ε
> 可能是 90%，也就是 90% 的概率会按照 Q 函数来决定动作，但是你有 10% 的机率是随机的。通常在实现上 ε 会随着时间递减。在最开始的时候。因为还不知道哪个动作是比较好的，所以你会花比较大的力气在做探索。接下来随着训练的次数越来越多。已经比较确定说哪一个 Q 是比较好的。你就会减少你的探索，你会把 ε 的值变小，主要根据 Q 函数来决定你的动作，比较少随机决定动作，这是 ε-贪心
>
> 利用：1 − ε 90%
> 探索：ε 10%
> 通常 ε 就设一个很小的值，且 ε 会随着时间递减，即探索越来越小

### 代码实践

![image-20221114161431957](C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221114161431957.png)

**主函数**

```python
if __name__ == '__main__':
    env = gym.make("CliffWalking-v0")         #基于gym创建环境
    env = gridworld.CliffWalkingWapper(env)   
    train(env)
```

**训练函数**

```python
#训练500轮
def train(env,episodes=500,e_greed=0.1,lr=0.1,gamma=0.9):
    agent = SarsaAgent(
        n_states=env.observation_space.n,     #通过环境得到状态
        n_act= env.action_space.n,			  #通过环境得到动作
        lr= lr,
        gamma=gamma,
        e_greed=e_greed
    )
    is_render = False
    for e in range(episodes):
        ep_reward =train_episode(env,agent,is_render)   #训练一轮
        print('Epsode %s:reward= %0.1f'%(e,ep_reward))
        # 每隔50个episode渲染一下看看效果
        if e % 50 == 0:
            is_render = True
        else:
            is_render = False
    test_reward = test_episode(env,agent)    #训练完成后进行一轮测试
    print('test reward = %.1f' % (test_reward))
```

**测试一轮游戏**

```python
#测试一轮游戏
def test_episode(env,agent):
    total_reward = 0
    state = env.reset()
    while True:
        action = agent.predict(state)
        next_state ,reward,done,_ = env.step(action)
        total_reward += reward
        state  = next_state
        env.render()
        time.sleep(0.5)
        if done:break
    return total_reward
```

**训练一轮游戏**

```python
#训练一轮游戏
def train_episode(env,agent,is_render):
    total_reward = 0
    #重置环境
    state = env.reset()
    action = agent.act(state)                        #根据算法初始化随机选择一个动作
    while True:
        next_state ,reward,done,_ = env.step(action) #与环境进行交互，拿到此次交互的reward，下一次的 state
        next_action = agent.act(next_state)          #探索与利用得到下一个 action
        agent.learn(state, action, reward, next_state, next_action, done) #sarsa算法更新Q表格
        action = next_action
        state  = next_state
        total_reward += reward  #累加奖励                    
        if is_render:env.render()
        if done:break
    return total_reward
```

**Agent定义**

```python
#Q坐标横坐标为动作选择，比如 left right down up ，纵坐标为状态对应不同的状态对应不同的格子
class SarsaAgent():
    def __init__(self,n_states,n_act,e_greed=0.1,lr=0.1,gamma=0.9) :
        #定义参数
        self.e_greed = e_greed # 探索与利用中的探索概率
        self.n_states = n_states # 状态数量
        self.n_act = n_act  # 动作数量
        self.lr = lr        # 学习率
        self.gamma = gamma # 收益衰减率
        #初始化一个值全为0的Q表格
        self.Q = np.zeros((n_states,n_act))

    def predict(self,state): #从Q表格选出价值最大的action
        Q_list = self.Q[state,:]
        action =  np.random.choice(np.flatnonzero(Q_list==Q_list.max())) #若最大值不止一个，则随机采样
        return action

    def act(self,state):   #ε-贪心 算法的实现
        if np.random.uniform(0,1)<self.e_greed: #探索，随机选择一个action
            action = np.random.choice(self.n_act)
        else: #利用利用predict 筛选出一个reward最大的action
            action = self.predict(state)
        return action

    def learn(self,state,action,reward,next_state,next_action,done): #sarsa算法 更新Q表格

        cur_Q = self.Q[state , action]  #取当前动作和状态的Q表格的值
        if done:
            target_Q = reward           #判断此轮训练是否结束
        else:
            target_Q = reward + self.gamma*self.Q[next_state,next_action] #sarsa算法

        self.Q[state,action] += self.lr * (target_Q-cur_Q)   #更新Q表格
```

## Q-Learning

### 算法原理

<img src="https://pica.zhimg.com/v2-acde8c12a73ca67ec81fac72b1fd8923_1440w.jpg?source=172ae18b" alt="查看源图像" style="zoom:67%;" />

对于$Q-Learning$，我们会使用ϵ−贪婪法来选择新的动作，这部分和$SARSA$完全相同。但是对于价值函数的更新，$Q-Learning$使用的是贪婪法，而不是$SARSA$的ϵ−贪婪法。这一点就是$SARSA$和$Q-Learning$本质的区别。

![img](https://img2018.cnblogs.com/blog/1042406/201809/1042406-20180918202423478-583844904.jpg)

首先我们基于状态$S$，用ϵ−贪婪法选择到动作$A$, 然后执行动作$A$，得到奖励$R$，并进入状态$S^′$，此时，如果是$SARSA$，会继续基于状态$S′$，用ϵ−贪婪法选择$A^′$,然后来更新价值函数。但是$Q-Learning$则不同。

对于$Q-Learning$，它基于状态$S^′$，没有使用ϵ−贪婪法选择$A^′$，而是使用贪婪法选择$A^′$，也就是说，选择使$Q(S^′,a)$最大的$a$作为$A^′$来更新价值函数。用数学公式表示就是：

​																					$Q(S,A)=Q(S,A)+α(R+γ\mathop{max}\limits_{a}Q(S^′,a)−Q(S,A))$

对应到上图中就是在图下方的三个黑圆圈动作中选择一个使$Q(S^′,a)$最大的动作作为$A^′$。

此时选择的动作只会参与价值函数的更新，不会真正的执行。价值函数更新后，新的执行动作需要基于状态$S^′$，用ϵ−贪婪法重新选择得到。这一点也和$SARSA$稍有不同。对于$SARSA$，价值函数更新使用的$A^′$会作为下一阶段开始时候的执行动作。

### 代码实现

**Learn 函数修改**

```python
    def learn(self,state,action,reward,next_state,done): #Q-Learning 更新Q表格
        cur_Q = self.Q[state , action]
        if done:
            target_Q = reward
        else:
            target_Q = reward + self.gamma*self.Q[next_state,:].max()

        self.Q[state,action] += self.lr * (target_Q-cur_Q)
```

**训练一轮修改**

```python
#训练一轮游戏
def train_episode(env,agent,is_render):
    total_reward = 0
    #重置环境
    state = env.reset()
    while True:
        action = agent.act(state) #action
        next_state ,reward,done,_ = env.step(action) #与环境进行交互
        agent.learn(state, action, reward, next_state,done) #q_learning算法更新Q表格
        state  = next_state
        total_reward += reward
        if is_render:env.render()
        if done:break
    return total_reward
```

$Q-Learning$直接学习的是最优策略，而$SARSA$在学习最优策略的同时还在做探索。这导致我们在学习最优策略的时候，如果用$SARSA$，为了保证收敛，需要制定一个策略，使ϵ−贪婪法的超参数ϵ在迭代的过程中逐渐变小。$Q-Learning$没有这个烦恼。

另外一个就是$Q-Learning$直接学习最优策略，但是最优策略会依赖于训练中产生的一系列数据，所以受样本数据的影响较大，因此受到训练数据方差的影响很大，甚至会影响$Q$函数的收敛。$Q-Learning$的深度强化学习版$Deep Q-Learning$也有这个问题。

在学习过程中，$SARSA$在收敛的过程中鼓励探索，这样学习过程会比较平滑，不至于过于激进，导致出现像$Q-Learning$可能遇到一些特殊的最优“陷阱”。比如经典的强化学习问题"Cliff Walk"。

在实际应用中，如果我们是在模拟环境中训练强化学习模型，推荐使用$Q-Learning$，如果是在线生产环境中训练模型，则推荐使用$SARSA$。

## DQN 

### 算法原理

![查看源图像](https://ts1.cn.mm.bing.net/th/id/R-C.121dbec54d4e1b89ea27f8623faa215f?rik=Q02DV0xDeQYXnQ&riu=http%3a%2f%2fstatic.zybuluo.com%2fWuLiangchao%2f8gbw5uxcymp969jhi7etlbrc%2fimage_1cd2kiaol19ft2kb1dr4ricnorm.png&ehk=FTH8J4%2b4T9Ch68X3%2b%2fq0USVSKqVKc4wdSSvw1pGzxE8%3d&risl=&pid=ImgRaw&r=0)

对于$SARSA$和$Q-Learning$来说，使用的状态都是离散的有限个状态集合，此时问题的规模比较小，比较容易求解。当遇到复杂的状态集合时，无法在内存中维护这么大的一张$Q$表。

由于问题的状态集合规模大，一个可行的建模方法是价值函数的近似表示。方法是我们引入一个状态价值函数$v$, 这个函数由参数$w$描述，并接受状态$s$作为输入，计算后得到状态$s$的价值，即我们期望：

​																													$v(s,w)≈v_π(s)$

类似的，引入一个动作价值函数$q$，这个函数由参数$w$描述，并接受状态$s$与动作$a$作为输入，计算后得到动作价值，即我们期望：

​																													$q(s,a,w)≈q_π(s,a)$

价值函数近似的方法很多,用决策树，最近邻，傅里叶变换，神经网络来表达我们的状态价值函数。而最常见，应用最广泛的表示方法是神经网络，对于神经网络，可以使用$DNN$，$CNN$或者$RNN$。没有特别的限制。如果把我们计算价值函数的神经网络看做一个黑盒子，那么整个近似过程可以看做下面这三种情况：

<img src="https://img2018.cnblogs.com/blog/1042406/201809/1042406-20180928142605652-445522913.jpg" alt="img" style="zoom:80%;" />

对于状态价值函数，神经网络的输入是状态$s$的特征向量，输出是状态价值$v(s,w)$。对于动作价值函数，有两种方法，一种是输入状态$s$的特征向量和动作$a$，输出对应的动作价值$q(s,a,w)$，另一种是只输入状态$s$的特征向量，动作集合有多少个动作就有多少个输出$q(s,a_i,w)$。这里隐含了我们的动作是有限个的离散动作。

$Deep Q-Learning$算法的基本思路来源于$Q-Learning$。但是和$Q-Learning$不同的地方在于，它的$Q$值的计算不是直接通过状态值$s$和动作来计算，而是通过上面讲到的$Q$网络来计算的。这个$Q$网络是一个神经网络，我们一般简称$Deep Q-Learning$为$DQN$。

$DQN$的输入是我们的状态$s$对应的状态向量$ϕ(s)$， 输出是所有动作在该状态下的动作价值函数$Q$。$Q$网络可以是$DNN$，$CNN$或者$RNN$，没有具体的网络结构要求。

$DQN$主要使用的技巧是**经验回放（experience replay）**,即将每次和环境交互得到的奖励与状态更新情况都保存起来，用于后面目标$Q$值的更新。为什么需要经验回放呢？我们回忆一下$Q-Learning$，它是有一张$Q$表来保存所有的$Q$值的当前结果的，但是$DQN$是没有的，那么在做动作价值函数更新的时候，就需要其他的方法，这个方法就是经验回放。

### 代码实现

**神经网络定义**

```python
import torch
class MLP(torch.nn.Module):

    def __init__(self, obs_size,n_act):
        #super()用来调用父类(基类)的方法，__init__()是类的构造方法
        #super().__init__() 就是调用父类的init方法， 同样可以使用super()去调用父类的其他方法。
        super().__init__()
        self.mlp = self.__mlp(obs_size,n_act)

    def __mlp(self,obs_size,n_act):
        return torch.nn.Sequential(
            torch.nn.Linear(obs_size, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, n_act)
        )
        
    def forward(self, x):
        return self.mlp(x)
```

**探索概率衰减**

$𝜀$的值可随着智能体与环境的交互次数增多而减少，例如设定一个𝜀衰减值$𝜀_{𝑑𝑒𝑐𝑎𝑦}$。则每一次$𝜀$的更新可表达为

​																									$𝜀=𝜀−𝜀_{𝑑𝑒𝑐𝑎𝑦}$

```python
class EpsilonGreedy():

    def __init__( self,n_act, e_greed, decay_rate):
        self.n_act = n_act  # 动作数量
        self.epsilon = e_greed  # 探索与利用中的探索概率
        self.decay_rate = decay_rate # 衰减值

    def act(self,predct_method,obs):
        if np.random.uniform(0, 1) < self.epsilon:  #探索
            action = np.random.choice(self.n_act)
        else: # 利用
            action = predct_method(obs)
        self.epsilon = max(0.01,self.epsilon-self.decay_rate) 
        return action
```

**经验池构建**

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221117171752738.png" alt="image-20221117171752738" style="zoom: 50%;" />

指设定一个经验池, 将每一步交互缓存进经验池，积攒到一定程度后可以每一次取出Batch Size个“经验” 从而进行批量学习。

注意事项：

- 频批分开：学习频次与每次学习的“经验”数量，(Batch Size)是不同的。例如可设定为每4轮交互进行一次学习，每次学习从经验池中取出32轮交互经验。

-  延迟启动：前N轮的交互并不进行学习，等经验池中的经验积攒到一定程度后再开始学习。

经验池的好处：

1.	提高样本利用率
2.	打乱样本关联性
➢因为普通的机器学习样本之间的关系都是独立的。而智能体与环境交互产生的经验样本如果不经过处理则存在序列样本关联性，这对模型的更新不利。

```python
import random
import collections
from torch import FloatTensor

class ReplayBuffer(object):
    def __init__(self, max_size, num_steps=1 ):
        #deque 双向队列 类似于list的容器，可以快速的在队列头部和尾部添加、删除元素
        #当加入的元素超过容器容量时，会删除最初添加的元素，然后插入新的元素 先入先出
        self.buffer = collections.deque(maxlen=max_size)
        self.num_steps  = num_steps
       
    def append(self, exp):
        #向容器中添加元素，可添加多维向量
        self.buffer.append(exp)

    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        #zip(*) 可理解为解压 ，5维向量
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*mini_batch)
        #转换为torch的tensor张量
        obs_batch = FloatTensor(obs_batch)
        action_batch = FloatTensor(action_batch)
        reward_batch = FloatTensor(reward_batch)
        next_obs_batch = FloatTensor(next_obs_batch)
        done_batch = FloatTensor(done_batch)
        return obs_batch,action_batch,reward_batch,next_obs_batch,done_batch

    def __len__(self):
        #返回buffer的长度
        return len(self.buffer)
```

**DQNAgent定义**

固定Q目标（Nature DQN)

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221117172238707.png" alt="image-20221117172238707" style="zoom: 50%;" />

- 首先将Q函数复制一份作为目标Q函数。原先的Q函数则称为预测Q函数。

- Predict Q由预测Q函数得到，Target Q由目标Q函数得到。

- 模型学习的过程中仅迭代更新预测Q 函数的模型参数。目标Q函数的模型参数固定不变。

- 每隔一定次数将预测Q函数的模型参数同步给目标Q函数。

```python
import sys
import torch
#添加库路径，上两级文件夹
sys.path.append("../..")
from utils import torchUtils
import copy

class DQNAgent(object):

    def __init__(self,q_func, optimizer, explorer,replay_buffer, batch_size, replay_start_size,update_target_steps, n_act, gamma=0.9):
        '''
        :param q_func:  Q函数
        :param optimizer: 优化器
        :param explorer: 探索器
        :param replay_buffer: 经验回放器
        :param batch_size: 批次数量
        :param replay_start_size: 开始回放的次数
        :param update_target_steps: 同步参数的次数
        :param n_act: 动作数量
        :param gamma: 收益衰减率
        '''
        self.pred_func = q_func   #Q函数
        self.target_func = copy.deepcopy(q_func)  #target_Q函数
        self.update_target_steps = update_target_steps #同步参数的次数

        self.explorer = explorer  #探索器

        self.rb = replay_buffer   #经验池
        self.batch_size = batch_size #批次数量
        self.replay_start_size = replay_start_size #开始回放的次数

        self.optimizer = optimizer  #神经网络优化器
        self.criterion = torch.nn.MSELoss() #神经网络损失函数

        self.global_step = 0
        self.gamma = gamma  # 收益衰减率
        self.n_act = n_act # 动作数量

    # 根据经验得到action
    def predict(self, obs):
        obs = torch.FloatTensor(obs) #转换为tensor向量
        Q_list = self.pred_func(obs) #根据状态预测得到action价值最大的action
        action = int(torch.argmax(Q_list).detach().numpy()) #选取
        return action

    # 根据探索与利用得到action
    def act(self, obs):
        return self.explorer.act(self.predict,obs)

    def learn_batch(self, batch_obs, batch_action, batch_reward, batch_next_obs, batch_done):
        # predict_Q
        pred_Vs = self.pred_func(batch_obs)
        action_onehot = torchUtils.one_hot(batch_action, self.n_act)
        predict_Q = (pred_Vs * action_onehot).sum(1)
        # target_Q
        next_pred_Vs = self.target_func(batch_next_obs)
        best_V = next_pred_Vs.max(1)[0]
        target_Q = batch_reward + (1 - batch_done) * self.gamma * best_V

        self.optimizer.zero_grad()  # 梯度归0
        loss = self.criterion(predict_Q, target_Q)#计算均方差
        loss.backward() #反向传播
        self.optimizer.step() #优化参数

    def learn(self, obs, action, reward, next_obs, done):
        self.global_step += 1
        self.rb.append((obs, action, reward, next_obs, done)) 
        if len(self.rb) > self.replay_start_size and self.global_step % self.rb.num_steps == 0:
            #从经验池随机选取一batch数据进行训练
            self.learn_batch(*self.rb.sample(self.batch_size))
        if self.global_step % self.update_target_steps==0:
            #每隔update_target_steps步 为TargetQ更新参数
            self.sync_target()

    def sync_target(self):
        #神经网络参数拷贝
        for target_param, param in zip(self.target_func.parameters(), self.pred_func.parameters()):
            target_param.data.copy_(param.data)
```

**模型训练**

```python
import agents,modules,replay_buffers,explorers
import gym
import torch

class TrainManager():

    def __init__(self,
                env,#环境
                episodes=1000,#轮次数量
                batch_size = 32,#每一批次的数量
                num_steps=4,#进行学习的频次
                memory_size = 2000,#经验回放池的容量
                replay_start_size = 200,#开始回放的次数
                update_target_steps = 200,#同步参数的次数
                lr=0.001,#学习率
                gamma=0.9, #收益衰减率
                e_greed=0.1, #探索与利用中的探索概率
                e_gredd_decay = 1e-6 #探索与利用中探索概率的衰减步长
                ):

        n_act = env.action_space.n
        n_obs = env.observation_space.shape[0]

        self.env = env
        self.episodes = episodes
		#探索率
        explorer =  explorers.EpsilonGreedy(n_act,e_greed,e_gredd_decay)
        #神经网络近似Q函数
        q_func = modules.MLP(n_obs, n_act)
        #优化器
        optimizer = torch.optim.AdamW(q_func.parameters(), lr=lr)
        #经验回放池
        rb = replay_buffers.ReplayBuffer(memory_size, num_steps)
		#DQNAgent参数定义
        self.agent = agents.DQNAgent(
            q_func=q_func,
            optimizer=optimizer,
            explorer=explorer,
            replay_buffer = rb,
            batch_size=batch_size,
            replay_start_size = replay_start_size,
            update_target_steps = update_target_steps,
            n_act=n_act,
            gamma=gamma)

    def train_episode(self):
        total_reward = 0
        obs = self.env.reset()
        while True:
            action = self.agent.act(obs)
            next_obs, reward, done, _ = self.env.step(action)
            total_reward += reward
            self.agent.learn(obs, action, reward, next_obs, done)
            obs = next_obs
            if done: break
        print('e_greedy =',self.agent.explorer.epsilon)
        return total_reward

    def test_episode(self,is_render=False):
        total_reward = 0
        obs = self.env.reset()
        while True:
            action = self.agent.predict(obs)
            next_obs, reward, done, _ = self.env.step(action)
            total_reward += reward
            obs = next_obs
            if is_render:self.env.render()
            if done: break
        return total_reward

    def train(self):
        #训练1000lun
        for e in range(self.episodes):
            ep_reward = self.train_episode()
            print('Episode %s: reward = %.1f' % (e, ep_reward))
            if e % 100 == 0:
                test_reward = self.test_episode(False)
                print('test reward = %.1f' % (test_reward))

        # 进行最后的测试
        total_test_reward = 0
        for i in range(5):
            total_test_reward += self.test_episode(False)
        print('final test reward = %.1f' % (total_test_reward/5))

if __name__ == '__main__':
    env1 = gym.make("CartPole-v0")
    tm = TrainManager(env1)
    tm.train()
```

## Double DQN

### 算法原理

$Nature DQN$它通过使用两个相同的神经网络，以解决数据样本和网络训练之前的相关性，但是还是有其他值得优化的点。在$DDQN$之前，基本上所有的目标$Q$值都是通过贪婪法直接得到的，无论是$Q-Learning$， $DQN(NIPS 2013)$还是 $Nature DQN$，都是如此。比如对于$Nature DQN$,虽然用了两个$Q$网络并使用目标$Q$网络计算$Q$值，其第$j$个样本的目标$Q$值的计算还是贪婪法得到的，在$DDQN$这里，不再是直接在目标$Q$网络里面找各个动作中最大$Q$值，而是先在当前$Q$网络中先找出最大$Q$值对应的动作。

![查看源图像](https://pic1.zhimg.com/v2-afa15663181a31fc0a9ff87951a18bc4_r.jpg)

## Prioritized Replay DQN

### 算法原理

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221119115007505.png" alt="image-20221119115007505" style="zoom: 50%;" />

在$Prioritized Replay DQN$之前，我们已经讨论了很多种$DQN$，比如$Nature DQN$， $DDQN$等，他们都是通过经验回放来采样，进而做目标$Q$值的计算的。在采样的时候，我们是一视同仁，在经验回放池里面的所有的样本都有相同的被采样到的概率。

但是注意到在经验回放池里面的不同的样本由于$TD$误差的不同，对我们反向传播的作用是不一样的。$TD$误差越大，那么对我们反向传播的作用越大。而$TD$误差小的样本，由于$TD$误差小，对反向梯度的计算影响不大。在$Q$网络中，$TD$误差就是目标$Q$网络计算的目标$Q$值和当前$Q$网络计算的$Q$值之间的差距。

## Dueling DQN

### 算法原理

在前面讲到的$DDQN$中，我们通过优化目标$Q$值的计算来优化算法，在$Prioritized Replay DQN$中，我们通过优化经验回放池按权重采样来优化算法。而在$Dueling DQN$中，我们尝试通过优化神经网络的结构来优化算法。

具体如何优化网络结构呢？$Dueling DQN$考虑将$Q$网络分成两部分，第一部分是仅仅与状态$S$有关，与具体要采用的动作$A$无关，这部分我们叫做价值函数部分，记做$V(S,w,α)$,第二部分同时与状态状态$S$和动作$A$有关，这部分叫做优势函数$(Advantage Function)$部分,记为$A(S,A,w,β)$,那么最终我们的价值函数可以重新表示为：
																										$Q(S,A,w,α,β)=V(S,w,α)+A(S,A,w,β)$
其中，$w$是公共部分的网络参数，而$α$是价值函数独有部分的网络参数，而$β$是优势函数独有部分的网络参数。

![img](https://img2018.cnblogs.com/blog/1042406/201811/1042406-20181107202017462-788522227.png)

而在$Dueling DQN$中，我们在后面加了两个子网络结构，分别对应上面上到价格函数网络部分和优势函数网络部分。对应上面右图所示。最终$Q$网络的输出由价格函数网络的输出和优势函数网络的输出线性组合得到。

## PolicyGradient

### 算法原理

基于策略的方法首先需要将策略参数化。假设目标策略$π_θ$是一个随机性策略，并且处处可微，其中是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为:
$$
J(θ) = E_{s0}[V^{πθ}(s0)]
$$
其中，$s0$表示初始状态。现在有了目标函数，我们将目标函数对策略$θ$求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。

然后我们对目标函数求梯度，可以得到如下式子：
$$
∇_θJ(θ)=E_{πθ}[∇_θlogπθ(s,a)Q_π(s,a)]
$$
蒙特卡罗策略梯度$reinforce$算法, 使用价值函数$v(s)$来近似代替策略梯度公式里面的$Qπ(s,a)$。算法的流程很简单，如下所示：

　　　　输入：N个蒙特卡罗完整序列,训练步长$α$

　　　　输出：策略函数的参数$θ$

　1. for 每个蒙特卡罗序列:

　　　　a. 用蒙特卡罗法计算序列每个时间位置t的状态价值$v_t$

　　　　b. 对序列每个时间位置$t$，使用梯度上升法，更新策略函数的参数$θ$：

​																		$θ=θ+α∇_θlogπ_θ(s_t,a_t)v_t$

​    2. 返回策略函数的参数$θ$

　　这里的策略函数可以是$softmax$策略，高斯策略或者其他策略。

### 代码实现

**模型定义**

```python
import torch
#定义模型
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
    torch.nn.Softmax(dim=1),
)
model(torch.randn(2, 4))
```

**动作选取**

```python
import random
#得到一个动作
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)
    #[1, 4] -> [1, 2]
    prob = model(state)
    #根据概率选择一个动作
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]
    return action
get_action([1, 2, 3, 4])
```

**得到一局游戏的数据**

```python
#得到一局游戏的数据
def get_data():
    states = []
    rewards = []
    actions = []
    #初始化游戏
    state = env.reset()
    #玩到游戏结束为止
    over = False
    while not over:
        #根据当前状态得到一个动作
        action = get_action(state)

        #执行动作,得到反馈
        next_state, reward, over, _ = env.step(action)

        #记录数据样本
        states.append(state)
        rewards.append(reward)
        actions.append(action)

        #更新游戏状态,开始下一个动作
        state = next_state

    return states, rewards, actions
```

**训练实现**

```python
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    #玩N局游戏,每局游戏训练一次
    for epoch in range(1000):
        #玩一局游戏,得到数据
        states, rewards, actions = get_data()

        optimizer.zero_grad()

        #反馈的和,初始化为0
        reward_sum = 0

        #从最后一步算起
        for i in reversed(range(len(states))):

            #反馈的和,从最后一步的反馈开始计算
            #每往前一步,>>和<<都衰减0.02,然后再加上当前步的反馈
            reward_sum *= 0.98
            reward_sum += rewards[i]

            #重新计算对应动作的概率
            state = torch.FloatTensor(states[i]).reshape(1, 4)
            #[1, 4] -> [1, 2]
            prob = model(state)
            #[1, 2] -> scala
            prob = prob[0, actions[i]]

            #根据求导公式,符号取反是因为这里是求loss,所以优化方向相反，每一步的损失函数
            loss = -prob.log() * reward_sum

            #累积梯度，反向传播计算梯度
            loss.backward(retain_graph=True)
		#梯度下降
        optimizer.step()

        if epoch % 100 == 0:
            test_result = sum([test(play=False) for _ in range(10)]) / 10
            print(epoch, test_result)
```

## Actor-Critic



### 算法原理

## A3C

## DDPG

### 算法原理

# 论文

## 1.免模型强化学习

### a.Deep Q-Learning

| [1]  | [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih et al, 2013. **Algorithm: DQN.** |
| ---- | ------------------------------------------------------------ |

| [2]  | [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/abs/1507.06527), Hausknecht and Stone, 2015. **Algorithm: Deep Recurrent Q-Learning.** |
| ---- | ------------------------------------------------------------ |

| [3]  | [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581), Wang et al, 2015. **Algorithm: Dueling DQN.** |
| ---- | ------------------------------------------------------------ |

| [4]  | [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461), Hasselt et al 2015. **Algorithm: Double DQN.** |
| ---- | ------------------------------------------------------------ |

| [5]  | [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952), Schaul et al, 2015. **Algorithm: Prioritized Experience Replay (PER).** |
| ---- | ------------------------------------------------------------ |

| [6]  | [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298), Hessel et al, 2017. **Algorithm: Rainbow DQN.** |
| ---- | ------------------------------------------------------------ |

### b.策略梯度

| [7]  | [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783), Mnih et al, 2016. **Algorithm: A3C.** |
| ---- | ------------------------------------------------------------ |

| [8]  | [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477), Schulman et al, 2015. **Algorithm: TRPO.** |
| ---- | ------------------------------------------------------------ |

| [9]  | [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438), Schulman et al, 2015. **Algorithm: GAE.** |
| ---- | ------------------------------------------------------------ |

| [10] | [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347), Schulman et al, 2017. **Algorithm: PPO-Clip, PPO-Penalty.** |
| ---- | ------------------------------------------------------------ |

| [11] | [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286), Heess et al, 2017. **Algorithm: PPO-Penalty.** |
| ---- | ------------------------------------------------------------ |

| [12] | [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation](https://arxiv.org/abs/1708.05144), Wu et al, 2017. **Algorithm: ACKTR.** |
| ---- | ------------------------------------------------------------ |

| [13] | [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224), Wang et al, 2016. **Algorithm: ACER.** |
| ---- | ------------------------------------------------------------ |

| [14] | [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290), Haarnoja et al, 2018. **Algorithm: SAC.** |
| ---- | ------------------------------------------------------------ |

### c.确定策略梯度

| [15] | [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf), Silver et al, 2014. **Algorithm: DPG.** |
| ---- | ------------------------------------------------------------ |

| [16] | [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971), Lillicrap et al, 2015. **Algorithm: DDPG.** |
| ---- | ------------------------------------------------------------ |

| [17] | [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477), Fujimoto et al, 2018. **Algorithm: TD3.** |
| ---- | ------------------------------------------------------------ |

## 2.模仿学习和逆强化学习

# 四、项目


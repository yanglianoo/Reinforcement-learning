# å¼ºåŒ–å­¦ä¹ 

# ä¸€ã€å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ

![../_images/rl_diagram_transparent_bg.png](https://spinningup.readthedocs.io/zh_CN/latest/_images/rl_diagram_transparent_bg.png)

æ™ºèƒ½ä½“å’Œç¯å¢ƒçš„å¾ªç¯ä½œç”¨

å¼ºåŒ–å­¦ä¹ çš„ä¸»è¦è§’è‰²æ˜¯ **æ™ºèƒ½ä½“** å’Œ **ç¯å¢ƒ**,ç¯å¢ƒæ˜¯æ™ºèƒ½ä½“å­˜åœ¨å’Œäº’åŠ¨çš„ä¸–ç•Œã€‚æ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥çš„äº¤äº’ä¸­ï¼Œéƒ½ä¼šè·å¾—å¯¹äºæ‰€å¤„ç¯å¢ƒçŠ¶æ€çš„è§‚å¯Ÿï¼ˆæœ‰å¯èƒ½åªæ˜¯ä¸€éƒ¨åˆ†ï¼‰ï¼Œç„¶åå†³å®šä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„åŠ¨ä½œã€‚ç¯å¢ƒä¼šå› ä¸ºæ™ºèƒ½ä½“å¯¹å®ƒçš„åŠ¨ä½œè€Œæ”¹å˜ï¼Œä¹Ÿå¯èƒ½è‡ªå·±æ”¹å˜ã€‚

æ™ºèƒ½ä½“ä¹Ÿä¼šä»ç¯å¢ƒä¸­æ„ŸçŸ¥åˆ° **å¥–åŠ±** ä¿¡å·ï¼Œä¸€ä¸ªè¡¨æ˜å½“å‰çŠ¶æ€å¥½åçš„æ•°å­—ã€‚æ™ºèƒ½ä½“çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯è®¡å¥–åŠ±ï¼Œä¹Ÿå°±æ˜¯ **å›æŠ¥**ã€‚å¼ºåŒ–å­¦ä¹ å°±æ˜¯æ™ºèƒ½ä½“é€šè¿‡å­¦ä¹ æ¥å®Œæˆç›®æ ‡çš„æ–¹æ³•ã€‚

åŸºæœ¬æœ¯è¯­ï¼š

- çŠ¶æ€å’Œè§‚å¯Ÿ(states and observations)
- åŠ¨ä½œç©ºé—´(action spaces)
- ç­–ç•¥(policies)
- è¡ŒåŠ¨è½¨è¿¹(trajectories)
- ä¸åŒçš„å›æŠ¥å…¬å¼(formulations of return)
- å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–é—®é¢˜(the RL optimization problem)
- å€¼å‡½æ•°(value functions)

## å¼ºåŒ–å­¦ä¹ å»ºæ¨¡

### **åŸºæœ¬å…ƒç´ **

- ç¯å¢ƒçŠ¶æ€ $S$ ,$t$ æ—¶åˆ»ç¯å¢ƒçš„çŠ¶æ€$S_t$æ˜¯å®ƒçš„ç¯å¢ƒçŠ¶æ€é›†ä¸­æŸä¸€ä¸ªçŠ¶æ€
- ä¸ªä½“åŠ¨ä½œ$A$, $t$ æ—¶åˆ»ä¸ªä½“é‡‡å–çš„åŠ¨ä½œ$A_t$æ˜¯å®ƒçš„åŠ¨ä½œé›†ä¸­æŸä¸€ä¸ªåŠ¨ä½œ
- ç¯å¢ƒçš„å¥–åŠ±$R$,$t$ æ—¶åˆ»ä¸ªä½“åœ¨çŠ¶æ€$S_t$é‡‡å–çš„åŠ¨ä½œ$A_t$å¯¹åº”çš„å¥–åŠ±$R_{t+1}$ä¼šåœ¨$t+1$æ—¶åˆ»å¾—åˆ°ã€‚
- ä¸ªä½“ç­–ç•¥ $Ï€$,  $Ï€(a|s)=P(A_t=a|S_t=s)$ ,åœ¨çŠ¶æ€$S$æ—¶é‡‡å–åŠ¨ä½œ$a$çš„æ¦‚ç‡
- ä¸ªä½“åœ¨ç­–ç•¥$Ï€$å’ŒçŠ¶æ€$S$æ—¶ï¼Œé‡‡å–è¡ŒåŠ¨åçš„ä»·å€¼ï¼ˆvalueï¼‰$v_Ï€(s)$ 

â€‹                                                                                    $v_Ï€(s)=E_Ï€(R_{t+1}+Î³R_{t+2}+Î³^2R_{t+3}+...|S_t=s)$

- $Î³$æ˜¯å³å¥–åŠ±è¡°å‡å› å­ï¼Œåœ¨[0ï¼Œ1]

- çŠ¶æ€è½¬åŒ–æ¨¡å‹   $ P^a_{ssâ€²}$,   åœ¨çŠ¶æ€$s$ä¸‹é‡‡å–åŠ¨ä½œ$a$,è½¬åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€$sâ€²$çš„æ¦‚ç‡

### ç¯å¢ƒè½¬æ¢æ¨¡å‹

å‡è®¾è½¬åŒ–åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€$sâ€²$çš„æ¦‚ç‡ä»…ä¸ä¸Šä¸€ä¸ªçŠ¶æ€$s$æœ‰å…³ï¼Œä¸ä¹‹å‰çš„çŠ¶æ€æ— å…³ã€‚ç”¨å…¬å¼è¡¨ç¤ºå°±æ˜¯ï¼š

â€‹																						$P^a_{ssâ€²}=E(S_{t+1=sâ€²}|S_t=s,A_t=a)$

å‡è®¾åœ¨çŠ¶æ€$s$æ—¶é‡‡å–åŠ¨ä½œ$a$çš„æ¦‚ç‡ä»…ä¸å½“å‰çŠ¶æ€$s$æœ‰å…³ï¼Œä¸å…¶ä»–çš„è¦ç´ æ— å…³ã€‚ç”¨å…¬å¼è¡¨ç¤ºå°±æ˜¯:

â€‹																							$Ï€(a|s)=P(A_t=a|S_t=s)$



### ä»·å€¼å‡½æ•°

$v_Ï€(s)$ï¼š çŠ¶æ€ä»·å€¼å‡½æ•°

â€‹																						$v_Ï€(s)=E_Ï€(R_{t+1}+Î³R_{t+2}+Î³^2R_{t+3}+...|S_t=s)$

$q_Ï€(s,a)$:   åŠ¨ä½œä»·å€¼å‡½æ•°

â€‹														$q_Ï€(s,a)=E_Ï€(G_t|S_t=s,A_t=a)=E_Ï€(R_{t+1}+Î³R_{t+2}+Î³^2R_{t+3}+...|S_t=s,A_t=a)$

$v_Ï€(s)$ ä¸ $q_Ï€(s,a)$ ä¹‹é—´çš„è½¬æ¢å…³ç³»ï¼š



![image-20221111153105853](C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221111153105853.png)

> ==çŠ¶æ€ä»·å€¼å’ŒåŠ¨ä½œä»·å€¼çš„è§£é‡Š==
>
> çŠ¶æ€ä»·å€¼å‡½æ•°æ˜¯åœ¨ä¸€ä¸ªç¡®å®šçš„ç­–ç•¥$Ï€$ä¸‹ï¼Œåœ¨å½“å‰çŠ¶æ€ä¸‹é‡‡å–è¡ŒåŠ¨åçš„ä»·å€¼ï¼Œä¸è€ƒè™‘é‡‡å–å“ªä¸ªç¡®å®šçš„åŠ¨ä½œï¼Œæ˜¯åœ¨æ­¤ç­–ç•¥ä¸‹æ­¤çŠ¶æ€ä¸‹æ‰€æœ‰åŠ¨ä½œäº§ç”Ÿçš„ä»·å€¼æœŸæœ›
>
> åŠ¨ä½œä»·å€¼å‡½æ•°æ˜¯åœ¨ä¸€ä¸ªç¡®å®šçš„ç­–ç•¥$Ï€$ä¸‹ï¼Œåœ¨å½“å‰çŠ¶æ€ä¸‹é‡‡å–æŸä¸ªç¡®å®šçš„åŠ¨ä½œåçš„ä»·å€¼ï¼Œç”±åœ¨å½“å‰çŠ¶æ€ä¸‹é‡‡å–æŸä¸ªç¡®å®šçš„åŠ¨ä½œåä¼šå¾—åˆ°ä¸€ä¸ªä»·å€¼å³$R^a_s$, æ­¤æ—¶çŠ¶æ€å‘ç”Ÿæ”¹å˜ï¼ŒçŠ¶æ€ä¼šæ ¹æ®ç¯å¢ƒçŠ¶æ€è½¬ç§»ç­–ç•¥è½¬æ¢ä¸ºä¸åŒçš„çŠ¶æ€ï¼Œå°†è½¬æ¢åæ‰€æœ‰çš„çŠ¶æ€ä»·å€¼æ±‚æœŸæœ›å†åŠ ä¸Š$R^a_s$å°±å¾—åˆ°äº†åŠ¨ä½œä»·å€¼

### æœ€ä¼˜ä»·å€¼å‡½æ•°ï¼š

å¯»æ‰¾è¾ƒä¼˜ç­–ç•¥å¯ä»¥é€šè¿‡å¯»æ‰¾è¾ƒä¼˜çš„ä»·å€¼å‡½æ•°æ¥å®Œæˆã€‚å¯ä»¥å®šä¹‰æœ€ä¼˜çŠ¶æ€ä»·å€¼å‡½æ•°æ˜¯æ‰€æœ‰ç­–ç•¥ä¸‹äº§ç”Ÿçš„ä¼—å¤šçŠ¶æ€ä»·å€¼å‡½æ•°ä¸­çš„æœ€å¤§è€…ï¼Œå³ï¼š

â€‹																									$vâˆ—(s)=\mathop{max}\limits_{Ï€}v_Ï€(s)$

åŒç†ä¹Ÿå¯ä»¥å®šä¹‰æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°æ˜¯æ‰€æœ‰ç­–ç•¥ä¸‹äº§ç”Ÿçš„ä¼—å¤šåŠ¨ä½œçŠ¶æ€ä»·å€¼å‡½æ•°ä¸­çš„æœ€å¤§è€…ï¼Œå³ï¼š

â€‹																							  	$q_âˆ—(s,a)=\mathop{max}\limits_{Ï€}qÏ€(s,a)$

åªè¦æˆ‘ä»¬æ‰¾åˆ°äº†æœ€å¤§çš„çŠ¶æ€ä»·å€¼å‡½æ•°æˆ–è€…åŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œé‚£ä¹ˆå¯¹åº”çš„ç­–ç•¥$Ï€^âˆ—$å°±æ˜¯æˆ‘ä»¬å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„è§£ã€‚åŒæ—¶ï¼Œåˆ©ç”¨çŠ¶æ€ä»·å€¼å‡½æ•°å’ŒåŠ¨ä½œä»·å€¼å‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¾—åˆ°:

â€‹																							  	$v_âˆ—(s)=\mathop{max}\limits_{a}qâˆ—(s,a)$

åè¿‡æ¥çš„æœ€ä¼˜ä»·å€¼å‡½æ•°å…³ç³»ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°ï¼š

â€‹																								$q_âˆ—(s,a)=R^a_s+Î³\sum\limits_{sâ€²âˆˆS}P^a_{ssâ€²}v_âˆ—(sâ€²)$

## çŠ¶æ€å’Œè§‚å¯Ÿ

ä¸€ä¸ª **çŠ¶æ€** ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) æ˜¯ä¸€ä¸ªå…³äºè¿™ä¸ªä¸–ç•ŒçŠ¶æ€çš„å®Œæ•´æè¿°ã€‚è¿™ä¸ªä¸–ç•Œé™¤äº†çŠ¶æ€ä»¥å¤–æ²¡æœ‰åˆ«çš„ä¿¡æ¯ã€‚**è§‚å¯Ÿ** ![o](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg) æ˜¯å¯¹äºä¸€ä¸ªçŠ¶æ€çš„éƒ¨åˆ†æè¿°ï¼Œå¯èƒ½ä¼šæ¼æ‰ä¸€äº›ä¿¡æ¯ã€‚

åœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç”¨ [å®æ•°å‘é‡ã€çŸ©é˜µæˆ–è€…æ›´é«˜é˜¶çš„å¼ é‡ï¼ˆtensorï¼‰](https://en.wikipedia.org/wiki/Real_coordinate_space) è¡¨ç¤ºçŠ¶æ€å’Œè§‚å¯Ÿã€‚æ¯”å¦‚è¯´ï¼Œè§†è§‰ä¸Šçš„ **è§‚å¯Ÿ** å¯ä»¥ç”¨RGBçŸ©é˜µçš„æ–¹å¼è¡¨ç¤ºå…¶åƒç´ å€¼ï¼›æœºå™¨äººçš„ **çŠ¶æ€** å¯ä»¥é€šè¿‡å…³èŠ‚è§’åº¦å’Œé€Ÿåº¦æ¥è¡¨ç¤ºã€‚

å¦‚æœæ™ºèƒ½ä½“è§‚å¯Ÿåˆ°ç¯å¢ƒçš„å…¨éƒ¨çŠ¶æ€ï¼Œæˆ‘ä»¬é€šå¸¸è¯´ç¯å¢ƒæ˜¯è¢« **å…¨é¢è§‚å¯Ÿ** çš„ã€‚å¦‚æœæ™ºèƒ½ä½“åªèƒ½è§‚å¯Ÿåˆ°ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º **éƒ¨åˆ†è§‚å¯Ÿ**ã€‚

> å¼ºåŒ–å­¦ä¹ æœ‰æ—¶å€™ç”¨è¿™ä¸ªç¬¦å· ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) ä»£è¡¨çŠ¶æ€ , æœ‰äº›åœ°æ–¹ä¹Ÿä¼šå†™ä½œè§‚å¯Ÿç¬¦å· ![o](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/a97d2a90fb666358380ca3bbc433d8f9ab7c7e42.svg). å°¤å…¶æ˜¯ï¼Œå½“æ™ºèƒ½ä½“åœ¨å†³å®šé‡‡å–ä»€ä¹ˆåŠ¨ä½œçš„æ—¶å€™ï¼Œç¬¦å·ä¸Šçš„è¡¨ç¤ºæŒ‰ç†åŠ¨ä½œæ˜¯åŸºäºçŠ¶æ€çš„ï¼Œä½†å®é™…ä¸Šï¼ŒåŠ¨ä½œæ˜¯åŸºäºè§‚å¯Ÿçš„ï¼Œå› ä¸ºæ™ºèƒ½ä½“å¹¶ä¸èƒ½çŸ¥é“çŠ¶æ€ï¼ˆåªèƒ½é€šè¿‡è§‚å¯Ÿäº†è§£çŠ¶æ€)ã€‚

## åŠ¨ä½œç©ºé—´

ä¸åŒçš„ç¯å¢ƒæœ‰ä¸åŒçš„åŠ¨ä½œã€‚æ‰€æœ‰æœ‰æ•ˆåŠ¨ä½œçš„é›†åˆç§°ä¹‹ä¸º **åŠ¨ä½œç©ºé—´**ã€‚æœ‰äº›ç¯å¢ƒï¼Œæ¯”å¦‚è¯´ Atari æ¸¸æˆå’Œå›´æ£‹ï¼Œå±äº **ç¦»æ•£åŠ¨ä½œç©ºé—´**ï¼Œè¿™ç§æƒ…å†µä¸‹æ™ºèƒ½ä½“åªèƒ½é‡‡å–æœ‰é™çš„åŠ¨ä½œã€‚å…¶ä»–çš„ä¸€äº›ç¯å¢ƒï¼Œæ¯”å¦‚æ™ºèƒ½ä½“åœ¨ç‰©ç†ä¸–ç•Œä¸­æ§åˆ¶æœºå™¨äººï¼Œå±äº **è¿ç»­åŠ¨ä½œç©ºé—´**ã€‚åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ï¼ŒåŠ¨ä½œæ˜¯å®æ•°å‘é‡ã€‚

è¿™ç§åŒºåˆ«å¯¹äºæ·±åº¦å¼ºåŒ–å­¦ä¹ æ¥è¯´ï¼Œå½±å“æ·±è¿œã€‚æœ‰äº›ç§ç±»çš„ç®—æ³•åªèƒ½ç›´æ¥ç”¨åœ¨æŸäº›æ¡ˆä¾‹ä¸Šï¼Œå¦‚æœéœ€è¦ç”¨åœ¨åˆ«çš„åœ°æ–¹ï¼Œå¯èƒ½å°±éœ€è¦å¤§é‡é‡å†™ä»£ç ã€‚

## é©¬å°”å¯å¤«å†³ç­–

**é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹** (Markov Decision Processes, MDPs)ã€‚MDPæ˜¯ä¸€ä¸ª5å…ƒç»„ ![\langle S, A, R, P, \rho_0 \rangle](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/8856e84a582f7587b47f6cc8c7846da6994492e9.svg)ï¼Œå…¶ä¸­

- ![S](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/54f67ffbc0534f8d941160590017216926db1975.svg) æ˜¯æ‰€æœ‰æœ‰æ•ˆçŠ¶æ€çš„é›†åˆ,
- ![A](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/e03dd1414bcec5b3baac929fbed8ba0ef00b2d0b.svg) æ˜¯æ‰€æœ‰æœ‰æ•ˆåŠ¨ä½œçš„é›†åˆ,
- ![R : S \times A \times S \to \mathbb{R}](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/d424f8df005c1a370f1be27a7d7827895b36451c.svg) æ˜¯å¥–åŠ±å‡½æ•°ï¼Œå…¶ä¸­ ![r_t = R(s_t, a_t, s_{t+1})](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/ecc7ba14305238ee709bb0e6bd888c500f7f6c5b.svg),
- ![P : S \times A \to \mathcal{P}(S)](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/32d7cfdd3132ef22cbabbf8c37729375368a755a.svg) æ˜¯è½¬æ€è½¬ç§»çš„è§„åˆ™ï¼Œå…¶ä¸­ ![P(s'|s,a)](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/7b7f50f2f22d884bf4bfaab439ad6f4e95d7de85.svg) æ˜¯åœ¨çŠ¶æ€ ![s](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/5ecb694c8b2755909226b2d74b8b998d9b4e6148.svg) ä¸‹ é‡‡å–åŠ¨ä½œ ![a](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/7299c243b08052a2a26e53de560e7002cb31b38f.svg) è½¬ç§»åˆ°çŠ¶æ€ ![s'](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/2767335e46fe0770449b884e214f8b3df8958031.svg) çš„æ¦‚ç‡ã€‚
- ![\rho_0](https://spinningup.readthedocs.io/zh_CN/latest/_images/math/3607328515af96dafab2766b882383d20739b1c0.svg) æ˜¯å¼€å§‹çŠ¶æ€çš„åˆ†å¸ƒã€‚

å¦‚æœæŒ‰ç…§çœŸå®çš„ç¯å¢ƒè½¬åŒ–è¿‡ç¨‹çœ‹ï¼Œè½¬åŒ–åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€sâ€²çš„æ¦‚ç‡æ—¢ä¸ä¸Šä¸€ä¸ªçŠ¶æ€sæœ‰å…³ï¼Œè¿˜ä¸ä¸Šä¸Šä¸ªçŠ¶æ€ï¼Œä»¥åŠä¸Šä¸Šä¸Šä¸ªçŠ¶æ€æœ‰å…³ã€‚è¿™ä¸€ä¼šå¯¼è‡´æˆ‘ä»¬çš„ç¯å¢ƒè½¬åŒ–æ¨¡å‹éå¸¸å¤æ‚ï¼Œå¤æ‚åˆ°éš¾ä»¥å»ºæ¨¡ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¯¹å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒè½¬åŒ–æ¨¡å‹è¿›è¡Œç®€åŒ–ã€‚ç®€åŒ–çš„æ–¹æ³•å°±æ˜¯å‡è®¾çŠ¶æ€è½¬åŒ–çš„é©¬å°”ç§‘å¤«æ€§ï¼Œä¹Ÿå°±æ˜¯å‡è®¾è½¬åŒ–åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€sâ€²çš„æ¦‚ç‡ä»…ä¸ä¸Šä¸€ä¸ªçŠ¶æ€sæœ‰å…³ï¼Œä¸ä¹‹å‰çš„çŠ¶æ€æ— å…³ã€‚

# äºŒã€å¼ºåŒ–å­¦ä¹ ç®—æ³•æ¦‚è¿°

## å¼ºåŒ–å­¦ä¹ ç®—æ³•åˆ†ç±»

![../_images/rl_algorithms_9_15.svg](https://spinningup.readthedocs.io/zh_CN/latest/_images/rl_algorithms_9_15.svg)

> æ›´åŠ å‰æ²¿çš„å†…å®¹ï¼Œä¾‹å¦‚æ¢ç´¢å­¦ä¹ ï¼ˆexplorationï¼‰ï¼Œè¿ç§»å­¦ä¹ ï¼ˆtransfer learningï¼‰ï¼Œå…ƒå­¦ä¹ ï¼ˆmeta learningï¼‰ç­‰

## å…æ¨¡å‹å­¦ä¹ ï¼ˆModel-Freeï¼‰ vs æœ‰æ¨¡å‹å­¦ä¹ ï¼ˆModel-Basedï¼‰





# ä¸‰ã€ç®—æ³•å®ç°

> åšå®¢å‚è€ƒï¼š[0084. å¼ºåŒ–å­¦ä¹  - éšç¬”åˆ†ç±» - åˆ˜å»ºå¹³Pinard - åšå®¢å›­ (cnblogs.com)](https://www.cnblogs.com/pinard/category/1254674.html)
>
> ä»£ç å‚è€ƒï¼šhttps://github.com/rexrex9/reinforcement_torch_pfrl
>
> â€‹				   https://github.com/lansinuote/Simple_Reinforcement_Learning

## SARSA ç®—æ³•

### ç®—æ³•åŸç†

![img](https://pic4.zhimg.com/80/v2-69cd564673788eb073d8498612b4800b_1440w.png)

SARSAç®—æ³•ï¼Œå±äºåœ¨çº¿æ§åˆ¶è¿™ä¸€ç±»ï¼Œå³ä¸€ç›´ä½¿ç”¨ä¸€ä¸ª**ç¡®å®šçš„ç­–ç•¥**æ¥æ›´æ–°ä»·å€¼å‡½æ•°å’Œé€‰æ‹©æ–°çš„åŠ¨ä½œï¼Œç”±S,A,R,S,Aå‡ ä¸ªå­—æ¯ç»„æˆçš„ã€‚è€ŒS,A,Råˆ†åˆ«ä»£è¡¨çŠ¶æ€ï¼ˆStateï¼‰ï¼ŒåŠ¨ä½œ(Action),å¥–åŠ±(Reward)ï¼Œ

<img src="https://images2018.cnblogs.com/blog/1042406/201809/1042406-20180909173602306-477774715.jpg" alt="img" style="zoom:50%;" />

åœ¨è¿­ä»£çš„æ—¶å€™ï¼Œæˆ‘ä»¬é¦–å…ˆåŸºäºÏµ-è´ªå©ªæ³•åœ¨å½“å‰çŠ¶æ€$S$é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ$A$ï¼Œè¿™æ ·ç³»ç»Ÿä¼šè½¬åˆ°ä¸€ä¸ªæ–°çš„çŠ¶æ€$Sâ€²$, åŒæ—¶ç»™æˆ‘ä»¬ä¸€ä¸ªå³æ—¶å¥–åŠ±$R$, åœ¨æ–°çš„çŠ¶æ€$S^â€²$ï¼Œæˆ‘ä»¬ä¼šåŸºäºÏµâˆ’è´ªå©ªæ³•åœ¨çŠ¶æ€$S^â€²$é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ$A^â€²$ï¼Œä½†æ˜¯æ³¨æ„è¿™æ—¶å€™æˆ‘ä»¬å¹¶ä¸æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œ$A^â€²$ï¼Œåªæ˜¯ç”¨æ¥æ›´æ–°çš„æˆ‘ä»¬çš„ä»·å€¼å‡½æ•°ï¼Œä»·å€¼å‡½æ•°çš„æ›´æ–°å…¬å¼æ˜¯ï¼š

â€‹																							$$Q(S,A)=Q(S,A)+Î±(R+Î³Q(S^â€²,A^â€²)âˆ’Q(S,A))$$

ç”¨ä¸‹ä¸€æ­¥çš„Qå€¼æ¥æ›´æ–°ä¸Šä¸€æ­¥çš„Qå€¼

> Îµ-è´ªå©ªæ³•çš„æ„æ€æ˜¯è¯´ï¼Œæˆ‘ä»¬æœ‰ 1 âˆ’ Îµ çš„æ¦‚ç‡ä¼šæŒ‰ç…§ Q å‡½æ•°æ¥å†³å®šåŠ¨ä½œï¼Œé€šå¸¸ Îµ å°±è®¾ä¸€ä¸ªå¾ˆå°çš„å€¼ï¼Œ1 âˆ’ Îµ
> å¯èƒ½æ˜¯ 90%ï¼Œä¹Ÿå°±æ˜¯ 90% çš„æ¦‚ç‡ä¼šæŒ‰ç…§ Q å‡½æ•°æ¥å†³å®šåŠ¨ä½œï¼Œä½†æ˜¯ä½ æœ‰ 10% çš„æœºç‡æ˜¯éšæœºçš„ã€‚é€šå¸¸åœ¨å®ç°ä¸Š Îµ ä¼šéšç€æ—¶é—´é€’å‡ã€‚åœ¨æœ€å¼€å§‹çš„æ—¶å€™ã€‚å› ä¸ºè¿˜ä¸çŸ¥é“å“ªä¸ªåŠ¨ä½œæ˜¯æ¯”è¾ƒå¥½çš„ï¼Œæ‰€ä»¥ä½ ä¼šèŠ±æ¯”è¾ƒå¤§çš„åŠ›æ°”åœ¨åšæ¢ç´¢ã€‚æ¥ä¸‹æ¥éšç€è®­ç»ƒçš„æ¬¡æ•°è¶Šæ¥è¶Šå¤šã€‚å·²ç»æ¯”è¾ƒç¡®å®šè¯´å“ªä¸€ä¸ª Q æ˜¯æ¯”è¾ƒå¥½çš„ã€‚ä½ å°±ä¼šå‡å°‘ä½ çš„æ¢ç´¢ï¼Œä½ ä¼šæŠŠ Îµ çš„å€¼å˜å°ï¼Œä¸»è¦æ ¹æ® Q å‡½æ•°æ¥å†³å®šä½ çš„åŠ¨ä½œï¼Œæ¯”è¾ƒå°‘éšæœºå†³å®šåŠ¨ä½œï¼Œè¿™æ˜¯ Îµ-è´ªå¿ƒ
>
> åˆ©ç”¨ï¼š1 âˆ’ Îµ 90%
> æ¢ç´¢ï¼šÎµ 10%
> é€šå¸¸ Îµ å°±è®¾ä¸€ä¸ªå¾ˆå°çš„å€¼ï¼Œä¸” Îµ ä¼šéšç€æ—¶é—´é€’å‡ï¼Œå³æ¢ç´¢è¶Šæ¥è¶Šå°

### ä»£ç å®è·µ

![image-20221114161431957](C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221114161431957.png)

**ä¸»å‡½æ•°**

```python
if __name__ == '__main__':
    env = gym.make("CliffWalking-v0")         #åŸºäºgymåˆ›å»ºç¯å¢ƒ
    env = gridworld.CliffWalkingWapper(env)   
    train(env)
```

**è®­ç»ƒå‡½æ•°**

```python
#è®­ç»ƒ500è½®
def train(env,episodes=500,e_greed=0.1,lr=0.1,gamma=0.9):
    agent = SarsaAgent(
        n_states=env.observation_space.n,     #é€šè¿‡ç¯å¢ƒå¾—åˆ°çŠ¶æ€
        n_act= env.action_space.n,			  #é€šè¿‡ç¯å¢ƒå¾—åˆ°åŠ¨ä½œ
        lr= lr,
        gamma=gamma,
        e_greed=e_greed
    )
    is_render = False
    for e in range(episodes):
        ep_reward =train_episode(env,agent,is_render)   #è®­ç»ƒä¸€è½®
        print('Epsode %s:reward= %0.1f'%(e,ep_reward))
        # æ¯éš”50ä¸ªepisodeæ¸²æŸ“ä¸€ä¸‹çœ‹çœ‹æ•ˆæœ
        if e % 50 == 0:
            is_render = True
        else:
            is_render = False
    test_reward = test_episode(env,agent)    #è®­ç»ƒå®Œæˆåè¿›è¡Œä¸€è½®æµ‹è¯•
    print('test reward = %.1f' % (test_reward))
```

**æµ‹è¯•ä¸€è½®æ¸¸æˆ**

```python
#æµ‹è¯•ä¸€è½®æ¸¸æˆ
def test_episode(env,agent):
    total_reward = 0
    state = env.reset()
    while True:
        action = agent.predict(state)
        next_state ,reward,done,_ = env.step(action)
        total_reward += reward
        state  = next_state
        env.render()
        time.sleep(0.5)
        if done:break
    return total_reward
```

**è®­ç»ƒä¸€è½®æ¸¸æˆ**

```python
#è®­ç»ƒä¸€è½®æ¸¸æˆ
def train_episode(env,agent,is_render):
    total_reward = 0
    #é‡ç½®ç¯å¢ƒ
    state = env.reset()
    action = agent.act(state)                        #æ ¹æ®ç®—æ³•åˆå§‹åŒ–éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    while True:
        next_state ,reward,done,_ = env.step(action) #ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œæ‹¿åˆ°æ­¤æ¬¡äº¤äº’çš„rewardï¼Œä¸‹ä¸€æ¬¡çš„ state
        next_action = agent.act(next_state)          #æ¢ç´¢ä¸åˆ©ç”¨å¾—åˆ°ä¸‹ä¸€ä¸ª action
        agent.learn(state, action, reward, next_state, next_action, done) #sarsaç®—æ³•æ›´æ–°Qè¡¨æ ¼
        action = next_action
        state  = next_state
        total_reward += reward  #ç´¯åŠ å¥–åŠ±                    
        if is_render:env.render()
        if done:break
    return total_reward
```

**Agentå®šä¹‰**

```python
#Qåæ ‡æ¨ªåæ ‡ä¸ºåŠ¨ä½œé€‰æ‹©ï¼Œæ¯”å¦‚ left right down up ï¼Œçºµåæ ‡ä¸ºçŠ¶æ€å¯¹åº”ä¸åŒçš„çŠ¶æ€å¯¹åº”ä¸åŒçš„æ ¼å­
class SarsaAgent():
    def __init__(self,n_states,n_act,e_greed=0.1,lr=0.1,gamma=0.9) :
        #å®šä¹‰å‚æ•°
        self.e_greed = e_greed # æ¢ç´¢ä¸åˆ©ç”¨ä¸­çš„æ¢ç´¢æ¦‚ç‡
        self.n_states = n_states # çŠ¶æ€æ•°é‡
        self.n_act = n_act  # åŠ¨ä½œæ•°é‡
        self.lr = lr        # å­¦ä¹ ç‡
        self.gamma = gamma # æ”¶ç›Šè¡°å‡ç‡
        #åˆå§‹åŒ–ä¸€ä¸ªå€¼å…¨ä¸º0çš„Qè¡¨æ ¼
        self.Q = np.zeros((n_states,n_act))

    def predict(self,state): #ä»Qè¡¨æ ¼é€‰å‡ºä»·å€¼æœ€å¤§çš„action
        Q_list = self.Q[state,:]
        action =  np.random.choice(np.flatnonzero(Q_list==Q_list.max())) #è‹¥æœ€å¤§å€¼ä¸æ­¢ä¸€ä¸ªï¼Œåˆ™éšæœºé‡‡æ ·
        return action

    def act(self,state):   #Îµ-è´ªå¿ƒ ç®—æ³•çš„å®ç°
        if np.random.uniform(0,1)<self.e_greed: #æ¢ç´¢ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ªaction
            action = np.random.choice(self.n_act)
        else: #åˆ©ç”¨åˆ©ç”¨predict ç­›é€‰å‡ºä¸€ä¸ªrewardæœ€å¤§çš„action
            action = self.predict(state)
        return action

    def learn(self,state,action,reward,next_state,next_action,done): #sarsaç®—æ³• æ›´æ–°Qè¡¨æ ¼

        cur_Q = self.Q[state , action]  #å–å½“å‰åŠ¨ä½œå’ŒçŠ¶æ€çš„Qè¡¨æ ¼çš„å€¼
        if done:
            target_Q = reward           #åˆ¤æ–­æ­¤è½®è®­ç»ƒæ˜¯å¦ç»“æŸ
        else:
            target_Q = reward + self.gamma*self.Q[next_state,next_action] #sarsaç®—æ³•

        self.Q[state,action] += self.lr * (target_Q-cur_Q)   #æ›´æ–°Qè¡¨æ ¼
```

## Q-Learning

### ç®—æ³•åŸç†

<img src="https://pica.zhimg.com/v2-acde8c12a73ca67ec81fac72b1fd8923_1440w.jpg?source=172ae18b" alt="æŸ¥çœ‹æºå›¾åƒ" style="zoom:67%;" />

å¯¹äº$Q-Learning$ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨Ïµâˆ’è´ªå©ªæ³•æ¥é€‰æ‹©æ–°çš„åŠ¨ä½œï¼Œè¿™éƒ¨åˆ†å’Œ$SARSA$å®Œå…¨ç›¸åŒã€‚ä½†æ˜¯å¯¹äºä»·å€¼å‡½æ•°çš„æ›´æ–°ï¼Œ$Q-Learning$ä½¿ç”¨çš„æ˜¯è´ªå©ªæ³•ï¼Œè€Œä¸æ˜¯$SARSA$çš„Ïµâˆ’è´ªå©ªæ³•ã€‚è¿™ä¸€ç‚¹å°±æ˜¯$SARSA$å’Œ$Q-Learning$æœ¬è´¨çš„åŒºåˆ«ã€‚

![img](https://img2018.cnblogs.com/blog/1042406/201809/1042406-20180918202423478-583844904.jpg)

é¦–å…ˆæˆ‘ä»¬åŸºäºçŠ¶æ€$S$ï¼Œç”¨Ïµâˆ’è´ªå©ªæ³•é€‰æ‹©åˆ°åŠ¨ä½œ$A$, ç„¶åæ‰§è¡ŒåŠ¨ä½œ$A$ï¼Œå¾—åˆ°å¥–åŠ±$R$ï¼Œå¹¶è¿›å…¥çŠ¶æ€$S^â€²$ï¼Œæ­¤æ—¶ï¼Œå¦‚æœæ˜¯$SARSA$ï¼Œä¼šç»§ç»­åŸºäºçŠ¶æ€$Sâ€²$ï¼Œç”¨Ïµâˆ’è´ªå©ªæ³•é€‰æ‹©$A^â€²$,ç„¶åæ¥æ›´æ–°ä»·å€¼å‡½æ•°ã€‚ä½†æ˜¯$Q-Learning$åˆ™ä¸åŒã€‚

å¯¹äº$Q-Learning$ï¼Œå®ƒåŸºäºçŠ¶æ€$S^â€²$ï¼Œæ²¡æœ‰ä½¿ç”¨Ïµâˆ’è´ªå©ªæ³•é€‰æ‹©$A^â€²$ï¼Œè€Œæ˜¯ä½¿ç”¨è´ªå©ªæ³•é€‰æ‹©$A^â€²$ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œé€‰æ‹©ä½¿$Q(S^â€²,a)$æœ€å¤§çš„$a$ä½œä¸º$A^â€²$æ¥æ›´æ–°ä»·å€¼å‡½æ•°ã€‚ç”¨æ•°å­¦å…¬å¼è¡¨ç¤ºå°±æ˜¯ï¼š

â€‹																					$Q(S,A)=Q(S,A)+Î±(R+Î³\mathop{max}\limits_{a}Q(S^â€²,a)âˆ’Q(S,A))$

å¯¹åº”åˆ°ä¸Šå›¾ä¸­å°±æ˜¯åœ¨å›¾ä¸‹æ–¹çš„ä¸‰ä¸ªé»‘åœ†åœˆåŠ¨ä½œä¸­é€‰æ‹©ä¸€ä¸ªä½¿$Q(S^â€²,a)$æœ€å¤§çš„åŠ¨ä½œä½œä¸º$A^â€²$ã€‚

æ­¤æ—¶é€‰æ‹©çš„åŠ¨ä½œåªä¼šå‚ä¸ä»·å€¼å‡½æ•°çš„æ›´æ–°ï¼Œä¸ä¼šçœŸæ­£çš„æ‰§è¡Œã€‚ä»·å€¼å‡½æ•°æ›´æ–°åï¼Œæ–°çš„æ‰§è¡ŒåŠ¨ä½œéœ€è¦åŸºäºçŠ¶æ€$S^â€²$ï¼Œç”¨Ïµâˆ’è´ªå©ªæ³•é‡æ–°é€‰æ‹©å¾—åˆ°ã€‚è¿™ä¸€ç‚¹ä¹Ÿå’Œ$SARSA$ç¨æœ‰ä¸åŒã€‚å¯¹äº$SARSA$ï¼Œä»·å€¼å‡½æ•°æ›´æ–°ä½¿ç”¨çš„$A^â€²$ä¼šä½œä¸ºä¸‹ä¸€é˜¶æ®µå¼€å§‹æ—¶å€™çš„æ‰§è¡ŒåŠ¨ä½œã€‚

### ä»£ç å®ç°

**Learn å‡½æ•°ä¿®æ”¹**

```python
    def learn(self,state,action,reward,next_state,done): #Q-Learning æ›´æ–°Qè¡¨æ ¼
        cur_Q = self.Q[state , action]
        if done:
            target_Q = reward
        else:
            target_Q = reward + self.gamma*self.Q[next_state,:].max()

        self.Q[state,action] += self.lr * (target_Q-cur_Q)
```

**è®­ç»ƒä¸€è½®ä¿®æ”¹**

```python
#è®­ç»ƒä¸€è½®æ¸¸æˆ
def train_episode(env,agent,is_render):
    total_reward = 0
    #é‡ç½®ç¯å¢ƒ
    state = env.reset()
    while True:
        action = agent.act(state) #action
        next_state ,reward,done,_ = env.step(action) #ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’
        agent.learn(state, action, reward, next_state,done) #q_learningç®—æ³•æ›´æ–°Qè¡¨æ ¼
        state  = next_state
        total_reward += reward
        if is_render:env.render()
        if done:break
    return total_reward
```

$Q-Learning$ç›´æ¥å­¦ä¹ çš„æ˜¯æœ€ä¼˜ç­–ç•¥ï¼Œè€Œ$SARSA$åœ¨å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„åŒæ—¶è¿˜åœ¨åšæ¢ç´¢ã€‚è¿™å¯¼è‡´æˆ‘ä»¬åœ¨å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„æ—¶å€™ï¼Œå¦‚æœç”¨$SARSA$ï¼Œä¸ºäº†ä¿è¯æ”¶æ•›ï¼Œéœ€è¦åˆ¶å®šä¸€ä¸ªç­–ç•¥ï¼Œä½¿Ïµâˆ’è´ªå©ªæ³•çš„è¶…å‚æ•°Ïµåœ¨è¿­ä»£çš„è¿‡ç¨‹ä¸­é€æ¸å˜å°ã€‚$Q-Learning$æ²¡æœ‰è¿™ä¸ªçƒ¦æ¼ã€‚

å¦å¤–ä¸€ä¸ªå°±æ˜¯$Q-Learning$ç›´æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼Œä½†æ˜¯æœ€ä¼˜ç­–ç•¥ä¼šä¾èµ–äºè®­ç»ƒä¸­äº§ç”Ÿçš„ä¸€ç³»åˆ—æ•°æ®ï¼Œæ‰€ä»¥å—æ ·æœ¬æ•°æ®çš„å½±å“è¾ƒå¤§ï¼Œå› æ­¤å—åˆ°è®­ç»ƒæ•°æ®æ–¹å·®çš„å½±å“å¾ˆå¤§ï¼Œç”šè‡³ä¼šå½±å“$Q$å‡½æ•°çš„æ”¶æ•›ã€‚$Q-Learning$çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç‰ˆ$Deep Q-Learning$ä¹Ÿæœ‰è¿™ä¸ªé—®é¢˜ã€‚

åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œ$SARSA$åœ¨æ”¶æ•›çš„è¿‡ç¨‹ä¸­é¼“åŠ±æ¢ç´¢ï¼Œè¿™æ ·å­¦ä¹ è¿‡ç¨‹ä¼šæ¯”è¾ƒå¹³æ»‘ï¼Œä¸è‡³äºè¿‡äºæ¿€è¿›ï¼Œå¯¼è‡´å‡ºç°åƒ$Q-Learning$å¯èƒ½é‡åˆ°ä¸€äº›ç‰¹æ®Šçš„æœ€ä¼˜â€œé™·é˜±â€ã€‚æ¯”å¦‚ç»å…¸çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜"Cliff Walk"ã€‚

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¦‚æœæˆ‘ä»¬æ˜¯åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œæ¨èä½¿ç”¨$Q-Learning$ï¼Œå¦‚æœæ˜¯åœ¨çº¿ç”Ÿäº§ç¯å¢ƒä¸­è®­ç»ƒæ¨¡å‹ï¼Œåˆ™æ¨èä½¿ç”¨$SARSA$ã€‚

## DQN 

### ç®—æ³•åŸç†

![æŸ¥çœ‹æºå›¾åƒ](https://ts1.cn.mm.bing.net/th/id/R-C.121dbec54d4e1b89ea27f8623faa215f?rik=Q02DV0xDeQYXnQ&riu=http%3a%2f%2fstatic.zybuluo.com%2fWuLiangchao%2f8gbw5uxcymp969jhi7etlbrc%2fimage_1cd2kiaol19ft2kb1dr4ricnorm.png&ehk=FTH8J4%2b4T9Ch68X3%2b%2fq0USVSKqVKc4wdSSvw1pGzxE8%3d&risl=&pid=ImgRaw&r=0)

å¯¹äº$SARSA$å’Œ$Q-Learning$æ¥è¯´ï¼Œä½¿ç”¨çš„çŠ¶æ€éƒ½æ˜¯ç¦»æ•£çš„æœ‰é™ä¸ªçŠ¶æ€é›†åˆï¼Œæ­¤æ—¶é—®é¢˜çš„è§„æ¨¡æ¯”è¾ƒå°ï¼Œæ¯”è¾ƒå®¹æ˜“æ±‚è§£ã€‚å½“é‡åˆ°å¤æ‚çš„çŠ¶æ€é›†åˆæ—¶ï¼Œæ— æ³•åœ¨å†…å­˜ä¸­ç»´æŠ¤è¿™ä¹ˆå¤§çš„ä¸€å¼ $Q$è¡¨ã€‚

ç”±äºé—®é¢˜çš„çŠ¶æ€é›†åˆè§„æ¨¡å¤§ï¼Œä¸€ä¸ªå¯è¡Œçš„å»ºæ¨¡æ–¹æ³•æ˜¯ä»·å€¼å‡½æ•°çš„è¿‘ä¼¼è¡¨ç¤ºã€‚æ–¹æ³•æ˜¯æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªçŠ¶æ€ä»·å€¼å‡½æ•°$v$, è¿™ä¸ªå‡½æ•°ç”±å‚æ•°$w$æè¿°ï¼Œå¹¶æ¥å—çŠ¶æ€$s$ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—åå¾—åˆ°çŠ¶æ€$s$çš„ä»·å€¼ï¼Œå³æˆ‘ä»¬æœŸæœ›ï¼š

â€‹																													$v(s,w)â‰ˆv_Ï€(s)$

ç±»ä¼¼çš„ï¼Œå¼•å…¥ä¸€ä¸ªåŠ¨ä½œä»·å€¼å‡½æ•°$q$ï¼Œè¿™ä¸ªå‡½æ•°ç”±å‚æ•°$w$æè¿°ï¼Œå¹¶æ¥å—çŠ¶æ€$s$ä¸åŠ¨ä½œ$a$ä½œä¸ºè¾“å…¥ï¼Œè®¡ç®—åå¾—åˆ°åŠ¨ä½œä»·å€¼ï¼Œå³æˆ‘ä»¬æœŸæœ›ï¼š

â€‹																													$q(s,a,w)â‰ˆq_Ï€(s,a)$

ä»·å€¼å‡½æ•°è¿‘ä¼¼çš„æ–¹æ³•å¾ˆå¤š,ç”¨å†³ç­–æ ‘ï¼Œæœ€è¿‘é‚»ï¼Œå‚…é‡Œå¶å˜æ¢ï¼Œç¥ç»ç½‘ç»œæ¥è¡¨è¾¾æˆ‘ä»¬çš„çŠ¶æ€ä»·å€¼å‡½æ•°ã€‚è€Œæœ€å¸¸è§ï¼Œåº”ç”¨æœ€å¹¿æ³›çš„è¡¨ç¤ºæ–¹æ³•æ˜¯ç¥ç»ç½‘ç»œï¼Œå¯¹äºç¥ç»ç½‘ç»œï¼Œå¯ä»¥ä½¿ç”¨$DNN$ï¼Œ$CNN$æˆ–è€…$RNN$ã€‚æ²¡æœ‰ç‰¹åˆ«çš„é™åˆ¶ã€‚å¦‚æœæŠŠæˆ‘ä»¬è®¡ç®—ä»·å€¼å‡½æ•°çš„ç¥ç»ç½‘ç»œçœ‹åšä¸€ä¸ªé»‘ç›’å­ï¼Œé‚£ä¹ˆæ•´ä¸ªè¿‘ä¼¼è¿‡ç¨‹å¯ä»¥çœ‹åšä¸‹é¢è¿™ä¸‰ç§æƒ…å†µï¼š

<img src="https://img2018.cnblogs.com/blog/1042406/201809/1042406-20180928142605652-445522913.jpg" alt="img" style="zoom:80%;" />

å¯¹äºçŠ¶æ€ä»·å€¼å‡½æ•°ï¼Œç¥ç»ç½‘ç»œçš„è¾“å…¥æ˜¯çŠ¶æ€$s$çš„ç‰¹å¾å‘é‡ï¼Œè¾“å‡ºæ˜¯çŠ¶æ€ä»·å€¼$v(s,w)$ã€‚å¯¹äºåŠ¨ä½œä»·å€¼å‡½æ•°ï¼Œæœ‰ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§æ˜¯è¾“å…¥çŠ¶æ€$s$çš„ç‰¹å¾å‘é‡å’ŒåŠ¨ä½œ$a$ï¼Œè¾“å‡ºå¯¹åº”çš„åŠ¨ä½œä»·å€¼$q(s,a,w)$ï¼Œå¦ä¸€ç§æ˜¯åªè¾“å…¥çŠ¶æ€$s$çš„ç‰¹å¾å‘é‡ï¼ŒåŠ¨ä½œé›†åˆæœ‰å¤šå°‘ä¸ªåŠ¨ä½œå°±æœ‰å¤šå°‘ä¸ªè¾“å‡º$q(s,a_i,w)$ã€‚è¿™é‡Œéšå«äº†æˆ‘ä»¬çš„åŠ¨ä½œæ˜¯æœ‰é™ä¸ªçš„ç¦»æ•£åŠ¨ä½œã€‚

$Deep Q-Learning$ç®—æ³•çš„åŸºæœ¬æ€è·¯æ¥æºäº$Q-Learning$ã€‚ä½†æ˜¯å’Œ$Q-Learning$ä¸åŒçš„åœ°æ–¹åœ¨äºï¼Œå®ƒçš„$Q$å€¼çš„è®¡ç®—ä¸æ˜¯ç›´æ¥é€šè¿‡çŠ¶æ€å€¼$s$å’ŒåŠ¨ä½œæ¥è®¡ç®—ï¼Œè€Œæ˜¯é€šè¿‡ä¸Šé¢è®²åˆ°çš„$Q$ç½‘ç»œæ¥è®¡ç®—çš„ã€‚è¿™ä¸ª$Q$ç½‘ç»œæ˜¯ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ä¸€èˆ¬ç®€ç§°$Deep Q-Learning$ä¸º$DQN$ã€‚

$DQN$çš„è¾“å…¥æ˜¯æˆ‘ä»¬çš„çŠ¶æ€$s$å¯¹åº”çš„çŠ¶æ€å‘é‡$Ï•(s)$ï¼Œ è¾“å‡ºæ˜¯æ‰€æœ‰åŠ¨ä½œåœ¨è¯¥çŠ¶æ€ä¸‹çš„åŠ¨ä½œä»·å€¼å‡½æ•°$Q$ã€‚$Q$ç½‘ç»œå¯ä»¥æ˜¯$DNN$ï¼Œ$CNN$æˆ–è€…$RNN$ï¼Œæ²¡æœ‰å…·ä½“çš„ç½‘ç»œç»“æ„è¦æ±‚ã€‚

$DQN$ä¸»è¦ä½¿ç”¨çš„æŠ€å·§æ˜¯**ç»éªŒå›æ”¾ï¼ˆexperience replayï¼‰**,å³å°†æ¯æ¬¡å’Œç¯å¢ƒäº¤äº’å¾—åˆ°çš„å¥–åŠ±ä¸çŠ¶æ€æ›´æ–°æƒ…å†µéƒ½ä¿å­˜èµ·æ¥ï¼Œç”¨äºåé¢ç›®æ ‡$Q$å€¼çš„æ›´æ–°ã€‚ä¸ºä»€ä¹ˆéœ€è¦ç»éªŒå›æ”¾å‘¢ï¼Ÿæˆ‘ä»¬å›å¿†ä¸€ä¸‹$Q-Learning$ï¼Œå®ƒæ˜¯æœ‰ä¸€å¼ $Q$è¡¨æ¥ä¿å­˜æ‰€æœ‰çš„$Q$å€¼çš„å½“å‰ç»“æœçš„ï¼Œä½†æ˜¯$DQN$æ˜¯æ²¡æœ‰çš„ï¼Œé‚£ä¹ˆåœ¨åšåŠ¨ä½œä»·å€¼å‡½æ•°æ›´æ–°çš„æ—¶å€™ï¼Œå°±éœ€è¦å…¶ä»–çš„æ–¹æ³•ï¼Œè¿™ä¸ªæ–¹æ³•å°±æ˜¯ç»éªŒå›æ”¾ã€‚

### ä»£ç å®ç°

**ç¥ç»ç½‘ç»œå®šä¹‰**

```python
import torch
class MLP(torch.nn.Module):

    def __init__(self, obs_size,n_act):
        #super()ç”¨æ¥è°ƒç”¨çˆ¶ç±»(åŸºç±»)çš„æ–¹æ³•ï¼Œ__init__()æ˜¯ç±»çš„æ„é€ æ–¹æ³•
        #super().__init__() å°±æ˜¯è°ƒç”¨çˆ¶ç±»çš„initæ–¹æ³•ï¼Œ åŒæ ·å¯ä»¥ä½¿ç”¨super()å»è°ƒç”¨çˆ¶ç±»çš„å…¶ä»–æ–¹æ³•ã€‚
        super().__init__()
        self.mlp = self.__mlp(obs_size,n_act)

    def __mlp(self,obs_size,n_act):
        return torch.nn.Sequential(
            torch.nn.Linear(obs_size, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, 50),
            torch.nn.ReLU(),
            torch.nn.Linear(50, n_act)
        )
        
    def forward(self, x):
        return self.mlp(x)
```

**æ¢ç´¢æ¦‚ç‡è¡°å‡**

$ğœ€$çš„å€¼å¯éšç€æ™ºèƒ½ä½“ä¸ç¯å¢ƒçš„äº¤äº’æ¬¡æ•°å¢å¤šè€Œå‡å°‘ï¼Œä¾‹å¦‚è®¾å®šä¸€ä¸ªğœ€è¡°å‡å€¼$ğœ€_{ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦}$ã€‚åˆ™æ¯ä¸€æ¬¡$ğœ€$çš„æ›´æ–°å¯è¡¨è¾¾ä¸º

â€‹																									$ğœ€=ğœ€âˆ’ğœ€_{ğ‘‘ğ‘’ğ‘ğ‘ğ‘¦}$

```python
class EpsilonGreedy():

    def __init__( self,n_act, e_greed, decay_rate):
        self.n_act = n_act  # åŠ¨ä½œæ•°é‡
        self.epsilon = e_greed  # æ¢ç´¢ä¸åˆ©ç”¨ä¸­çš„æ¢ç´¢æ¦‚ç‡
        self.decay_rate = decay_rate # è¡°å‡å€¼

    def act(self,predct_method,obs):
        if np.random.uniform(0, 1) < self.epsilon:  #æ¢ç´¢
            action = np.random.choice(self.n_act)
        else: # åˆ©ç”¨
            action = predct_method(obs)
        self.epsilon = max(0.01,self.epsilon-self.decay_rate) 
        return action
```

**ç»éªŒæ± æ„å»º**

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221117171752738.png" alt="image-20221117171752738" style="zoom: 50%;" />

æŒ‡è®¾å®šä¸€ä¸ªç»éªŒæ± , å°†æ¯ä¸€æ­¥äº¤äº’ç¼“å­˜è¿›ç»éªŒæ± ï¼Œç§¯æ”’åˆ°ä¸€å®šç¨‹åº¦åå¯ä»¥æ¯ä¸€æ¬¡å–å‡ºBatch Sizeä¸ªâ€œç»éªŒâ€ ä»è€Œè¿›è¡Œæ‰¹é‡å­¦ä¹ ã€‚

æ³¨æ„äº‹é¡¹ï¼š

- é¢‘æ‰¹åˆ†å¼€ï¼šå­¦ä¹ é¢‘æ¬¡ä¸æ¯æ¬¡å­¦ä¹ çš„â€œç»éªŒâ€æ•°é‡ï¼Œ(Batch Size)æ˜¯ä¸åŒçš„ã€‚ä¾‹å¦‚å¯è®¾å®šä¸ºæ¯4è½®äº¤äº’è¿›è¡Œä¸€æ¬¡å­¦ä¹ ï¼Œæ¯æ¬¡å­¦ä¹ ä»ç»éªŒæ± ä¸­å–å‡º32è½®äº¤äº’ç»éªŒã€‚

-  å»¶è¿Ÿå¯åŠ¨ï¼šå‰Nè½®çš„äº¤äº’å¹¶ä¸è¿›è¡Œå­¦ä¹ ï¼Œç­‰ç»éªŒæ± ä¸­çš„ç»éªŒç§¯æ”’åˆ°ä¸€å®šç¨‹åº¦åå†å¼€å§‹å­¦ä¹ ã€‚

ç»éªŒæ± çš„å¥½å¤„ï¼š

1.	æé«˜æ ·æœ¬åˆ©ç”¨ç‡
2.	æ‰“ä¹±æ ·æœ¬å…³è”æ€§
â¢å› ä¸ºæ™®é€šçš„æœºå™¨å­¦ä¹ æ ·æœ¬ä¹‹é—´çš„å…³ç³»éƒ½æ˜¯ç‹¬ç«‹çš„ã€‚è€Œæ™ºèƒ½ä½“ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„ç»éªŒæ ·æœ¬å¦‚æœä¸ç»è¿‡å¤„ç†åˆ™å­˜åœ¨åºåˆ—æ ·æœ¬å…³è”æ€§ï¼Œè¿™å¯¹æ¨¡å‹çš„æ›´æ–°ä¸åˆ©ã€‚

```python
import random
import collections
from torch import FloatTensor

class ReplayBuffer(object):
    def __init__(self, max_size, num_steps=1 ):
        #deque åŒå‘é˜Ÿåˆ— ç±»ä¼¼äºlistçš„å®¹å™¨ï¼Œå¯ä»¥å¿«é€Ÿçš„åœ¨é˜Ÿåˆ—å¤´éƒ¨å’Œå°¾éƒ¨æ·»åŠ ã€åˆ é™¤å…ƒç´ 
        #å½“åŠ å…¥çš„å…ƒç´ è¶…è¿‡å®¹å™¨å®¹é‡æ—¶ï¼Œä¼šåˆ é™¤æœ€åˆæ·»åŠ çš„å…ƒç´ ï¼Œç„¶åæ’å…¥æ–°çš„å…ƒç´  å…ˆå…¥å…ˆå‡º
        self.buffer = collections.deque(maxlen=max_size)
        self.num_steps  = num_steps
       
    def append(self, exp):
        #å‘å®¹å™¨ä¸­æ·»åŠ å…ƒç´ ï¼Œå¯æ·»åŠ å¤šç»´å‘é‡
        self.buffer.append(exp)

    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        #zip(*) å¯ç†è§£ä¸ºè§£å‹ ï¼Œ5ç»´å‘é‡
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*mini_batch)
        #è½¬æ¢ä¸ºtorchçš„tensorå¼ é‡
        obs_batch = FloatTensor(obs_batch)
        action_batch = FloatTensor(action_batch)
        reward_batch = FloatTensor(reward_batch)
        next_obs_batch = FloatTensor(next_obs_batch)
        done_batch = FloatTensor(done_batch)
        return obs_batch,action_batch,reward_batch,next_obs_batch,done_batch

    def __len__(self):
        #è¿”å›bufferçš„é•¿åº¦
        return len(self.buffer)
```

**DQNAgentå®šä¹‰**

å›ºå®šQç›®æ ‡ï¼ˆNature DQN)

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221117172238707.png" alt="image-20221117172238707" style="zoom: 50%;" />

- é¦–å…ˆå°†Qå‡½æ•°å¤åˆ¶ä¸€ä»½ä½œä¸ºç›®æ ‡Qå‡½æ•°ã€‚åŸå…ˆçš„Qå‡½æ•°åˆ™ç§°ä¸ºé¢„æµ‹Qå‡½æ•°ã€‚

- Predict Qç”±é¢„æµ‹Qå‡½æ•°å¾—åˆ°ï¼ŒTarget Qç”±ç›®æ ‡Qå‡½æ•°å¾—åˆ°ã€‚

- æ¨¡å‹å­¦ä¹ çš„è¿‡ç¨‹ä¸­ä»…è¿­ä»£æ›´æ–°é¢„æµ‹Q å‡½æ•°çš„æ¨¡å‹å‚æ•°ã€‚ç›®æ ‡Qå‡½æ•°çš„æ¨¡å‹å‚æ•°å›ºå®šä¸å˜ã€‚

- æ¯éš”ä¸€å®šæ¬¡æ•°å°†é¢„æµ‹Qå‡½æ•°çš„æ¨¡å‹å‚æ•°åŒæ­¥ç»™ç›®æ ‡Qå‡½æ•°ã€‚

```python
import sys
import torch
#æ·»åŠ åº“è·¯å¾„ï¼Œä¸Šä¸¤çº§æ–‡ä»¶å¤¹
sys.path.append("../..")
from utils import torchUtils
import copy

class DQNAgent(object):

    def __init__(self,q_func, optimizer, explorer,replay_buffer, batch_size, replay_start_size,update_target_steps, n_act, gamma=0.9):
        '''
        :param q_func:  Qå‡½æ•°
        :param optimizer: ä¼˜åŒ–å™¨
        :param explorer: æ¢ç´¢å™¨
        :param replay_buffer: ç»éªŒå›æ”¾å™¨
        :param batch_size: æ‰¹æ¬¡æ•°é‡
        :param replay_start_size: å¼€å§‹å›æ”¾çš„æ¬¡æ•°
        :param update_target_steps: åŒæ­¥å‚æ•°çš„æ¬¡æ•°
        :param n_act: åŠ¨ä½œæ•°é‡
        :param gamma: æ”¶ç›Šè¡°å‡ç‡
        '''
        self.pred_func = q_func   #Qå‡½æ•°
        self.target_func = copy.deepcopy(q_func)  #target_Qå‡½æ•°
        self.update_target_steps = update_target_steps #åŒæ­¥å‚æ•°çš„æ¬¡æ•°

        self.explorer = explorer  #æ¢ç´¢å™¨

        self.rb = replay_buffer   #ç»éªŒæ± 
        self.batch_size = batch_size #æ‰¹æ¬¡æ•°é‡
        self.replay_start_size = replay_start_size #å¼€å§‹å›æ”¾çš„æ¬¡æ•°

        self.optimizer = optimizer  #ç¥ç»ç½‘ç»œä¼˜åŒ–å™¨
        self.criterion = torch.nn.MSELoss() #ç¥ç»ç½‘ç»œæŸå¤±å‡½æ•°

        self.global_step = 0
        self.gamma = gamma  # æ”¶ç›Šè¡°å‡ç‡
        self.n_act = n_act # åŠ¨ä½œæ•°é‡

    # æ ¹æ®ç»éªŒå¾—åˆ°action
    def predict(self, obs):
        obs = torch.FloatTensor(obs) #è½¬æ¢ä¸ºtensorå‘é‡
        Q_list = self.pred_func(obs) #æ ¹æ®çŠ¶æ€é¢„æµ‹å¾—åˆ°actionä»·å€¼æœ€å¤§çš„action
        action = int(torch.argmax(Q_list).detach().numpy()) #é€‰å–
        return action

    # æ ¹æ®æ¢ç´¢ä¸åˆ©ç”¨å¾—åˆ°action
    def act(self, obs):
        return self.explorer.act(self.predict,obs)

    def learn_batch(self, batch_obs, batch_action, batch_reward, batch_next_obs, batch_done):
        # predict_Q
        pred_Vs = self.pred_func(batch_obs)
        action_onehot = torchUtils.one_hot(batch_action, self.n_act)
        predict_Q = (pred_Vs * action_onehot).sum(1)
        # target_Q
        next_pred_Vs = self.target_func(batch_next_obs)
        best_V = next_pred_Vs.max(1)[0]
        target_Q = batch_reward + (1 - batch_done) * self.gamma * best_V

        self.optimizer.zero_grad()  # æ¢¯åº¦å½’0
        loss = self.criterion(predict_Q, target_Q)#è®¡ç®—å‡æ–¹å·®
        loss.backward() #åå‘ä¼ æ’­
        self.optimizer.step() #ä¼˜åŒ–å‚æ•°

    def learn(self, obs, action, reward, next_obs, done):
        self.global_step += 1
        self.rb.append((obs, action, reward, next_obs, done)) 
        if len(self.rb) > self.replay_start_size and self.global_step % self.rb.num_steps == 0:
            #ä»ç»éªŒæ± éšæœºé€‰å–ä¸€batchæ•°æ®è¿›è¡Œè®­ç»ƒ
            self.learn_batch(*self.rb.sample(self.batch_size))
        if self.global_step % self.update_target_steps==0:
            #æ¯éš”update_target_stepsæ­¥ ä¸ºTargetQæ›´æ–°å‚æ•°
            self.sync_target()

    def sync_target(self):
        #ç¥ç»ç½‘ç»œå‚æ•°æ‹·è´
        for target_param, param in zip(self.target_func.parameters(), self.pred_func.parameters()):
            target_param.data.copy_(param.data)
```

**æ¨¡å‹è®­ç»ƒ**

```python
import agents,modules,replay_buffers,explorers
import gym
import torch

class TrainManager():

    def __init__(self,
                env,#ç¯å¢ƒ
                episodes=1000,#è½®æ¬¡æ•°é‡
                batch_size = 32,#æ¯ä¸€æ‰¹æ¬¡çš„æ•°é‡
                num_steps=4,#è¿›è¡Œå­¦ä¹ çš„é¢‘æ¬¡
                memory_size = 2000,#ç»éªŒå›æ”¾æ± çš„å®¹é‡
                replay_start_size = 200,#å¼€å§‹å›æ”¾çš„æ¬¡æ•°
                update_target_steps = 200,#åŒæ­¥å‚æ•°çš„æ¬¡æ•°
                lr=0.001,#å­¦ä¹ ç‡
                gamma=0.9, #æ”¶ç›Šè¡°å‡ç‡
                e_greed=0.1, #æ¢ç´¢ä¸åˆ©ç”¨ä¸­çš„æ¢ç´¢æ¦‚ç‡
                e_gredd_decay = 1e-6 #æ¢ç´¢ä¸åˆ©ç”¨ä¸­æ¢ç´¢æ¦‚ç‡çš„è¡°å‡æ­¥é•¿
                ):

        n_act = env.action_space.n
        n_obs = env.observation_space.shape[0]

        self.env = env
        self.episodes = episodes
		#æ¢ç´¢ç‡
        explorer =  explorers.EpsilonGreedy(n_act,e_greed,e_gredd_decay)
        #ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°
        q_func = modules.MLP(n_obs, n_act)
        #ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(q_func.parameters(), lr=lr)
        #ç»éªŒå›æ”¾æ± 
        rb = replay_buffers.ReplayBuffer(memory_size, num_steps)
		#DQNAgentå‚æ•°å®šä¹‰
        self.agent = agents.DQNAgent(
            q_func=q_func,
            optimizer=optimizer,
            explorer=explorer,
            replay_buffer = rb,
            batch_size=batch_size,
            replay_start_size = replay_start_size,
            update_target_steps = update_target_steps,
            n_act=n_act,
            gamma=gamma)

    def train_episode(self):
        total_reward = 0
        obs = self.env.reset()
        while True:
            action = self.agent.act(obs)
            next_obs, reward, done, _ = self.env.step(action)
            total_reward += reward
            self.agent.learn(obs, action, reward, next_obs, done)
            obs = next_obs
            if done: break
        print('e_greedy =',self.agent.explorer.epsilon)
        return total_reward

    def test_episode(self,is_render=False):
        total_reward = 0
        obs = self.env.reset()
        while True:
            action = self.agent.predict(obs)
            next_obs, reward, done, _ = self.env.step(action)
            total_reward += reward
            obs = next_obs
            if is_render:self.env.render()
            if done: break
        return total_reward

    def train(self):
        #è®­ç»ƒ1000lun
        for e in range(self.episodes):
            ep_reward = self.train_episode()
            print('Episode %s: reward = %.1f' % (e, ep_reward))
            if e % 100 == 0:
                test_reward = self.test_episode(False)
                print('test reward = %.1f' % (test_reward))

        # è¿›è¡Œæœ€åçš„æµ‹è¯•
        total_test_reward = 0
        for i in range(5):
            total_test_reward += self.test_episode(False)
        print('final test reward = %.1f' % (total_test_reward/5))

if __name__ == '__main__':
    env1 = gym.make("CartPole-v0")
    tm = TrainManager(env1)
    tm.train()
```

## Double DQN

### ç®—æ³•åŸç†

$Nature DQN$å®ƒé€šè¿‡ä½¿ç”¨ä¸¤ä¸ªç›¸åŒçš„ç¥ç»ç½‘ç»œï¼Œä»¥è§£å†³æ•°æ®æ ·æœ¬å’Œç½‘ç»œè®­ç»ƒä¹‹å‰çš„ç›¸å…³æ€§ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰å…¶ä»–å€¼å¾—ä¼˜åŒ–çš„ç‚¹ã€‚åœ¨$DDQN$ä¹‹å‰ï¼ŒåŸºæœ¬ä¸Šæ‰€æœ‰çš„ç›®æ ‡$Q$å€¼éƒ½æ˜¯é€šè¿‡è´ªå©ªæ³•ç›´æ¥å¾—åˆ°çš„ï¼Œæ— è®ºæ˜¯$Q-Learning$ï¼Œ $DQN(NIPS 2013)$è¿˜æ˜¯ $Nature DQN$ï¼Œéƒ½æ˜¯å¦‚æ­¤ã€‚æ¯”å¦‚å¯¹äº$Nature DQN$,è™½ç„¶ç”¨äº†ä¸¤ä¸ª$Q$ç½‘ç»œå¹¶ä½¿ç”¨ç›®æ ‡$Q$ç½‘ç»œè®¡ç®—$Q$å€¼ï¼Œå…¶ç¬¬$j$ä¸ªæ ·æœ¬çš„ç›®æ ‡$Q$å€¼çš„è®¡ç®—è¿˜æ˜¯è´ªå©ªæ³•å¾—åˆ°çš„ï¼Œåœ¨$DDQN$è¿™é‡Œï¼Œä¸å†æ˜¯ç›´æ¥åœ¨ç›®æ ‡$Q$ç½‘ç»œé‡Œé¢æ‰¾å„ä¸ªåŠ¨ä½œä¸­æœ€å¤§$Q$å€¼ï¼Œè€Œæ˜¯å…ˆåœ¨å½“å‰$Q$ç½‘ç»œä¸­å…ˆæ‰¾å‡ºæœ€å¤§$Q$å€¼å¯¹åº”çš„åŠ¨ä½œã€‚

![æŸ¥çœ‹æºå›¾åƒ](https://pic1.zhimg.com/v2-afa15663181a31fc0a9ff87951a18bc4_r.jpg)

## Prioritized Replay DQN

### ç®—æ³•åŸç†

<img src="C:/Users/timer/AppData/Roaming/Typora/typora-user-images/image-20221119115007505.png" alt="image-20221119115007505" style="zoom: 50%;" />

åœ¨$Prioritized Replay DQN$ä¹‹å‰ï¼Œæˆ‘ä»¬å·²ç»è®¨è®ºäº†å¾ˆå¤šç§$DQN$ï¼Œæ¯”å¦‚$Nature DQN$ï¼Œ $DDQN$ç­‰ï¼Œä»–ä»¬éƒ½æ˜¯é€šè¿‡ç»éªŒå›æ”¾æ¥é‡‡æ ·ï¼Œè¿›è€Œåšç›®æ ‡$Q$å€¼çš„è®¡ç®—çš„ã€‚åœ¨é‡‡æ ·çš„æ—¶å€™ï¼Œæˆ‘ä»¬æ˜¯ä¸€è§†åŒä»ï¼Œåœ¨ç»éªŒå›æ”¾æ± é‡Œé¢çš„æ‰€æœ‰çš„æ ·æœ¬éƒ½æœ‰ç›¸åŒçš„è¢«é‡‡æ ·åˆ°çš„æ¦‚ç‡ã€‚

ä½†æ˜¯æ³¨æ„åˆ°åœ¨ç»éªŒå›æ”¾æ± é‡Œé¢çš„ä¸åŒçš„æ ·æœ¬ç”±äº$TD$è¯¯å·®çš„ä¸åŒï¼Œå¯¹æˆ‘ä»¬åå‘ä¼ æ’­çš„ä½œç”¨æ˜¯ä¸ä¸€æ ·çš„ã€‚$TD$è¯¯å·®è¶Šå¤§ï¼Œé‚£ä¹ˆå¯¹æˆ‘ä»¬åå‘ä¼ æ’­çš„ä½œç”¨è¶Šå¤§ã€‚è€Œ$TD$è¯¯å·®å°çš„æ ·æœ¬ï¼Œç”±äº$TD$è¯¯å·®å°ï¼Œå¯¹åå‘æ¢¯åº¦çš„è®¡ç®—å½±å“ä¸å¤§ã€‚åœ¨$Q$ç½‘ç»œä¸­ï¼Œ$TD$è¯¯å·®å°±æ˜¯ç›®æ ‡$Q$ç½‘ç»œè®¡ç®—çš„ç›®æ ‡$Q$å€¼å’Œå½“å‰$Q$ç½‘ç»œè®¡ç®—çš„$Q$å€¼ä¹‹é—´çš„å·®è·ã€‚

## Dueling DQN

### ç®—æ³•åŸç†

åœ¨å‰é¢è®²åˆ°çš„$DDQN$ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–ç›®æ ‡$Q$å€¼çš„è®¡ç®—æ¥ä¼˜åŒ–ç®—æ³•ï¼Œåœ¨$Prioritized Replay DQN$ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–ç»éªŒå›æ”¾æ± æŒ‰æƒé‡é‡‡æ ·æ¥ä¼˜åŒ–ç®—æ³•ã€‚è€Œåœ¨$Dueling DQN$ä¸­ï¼Œæˆ‘ä»¬å°è¯•é€šè¿‡ä¼˜åŒ–ç¥ç»ç½‘ç»œçš„ç»“æ„æ¥ä¼˜åŒ–ç®—æ³•ã€‚

å…·ä½“å¦‚ä½•ä¼˜åŒ–ç½‘ç»œç»“æ„å‘¢ï¼Ÿ$Dueling DQN$è€ƒè™‘å°†$Q$ç½‘ç»œåˆ†æˆä¸¤éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†æ˜¯ä»…ä»…ä¸çŠ¶æ€$S$æœ‰å…³ï¼Œä¸å…·ä½“è¦é‡‡ç”¨çš„åŠ¨ä½œ$A$æ— å…³ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬å«åšä»·å€¼å‡½æ•°éƒ¨åˆ†ï¼Œè®°åš$V(S,w,Î±)$,ç¬¬äºŒéƒ¨åˆ†åŒæ—¶ä¸çŠ¶æ€çŠ¶æ€$S$å’ŒåŠ¨ä½œ$A$æœ‰å…³ï¼Œè¿™éƒ¨åˆ†å«åšä¼˜åŠ¿å‡½æ•°$(Advantage Function)$éƒ¨åˆ†,è®°ä¸º$A(S,A,w,Î²)$,é‚£ä¹ˆæœ€ç»ˆæˆ‘ä»¬çš„ä»·å€¼å‡½æ•°å¯ä»¥é‡æ–°è¡¨ç¤ºä¸ºï¼š
																										$Q(S,A,w,Î±,Î²)=V(S,w,Î±)+A(S,A,w,Î²)$
å…¶ä¸­ï¼Œ$w$æ˜¯å…¬å…±éƒ¨åˆ†çš„ç½‘ç»œå‚æ•°ï¼Œè€Œ$Î±$æ˜¯ä»·å€¼å‡½æ•°ç‹¬æœ‰éƒ¨åˆ†çš„ç½‘ç»œå‚æ•°ï¼Œè€Œ$Î²$æ˜¯ä¼˜åŠ¿å‡½æ•°ç‹¬æœ‰éƒ¨åˆ†çš„ç½‘ç»œå‚æ•°ã€‚

![img](https://img2018.cnblogs.com/blog/1042406/201811/1042406-20181107202017462-788522227.png)

è€Œåœ¨$Dueling DQN$ä¸­ï¼Œæˆ‘ä»¬åœ¨åé¢åŠ äº†ä¸¤ä¸ªå­ç½‘ç»œç»“æ„ï¼Œåˆ†åˆ«å¯¹åº”ä¸Šé¢ä¸Šåˆ°ä»·æ ¼å‡½æ•°ç½‘ç»œéƒ¨åˆ†å’Œä¼˜åŠ¿å‡½æ•°ç½‘ç»œéƒ¨åˆ†ã€‚å¯¹åº”ä¸Šé¢å³å›¾æ‰€ç¤ºã€‚æœ€ç»ˆ$Q$ç½‘ç»œçš„è¾“å‡ºç”±ä»·æ ¼å‡½æ•°ç½‘ç»œçš„è¾“å‡ºå’Œä¼˜åŠ¿å‡½æ•°ç½‘ç»œçš„è¾“å‡ºçº¿æ€§ç»„åˆå¾—åˆ°ã€‚

## PolicyGradient

### ç®—æ³•åŸç†

åŸºäºç­–ç•¥çš„æ–¹æ³•é¦–å…ˆéœ€è¦å°†ç­–ç•¥å‚æ•°åŒ–ã€‚å‡è®¾ç›®æ ‡ç­–ç•¥$Ï€_Î¸$æ˜¯ä¸€ä¸ªéšæœºæ€§ç­–ç•¥ï¼Œå¹¶ä¸”å¤„å¤„å¯å¾®ï¼Œå…¶ä¸­æ˜¯å¯¹åº”çš„å‚æ•°ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªçº¿æ€§æ¨¡å‹æˆ–è€…ç¥ç»ç½‘ç»œæ¨¡å‹æ¥ä¸ºè¿™æ ·ä¸€ä¸ªç­–ç•¥å‡½æ•°å»ºæ¨¡ï¼Œè¾“å…¥æŸä¸ªçŠ¶æ€ï¼Œç„¶åè¾“å‡ºä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¦å¯»æ‰¾ä¸€ä¸ªæœ€ä¼˜ç­–ç•¥å¹¶æœ€å¤§åŒ–è¿™ä¸ªç­–ç•¥åœ¨ç¯å¢ƒä¸­çš„æœŸæœ›å›æŠ¥ã€‚æˆ‘ä»¬å°†ç­–ç•¥å­¦ä¹ çš„ç›®æ ‡å‡½æ•°å®šä¹‰ä¸º:
$$
J(Î¸) = E_{s0}[V^{Ï€Î¸}(s0)]
$$
å…¶ä¸­ï¼Œ$s0$è¡¨ç¤ºåˆå§‹çŠ¶æ€ã€‚ç°åœ¨æœ‰äº†ç›®æ ‡å‡½æ•°ï¼Œæˆ‘ä»¬å°†ç›®æ ‡å‡½æ•°å¯¹ç­–ç•¥$Î¸$æ±‚å¯¼ï¼Œå¾—åˆ°å¯¼æ•°åï¼Œå°±å¯ä»¥ç”¨æ¢¯åº¦ä¸Šå‡æ–¹æ³•æ¥æœ€å¤§åŒ–è¿™ä¸ªç›®æ ‡å‡½æ•°ï¼Œä»è€Œå¾—åˆ°æœ€ä¼˜ç­–ç•¥ã€‚

ç„¶åæˆ‘ä»¬å¯¹ç›®æ ‡å‡½æ•°æ±‚æ¢¯åº¦ï¼Œå¯ä»¥å¾—åˆ°å¦‚ä¸‹å¼å­ï¼š
$$
âˆ‡_Î¸J(Î¸)=E_{Ï€Î¸}[âˆ‡_Î¸logÏ€Î¸(s,a)Q_Ï€(s,a)]
$$
è’™ç‰¹å¡ç½—ç­–ç•¥æ¢¯åº¦$reinforce$ç®—æ³•, ä½¿ç”¨ä»·å€¼å‡½æ•°$v(s)$æ¥è¿‘ä¼¼ä»£æ›¿ç­–ç•¥æ¢¯åº¦å…¬å¼é‡Œé¢çš„$QÏ€(s,a)$ã€‚ç®—æ³•çš„æµç¨‹å¾ˆç®€å•ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

ã€€ã€€ã€€ã€€è¾“å…¥ï¼šNä¸ªè’™ç‰¹å¡ç½—å®Œæ•´åºåˆ—,è®­ç»ƒæ­¥é•¿$Î±$

ã€€ã€€ã€€ã€€è¾“å‡ºï¼šç­–ç•¥å‡½æ•°çš„å‚æ•°$Î¸$

ã€€1. for æ¯ä¸ªè’™ç‰¹å¡ç½—åºåˆ—:

ã€€ã€€ã€€ã€€a. ç”¨è’™ç‰¹å¡ç½—æ³•è®¡ç®—åºåˆ—æ¯ä¸ªæ—¶é—´ä½ç½®tçš„çŠ¶æ€ä»·å€¼$v_t$

ã€€ã€€ã€€ã€€b. å¯¹åºåˆ—æ¯ä¸ªæ—¶é—´ä½ç½®$t$ï¼Œä½¿ç”¨æ¢¯åº¦ä¸Šå‡æ³•ï¼Œæ›´æ–°ç­–ç•¥å‡½æ•°çš„å‚æ•°$Î¸$ï¼š

â€‹																		$Î¸=Î¸+Î±âˆ‡_Î¸logÏ€_Î¸(s_t,a_t)v_t$

â€‹    2. è¿”å›ç­–ç•¥å‡½æ•°çš„å‚æ•°$Î¸$

ã€€ã€€è¿™é‡Œçš„ç­–ç•¥å‡½æ•°å¯ä»¥æ˜¯$softmax$ç­–ç•¥ï¼Œé«˜æ–¯ç­–ç•¥æˆ–è€…å…¶ä»–ç­–ç•¥ã€‚

### ä»£ç å®ç°

**æ¨¡å‹å®šä¹‰**

```python
import torch
#å®šä¹‰æ¨¡å‹
model = torch.nn.Sequential(
    torch.nn.Linear(4, 128),
    torch.nn.ReLU(),
    torch.nn.Linear(128, 2),
    torch.nn.Softmax(dim=1),
)
model(torch.randn(2, 4))
```

**åŠ¨ä½œé€‰å–**

```python
import random
#å¾—åˆ°ä¸€ä¸ªåŠ¨ä½œ
def get_action(state):
    state = torch.FloatTensor(state).reshape(1, 4)
    #[1, 4] -> [1, 2]
    prob = model(state)
    #æ ¹æ®æ¦‚ç‡é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]
    return action
get_action([1, 2, 3, 4])
```

**å¾—åˆ°ä¸€å±€æ¸¸æˆçš„æ•°æ®**

```python
#å¾—åˆ°ä¸€å±€æ¸¸æˆçš„æ•°æ®
def get_data():
    states = []
    rewards = []
    actions = []
    #åˆå§‹åŒ–æ¸¸æˆ
    state = env.reset()
    #ç©åˆ°æ¸¸æˆç»“æŸä¸ºæ­¢
    over = False
    while not over:
        #æ ¹æ®å½“å‰çŠ¶æ€å¾—åˆ°ä¸€ä¸ªåŠ¨ä½œ
        action = get_action(state)

        #æ‰§è¡ŒåŠ¨ä½œ,å¾—åˆ°åé¦ˆ
        next_state, reward, over, _ = env.step(action)

        #è®°å½•æ•°æ®æ ·æœ¬
        states.append(state)
        rewards.append(reward)
        actions.append(action)

        #æ›´æ–°æ¸¸æˆçŠ¶æ€,å¼€å§‹ä¸‹ä¸€ä¸ªåŠ¨ä½œ
        state = next_state

    return states, rewards, actions
```

**è®­ç»ƒå®ç°**

```python
def train():
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    #ç©Nå±€æ¸¸æˆ,æ¯å±€æ¸¸æˆè®­ç»ƒä¸€æ¬¡
    for epoch in range(1000):
        #ç©ä¸€å±€æ¸¸æˆ,å¾—åˆ°æ•°æ®
        states, rewards, actions = get_data()

        optimizer.zero_grad()

        #åé¦ˆçš„å’Œ,åˆå§‹åŒ–ä¸º0
        reward_sum = 0

        #ä»æœ€åä¸€æ­¥ç®—èµ·
        for i in reversed(range(len(states))):

            #åé¦ˆçš„å’Œ,ä»æœ€åä¸€æ­¥çš„åé¦ˆå¼€å§‹è®¡ç®—
            #æ¯å¾€å‰ä¸€æ­¥,>>å’Œ<<éƒ½è¡°å‡0.02,ç„¶åå†åŠ ä¸Šå½“å‰æ­¥çš„åé¦ˆ
            reward_sum *= 0.98
            reward_sum += rewards[i]

            #é‡æ–°è®¡ç®—å¯¹åº”åŠ¨ä½œçš„æ¦‚ç‡
            state = torch.FloatTensor(states[i]).reshape(1, 4)
            #[1, 4] -> [1, 2]
            prob = model(state)
            #[1, 2] -> scala
            prob = prob[0, actions[i]]

            #æ ¹æ®æ±‚å¯¼å…¬å¼,ç¬¦å·å–åæ˜¯å› ä¸ºè¿™é‡Œæ˜¯æ±‚loss,æ‰€ä»¥ä¼˜åŒ–æ–¹å‘ç›¸åï¼Œæ¯ä¸€æ­¥çš„æŸå¤±å‡½æ•°
            loss = -prob.log() * reward_sum

            #ç´¯ç§¯æ¢¯åº¦ï¼Œåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            loss.backward(retain_graph=True)
		#æ¢¯åº¦ä¸‹é™
        optimizer.step()

        if epoch % 100 == 0:
            test_result = sum([test(play=False) for _ in range(10)]) / 10
            print(epoch, test_result)
```

## Actor-Critic



### ç®—æ³•åŸç†

## A3C

## DDPG

### ç®—æ³•åŸç†

# è®ºæ–‡

## 1.å…æ¨¡å‹å¼ºåŒ–å­¦ä¹ 

### a.Deep Q-Learning

| [1]  | [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), Mnih et al, 2013. **Algorithm: DQN.** |
| ---- | ------------------------------------------------------------ |

| [2]  | [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/abs/1507.06527), Hausknecht and Stone, 2015. **Algorithm: Deep Recurrent Q-Learning.** |
| ---- | ------------------------------------------------------------ |

| [3]  | [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581), Wang et al, 2015. **Algorithm: Dueling DQN.** |
| ---- | ------------------------------------------------------------ |

| [4]  | [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461), Hasselt et al 2015. **Algorithm: Double DQN.** |
| ---- | ------------------------------------------------------------ |

| [5]  | [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952), Schaul et al, 2015. **Algorithm: Prioritized Experience Replay (PER).** |
| ---- | ------------------------------------------------------------ |

| [6]  | [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298), Hessel et al, 2017. **Algorithm: Rainbow DQN.** |
| ---- | ------------------------------------------------------------ |

### b.ç­–ç•¥æ¢¯åº¦

| [7]  | [Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783), Mnih et al, 2016. **Algorithm: A3C.** |
| ---- | ------------------------------------------------------------ |

| [8]  | [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477), Schulman et al, 2015. **Algorithm: TRPO.** |
| ---- | ------------------------------------------------------------ |

| [9]  | [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438), Schulman et al, 2015. **Algorithm: GAE.** |
| ---- | ------------------------------------------------------------ |

| [10] | [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347), Schulman et al, 2017. **Algorithm: PPO-Clip, PPO-Penalty.** |
| ---- | ------------------------------------------------------------ |

| [11] | [Emergence of Locomotion Behaviours in Rich Environments](https://arxiv.org/abs/1707.02286), Heess et al, 2017. **Algorithm: PPO-Penalty.** |
| ---- | ------------------------------------------------------------ |

| [12] | [Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation](https://arxiv.org/abs/1708.05144), Wu et al, 2017. **Algorithm: ACKTR.** |
| ---- | ------------------------------------------------------------ |

| [13] | [Sample Efficient Actor-Critic with Experience Replay](https://arxiv.org/abs/1611.01224), Wang et al, 2016. **Algorithm: ACER.** |
| ---- | ------------------------------------------------------------ |

| [14] | [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290), Haarnoja et al, 2018. **Algorithm: SAC.** |
| ---- | ------------------------------------------------------------ |

### c.ç¡®å®šç­–ç•¥æ¢¯åº¦

| [15] | [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf), Silver et al, 2014. **Algorithm: DPG.** |
| ---- | ------------------------------------------------------------ |

| [16] | [Continuous Control With Deep Reinforcement Learning](https://arxiv.org/abs/1509.02971), Lillicrap et al, 2015. **Algorithm: DDPG.** |
| ---- | ------------------------------------------------------------ |

| [17] | [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477), Fujimoto et al, 2018. **Algorithm: TD3.** |
| ---- | ------------------------------------------------------------ |

## 2.æ¨¡ä»¿å­¦ä¹ å’Œé€†å¼ºåŒ–å­¦ä¹ 

# å››ã€é¡¹ç›®


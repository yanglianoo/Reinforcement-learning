{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4422, 0.5578],\n",
       "         [0.3702, 0.6298]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.1027],\n",
       "         [0.0098]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#定义模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_td = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 1),\n",
    ")\n",
    "\n",
    "model(torch.randn(2, 4)), model_td(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "#得到一个动作\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    prob = model(state)\n",
    "\n",
    "    #根据概率选择一个动作\n",
    "    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0079, -0.0108, -0.0143,  0.0076],\n",
       "         [ 0.0077,  0.1846, -0.0141, -0.2895],\n",
       "         [ 0.0113,  0.3799, -0.0199, -0.5866],\n",
       "         [ 0.0189,  0.1850, -0.0316, -0.3003],\n",
       "         [ 0.0226,  0.3806, -0.0377, -0.6028],\n",
       "         [ 0.0303,  0.5762, -0.0497, -0.9071],\n",
       "         [ 0.0418,  0.3818, -0.0678, -0.6304],\n",
       "         [ 0.0494,  0.1877, -0.0805, -0.3599],\n",
       "         [ 0.0532,  0.3839, -0.0877, -0.6768],\n",
       "         [ 0.0608,  0.5801, -0.1012, -0.9957],\n",
       "         [ 0.0724,  0.7764, -0.1211, -1.3184],\n",
       "         [ 0.0880,  0.5830, -0.1475, -1.0659],\n",
       "         [ 0.0996,  0.3901, -0.1688, -0.8229],\n",
       "         [ 0.1074,  0.1976, -0.1853, -0.5877],\n",
       "         [ 0.1114,  0.3948, -0.1970, -0.9326]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1]]),\n",
       " tensor([[ 0.0077,  0.1846, -0.0141, -0.2895],\n",
       "         [ 0.0113,  0.3799, -0.0199, -0.5866],\n",
       "         [ 0.0189,  0.1850, -0.0316, -0.3003],\n",
       "         [ 0.0226,  0.3806, -0.0377, -0.6028],\n",
       "         [ 0.0303,  0.5762, -0.0497, -0.9071],\n",
       "         [ 0.0418,  0.3818, -0.0678, -0.6304],\n",
       "         [ 0.0494,  0.1877, -0.0805, -0.3599],\n",
       "         [ 0.0532,  0.3839, -0.0877, -0.6768],\n",
       "         [ 0.0608,  0.5801, -0.1012, -0.9957],\n",
       "         [ 0.0724,  0.7764, -0.1211, -1.3184],\n",
       "         [ 0.0880,  0.5830, -0.1475, -1.0659],\n",
       "         [ 0.0996,  0.3901, -0.1688, -0.8229],\n",
       "         [ 0.1074,  0.1976, -0.1853, -0.5877],\n",
       "         [ 0.1114,  0.3948, -0.1970, -0.9326],\n",
       "         [ 0.1193,  0.5920, -0.2157, -1.2801]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data():\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    overs = []\n",
    "\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "        #记录数据样本\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        next_states.append(next_state)\n",
    "        overs.append(over)\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #[b, 4]\n",
    "    states = torch.FloatTensor(states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    actions = torch.LongTensor(actions).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_states = torch.FloatTensor(next_states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    overs = torch.LongTensor(overs).reshape(-1, 1)\n",
    "\n",
    "    return states, rewards, actions, next_states, overs\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.090483997483998, 8.690100963999999, 8.260044, 6.724, 4.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#优势函数\n",
    "def get_advantages(deltas):\n",
    "    advantages = []\n",
    "\n",
    "    #反向遍历deltas\n",
    "    s = 0.0\n",
    "    for delta in deltas[::-1]:\n",
    "        s = 0.98 * 0.95 * s + delta\n",
    "        advantages.append(s)\n",
    "\n",
    "    #逆序\n",
    "    advantages.reverse()\n",
    "    return advantages\n",
    "\n",
    "\n",
    "get_advantages(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8251,
     "status": "ok",
     "timestamp": 1650011468229,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "BQXVYW2T_DcQ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31.3\n",
      "50 145.3\n",
      "100 198.5\n",
      "150 200.0\n",
      "200 200.0\n",
      "250 200.0\n",
      "300 200.0\n",
      "350 200.0\n",
      "400 200.0\n",
      "450 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    optimizer_td = torch.optim.Adam(model_td.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #玩N局游戏,每局游戏训练M次\n",
    "    for epoch in range(500):\n",
    "        #玩一局游戏,得到数据\n",
    "        #states -> [b, 4]\n",
    "        #rewards -> [b, 1]\n",
    "        #actions -> [b, 1]\n",
    "        #next_states -> [b, 4]\n",
    "        #overs -> [b, 1]\n",
    "        states, rewards, actions, next_states, overs = get_data()\n",
    "\n",
    "        #计算values和targets\n",
    "        #[b, 4] -> [b, 1]\n",
    "        values = model_td(states)\n",
    "\n",
    "        #[b, 4] -> [b, 1]\n",
    "        targets = model_td(next_states).detach()\n",
    "        targets = targets * 0.98\n",
    "        targets *= (1 - overs)\n",
    "        targets += rewards\n",
    "\n",
    "        #计算优势,这里的advantages有点像是策略梯度里的reward_sum\n",
    "        #只是这里计算的不是reward,而是target和value的差\n",
    "        #[b, 1]\n",
    "        deltas = (targets - values).squeeze(dim=1).tolist()\n",
    "        advantages = get_advantages(deltas)\n",
    "        advantages = torch.FloatTensor(advantages).reshape(-1, 1)\n",
    "\n",
    "        #取出每一步动作的概率\n",
    "        #[b, 2] -> [b, 2] -> [b, 1]\n",
    "        old_probs = model(states)\n",
    "        old_probs = old_probs.gather(dim=1, index=actions)\n",
    "        old_probs = old_probs.detach()\n",
    "\n",
    "        #每批数据反复训练10次\n",
    "        for _ in range(10):\n",
    "            #重新计算每一步动作的概率\n",
    "            #[b, 4] -> [b, 2]\n",
    "            new_probs = model(states)\n",
    "            #[b, 2] -> [b, 1]\n",
    "            new_probs = new_probs.gather(dim=1, index=actions)\n",
    "            new_probs = new_probs\n",
    "\n",
    "            #求出概率的变化\n",
    "            #[b, 1] - [b, 1] -> [b, 1]\n",
    "            ratios = new_probs / old_probs\n",
    "\n",
    "            #计算截断的和不截断的两份loss,取其中小的\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr1 = ratios * advantages\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            #重新计算value,并计算时序差分loss\n",
    "            values = model_td(states)\n",
    "            loss_td = loss_fn(values, targets)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer_td.zero_grad()\n",
    "            loss_td.backward()\n",
    "            optimizer_td.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

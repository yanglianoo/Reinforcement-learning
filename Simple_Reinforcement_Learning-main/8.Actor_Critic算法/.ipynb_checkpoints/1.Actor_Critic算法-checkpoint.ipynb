{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4340, 0.5660],\n",
       "         [0.3047, 0.6953]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.3946],\n",
       "         [0.3271]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#定义模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_td = sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 1),\n",
    ")\n",
    "\n",
    "model(torch.randn(2, 4)), model_td(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "#得到一个动作\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    prob = model(state)\n",
    "\n",
    "    #根据概率选择一个动作\n",
    "    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0451,  0.0146,  0.0370,  0.0127],\n",
       "         [-0.0448,  0.2092,  0.0372, -0.2681],\n",
       "         [-0.0406,  0.0135,  0.0319,  0.0361],\n",
       "         [-0.0403, -0.1820,  0.0326,  0.3387],\n",
       "         [-0.0440, -0.3776,  0.0394,  0.6414],\n",
       "         [-0.0515, -0.1830,  0.0522,  0.3614],\n",
       "         [-0.0552, -0.3789,  0.0594,  0.6701],\n",
       "         [-0.0627, -0.5748,  0.0728,  0.9809],\n",
       "         [-0.0742, -0.7708,  0.0925,  1.2955],\n",
       "         [-0.0897, -0.9669,  0.1184,  1.6157],\n",
       "         [-0.1090, -0.7734,  0.1507,  1.3621],\n",
       "         [-0.1245, -0.9701,  0.1779,  1.6979]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1]]),\n",
       " tensor([[-0.0448,  0.2092,  0.0372, -0.2681],\n",
       "         [-0.0406,  0.0135,  0.0319,  0.0361],\n",
       "         [-0.0403, -0.1820,  0.0326,  0.3387],\n",
       "         [-0.0440, -0.3776,  0.0394,  0.6414],\n",
       "         [-0.0515, -0.1830,  0.0522,  0.3614],\n",
       "         [-0.0552, -0.3789,  0.0594,  0.6701],\n",
       "         [-0.0627, -0.5748,  0.0728,  0.9809],\n",
       "         [-0.0742, -0.7708,  0.0925,  1.2955],\n",
       "         [-0.0897, -0.9669,  0.1184,  1.6157],\n",
       "         [-0.1090, -0.7734,  0.1507,  1.3621],\n",
       "         [-0.1245, -0.9701,  0.1779,  1.6979],\n",
       "         [-0.1439, -0.7774,  0.2119,  1.4654]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data():\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    overs = []\n",
    "\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "        #记录数据样本\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        next_states.append(next_state)\n",
    "        overs.append(over)\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #[b, 4]\n",
    "    states = torch.FloatTensor(states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    actions = torch.LongTensor(actions).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_states = torch.FloatTensor(next_states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    overs = torch.LongTensor(overs).reshape(-1, 1)\n",
    "\n",
    "    return states, rewards, actions, next_states, overs\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8251,
     "status": "ok",
     "timestamp": 1650011468229,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "BQXVYW2T_DcQ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21.4\n",
      "100 20.6\n",
      "200 42.3\n",
      "300 64.1\n",
      "400 147.3\n",
      "500 158.9\n",
      "600 195.3\n",
      "700 190.2\n",
      "800 200.0\n",
      "900 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    optimizer_td = torch.optim.Adam(model_td.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #玩N局游戏,每局游戏训练一次\n",
    "    for i in range(1000):\n",
    "        #玩一局游戏,得到数据\n",
    "        #states -> [b, 4]\n",
    "        #rewards -> [b, 1]\n",
    "        #actions -> [b, 1]\n",
    "        #next_states -> [b, 4]\n",
    "        #overs -> [b, 1]\n",
    "        states, rewards, actions, next_states, overs = get_data()\n",
    "\n",
    "        #计算values和targets\n",
    "        #[b, 4] -> [b ,1]\n",
    "        values = model_td(states)\n",
    "\n",
    "        #[b, 4] -> [b ,1]\n",
    "        targets = model_td(next_states) * 0.98\n",
    "        #[b ,1] * [b ,1] -> [b ,1]\n",
    "        targets *= (1 - overs)\n",
    "        #[b ,1] + [b ,1] -> [b ,1]\n",
    "        targets += rewards\n",
    "\n",
    "        #时序差分误差\n",
    "        #[b ,1] - [b ,1] -> [b ,1]\n",
    "        delta = (targets - values).detach()\n",
    "\n",
    "        #重新计算对应动作的概率\n",
    "        #[b, 4] -> [b ,2]\n",
    "        probs = model(states)\n",
    "        #[b ,2] -> [b ,1]\n",
    "        probs = probs.gather(dim=1, index=actions)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #只是把公式中的reward_sum替换为了时序差分的误差\n",
    "        #[b ,1] * [b ,1] -> [b ,1] -> scala\n",
    "        loss = (-probs.log() * delta).mean()\n",
    "\n",
    "        #时序差分的loss就是简单的value和target求mse loss即可\n",
    "        loss_td = loss_fn(values, targets.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer_td.zero_grad()\n",
    "        loss_td.backward()\n",
    "        optimizer_td.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(i, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c6403ce9ee30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('RL_Simple')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6925958b004b98bc0512a6d71e5da00858331a32f66ed7f503cad179ff8aa782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

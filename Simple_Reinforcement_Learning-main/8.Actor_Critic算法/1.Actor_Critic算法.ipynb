{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2915, 0.7085],\n",
       "         [0.3921, 0.6079]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[-0.0970],\n",
       "         [-0.0614]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#定义模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_td = sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 1),\n",
    ")\n",
    "\n",
    "model(torch.randn(2, 4)), model_td(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "#得到一个动作\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    #[1, 4] -> [1, 2]\n",
    "    prob = model(state)\n",
    "\n",
    "    #根据概率选择一个动作\n",
    "    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0397, -0.0265,  0.0420,  0.0132],\n",
       "         [-0.0402,  0.1680,  0.0422, -0.2660],\n",
       "         [-0.0369, -0.0277,  0.0369,  0.0397],\n",
       "         [-0.0374,  0.1669,  0.0377, -0.2411],\n",
       "         [-0.0341, -0.0287,  0.0329,  0.0632],\n",
       "         [-0.0347, -0.2243,  0.0342,  0.3661],\n",
       "         [-0.0392, -0.4199,  0.0415,  0.6694],\n",
       "         [-0.0476, -0.6156,  0.0549,  0.9748],\n",
       "         [-0.0599, -0.4212,  0.0744,  0.6999],\n",
       "         [-0.0683, -0.6173,  0.0884,  1.0150],\n",
       "         [-0.0806, -0.4234,  0.1087,  0.7513],\n",
       "         [-0.0891, -0.6199,  0.1237,  1.0761],\n",
       "         [-0.1015, -0.4266,  0.1452,  0.8247],\n",
       "         [-0.1100, -0.2337,  0.1617,  0.5810],\n",
       "         [-0.1147, -0.0412,  0.1733,  0.3433],\n",
       "         [-0.1155, -0.2383,  0.1802,  0.6852],\n",
       "         [-0.1203, -0.4354,  0.1939,  1.0288]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]),\n",
       " tensor([[-0.0402,  0.1680,  0.0422, -0.2660],\n",
       "         [-0.0369, -0.0277,  0.0369,  0.0397],\n",
       "         [-0.0374,  0.1669,  0.0377, -0.2411],\n",
       "         [-0.0341, -0.0287,  0.0329,  0.0632],\n",
       "         [-0.0347, -0.2243,  0.0342,  0.3661],\n",
       "         [-0.0392, -0.4199,  0.0415,  0.6694],\n",
       "         [-0.0476, -0.6156,  0.0549,  0.9748],\n",
       "         [-0.0599, -0.4212,  0.0744,  0.6999],\n",
       "         [-0.0683, -0.6173,  0.0884,  1.0150],\n",
       "         [-0.0806, -0.4234,  0.1087,  0.7513],\n",
       "         [-0.0891, -0.6199,  0.1237,  1.0761],\n",
       "         [-0.1015, -0.4266,  0.1452,  0.8247],\n",
       "         [-0.1100, -0.2337,  0.1617,  0.5810],\n",
       "         [-0.1147, -0.0412,  0.1733,  0.3433],\n",
       "         [-0.1155, -0.2383,  0.1802,  0.6852],\n",
       "         [-0.1203, -0.4354,  0.1939,  1.0288],\n",
       "         [-0.1290, -0.6325,  0.2145,  1.3755]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data():\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    next_states = []\n",
    "    overs = []\n",
    "\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "        #记录数据样本\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        next_states.append(next_state)\n",
    "        overs.append(over)\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #[b, 4]\n",
    "    states = torch.FloatTensor(states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    actions = torch.LongTensor(actions).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_states = torch.FloatTensor(next_states).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    overs = torch.LongTensor(overs).reshape(-1, 1)\n",
    "\n",
    "    return states, rewards, actions, next_states, overs\n",
    "\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 8251,
     "status": "ok",
     "timestamp": 1650011468229,
     "user": {
      "displayName": "Sam Lu",
      "userId": "15789059763790170725"
     },
     "user_tz": -480
    },
    "id": "BQXVYW2T_DcQ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 26.7\n",
      "100 15.3\n",
      "200 44.4\n",
      "300 94.8\n",
      "400 172.2\n",
      "500 170.9\n",
      "600 194.6\n",
      "700 200.0\n",
      "800 200.0\n",
      "900 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    optimizer_td = torch.optim.Adam(model_td.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #玩N局游戏,每局游戏训练一次\n",
    "    for i in range(1000):\n",
    "        #玩一局游戏,得到数据\n",
    "        #states -> [b, 4]\n",
    "        #rewards -> [b, 1]\n",
    "        #actions -> [b, 1]\n",
    "        #next_states -> [b, 4]\n",
    "        #overs -> [b, 1]\n",
    "        states, rewards, actions, next_states, overs = get_data()\n",
    "\n",
    "        #计算values和targets\n",
    "        #[b, 4] -> [b ,1]\n",
    "        values = model_td(states)\n",
    "\n",
    "        #[b, 4] -> [b ,1]\n",
    "        targets = model_td(next_states) * 0.98\n",
    "        #[b ,1] * [b ,1] -> [b ,1]\n",
    "        targets *= (1 - overs)\n",
    "        #[b ,1] + [b ,1] -> [b ,1]\n",
    "        targets += rewards\n",
    "\n",
    "        #时序差分误差\n",
    "        #[b ,1] - [b ,1] -> [b ,1]\n",
    "        delta = (targets - values).detach()\n",
    "\n",
    "        #重新计算对应动作的概率\n",
    "        #[b, 4] -> [b ,2]\n",
    "        probs = model(states)\n",
    "        #[b ,2] -> [b ,1]\n",
    "        probs = probs.gather(dim=1, index=actions)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #只是把公式中的reward_sum替换为了时序差分的误差\n",
    "        #[b ,1] * [b ,1] -> [b ,1] -> scala\n",
    "        loss = (-probs.log() * delta).mean()\n",
    "\n",
    "        #时序差分的loss就是简单的value和target求mse loss即可\n",
    "        loss_td = loss_fn(values, targets.detach())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer_td.zero_grad()\n",
    "        loss_td.backward()\n",
    "        optimizer_td.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(i, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('RL_Simple')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6925958b004b98bc0512a6d71e5da00858331a32f66ed7f503cad179ff8aa782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

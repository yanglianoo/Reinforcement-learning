{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2562]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.sequential(state) * 2.0\n",
    "\n",
    "\n",
    "model_action = Model()\n",
    "model_action_next = Model()\n",
    "\n",
    "model_action_next.load_state_dict(model_action.state_dict())\n",
    "\n",
    "model_action(torch.randn(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0311]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "model_value_next = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_value_next.load_state_dict(model_value.state_dict())\n",
    "\n",
    "model_value(torch.randn(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0794577558499697"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 3)\n",
    "    action = model_action(state).item()\n",
    "    #给动作添加噪声,增加探索\n",
    "    action += random.normalvariate(mu=0, sigma=0.01)\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " (array([ 0.9781273 , -0.20800708, -0.94385123], dtype=float32),\n",
       "  -0.04984974862998363,\n",
       "  -0.1329937692558043,\n",
       "  array([ 0.9651177 , -0.26181635, -1.107334  ], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step([action])\n",
    "\n",
    "        #记录数据样本\n",
    "        datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "\n",
    "update_data()\n",
    "\n",
    "len(datas), datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9422, -0.3351, -3.3499],\n",
       "         [ 0.7944,  0.6073, -4.1651],\n",
       "         [ 0.5650, -0.8251, -4.6804],\n",
       "         [ 0.9996, -0.0274, -3.1510],\n",
       "         [ 0.9400, -0.3411, -3.3633]]),\n",
       " tensor([[-0.2925],\n",
       "         [-0.1565],\n",
       "         [-0.6216],\n",
       "         [-0.1859],\n",
       "         [-0.2984]]),\n",
       " tensor([[-1.2391],\n",
       "         [-2.1608],\n",
       "         [-3.1326],\n",
       "         [-0.9937],\n",
       "         [-1.2524]]),\n",
       " tensor([[ 0.8658, -0.5003, -3.6451],\n",
       "         [ 0.8933,  0.4494, -3.7331],\n",
       "         [ 0.3248, -0.9458, -5.3925],\n",
       "         [ 0.9825, -0.1863, -3.1995],\n",
       "         [ 0.8622, -0.5066, -3.6639]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 3]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1070.7204075649688"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step([action])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0367],\n",
       "         [-0.0285],\n",
       "         [-0.1399],\n",
       "         [-0.0137],\n",
       "         [-0.0375]], grad_fn=<SliceBackward0>),\n",
       " tensor([[-1.2954],\n",
       "         [-2.1755],\n",
       "         [-3.3287],\n",
       "         [-1.0163],\n",
       "         [-1.3098]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(state, action):\n",
    "    #直接评估综合了state和action的value\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "\n",
    "    #[b, 4] -> [b, 1]\n",
    "    return model_value(input)\n",
    "\n",
    "\n",
    "def get_target(next_state, reward, over):\n",
    "    #对next_state的评估需要先把它对应的动作计算出来,这里用model_action_next来计算\n",
    "    #[b, 3] -> [b, 1]\n",
    "    action = model_action_next(next_state)\n",
    "\n",
    "    #和value的计算一样,action拼合进next_state里综合计算\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([next_state, action], dim=1)\n",
    "\n",
    "    #[b, 4] -> [b, 1]\n",
    "    target = model_value_next(input) * 0.98\n",
    "\n",
    "    #[b, 1] * [b, 1] -> [b, 1]\n",
    "    target *= (1 - over)\n",
    "\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_value(state, action)[:5], get_target(next_state, reward, over)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1446, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #首先把动作计算出来\n",
    "    #[b, 3] -> [b, 1]\n",
    "    action = model_action(state)\n",
    "\n",
    "    #像value计算那里一样,拼合state和action综合计算\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "\n",
    "    #使用value网络评估动作的价值,价值是越高越好\n",
    "    #因为这里是在计算loss,loss是越小越好,所以符号取反\n",
    "    #[b, 4] -> [b, 1] -> [1]\n",
    "    loss = -model_value(input).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 -1426.9143485602572\n",
      "20 4400 -782.2665580151744\n",
      "40 8400 -181.24392126393508\n",
      "60 10000 -213.92405181783744\n",
      "80 10000 -158.83671365329474\n",
      "100 10000 -152.75725311945126\n",
      "120 10000 -162.96483997992166\n",
      "140 10000 -172.0843508589916\n",
      "160 10000 -134.7957615513623\n",
      "180 10000 -79.0563939012981\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=5e-4)\n",
    "    optimizer_value = torch.optim.Adam(model_value.parameters(), lr=5e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(200):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算value和target\n",
    "            value = get_value(state, action)\n",
    "            target = get_target(next_state, reward, over)\n",
    "\n",
    "            #两者求差,计算loss,更新参数\n",
    "            loss_value = loss_fn(value, target)\n",
    "\n",
    "            optimizer_value.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "\n",
    "            #使用value网络评估action网络的loss,更新参数\n",
    "            loss_action = get_loss_action(state)\n",
    "\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #以一个小的比例更新\n",
    "            soft_update(model_action, model_action_next)\n",
    "            soft_update(model_value, model_value_next)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-246.88427889497422"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

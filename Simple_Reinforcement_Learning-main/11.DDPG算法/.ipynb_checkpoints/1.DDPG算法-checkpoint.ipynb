{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1383]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.sequential(state) * 2.0\n",
    "\n",
    "\n",
    "model_action = Model()\n",
    "model_action_next = Model()\n",
    "\n",
    "model_action_next.load_state_dict(model_action.state_dict())\n",
    "\n",
    "model_action(torch.randn(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0133]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "model_value_next = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "model_value_next.load_state_dict(model_value.state_dict())\n",
    "\n",
    "model_value(torch.randn(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48027890579378746"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 3)\n",
    "    action = model_action(state).item()\n",
    "    #给动作添加噪声,增加探索\n",
    "    action += random.normalvariate(mu=0, sigma=0.01)\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " (array([ 0.44267234, -0.89668345, -0.3420089 ], dtype=float32),\n",
       "  0.056713865923649825,\n",
       "  -1.2487326231058322,\n",
       "  array([ 0.39702764, -0.9178066 , -1.0060143 ], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step([action])\n",
    "\n",
    "        #记录数据样本\n",
    "        datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "\n",
    "update_data()\n",
    "\n",
    "len(datas), datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.9995, -0.0317,  4.8077],\n",
       "         [ 0.8980,  0.4399,  5.1972],\n",
       "         [ 0.9811,  0.1936,  4.9508],\n",
       "         [ 0.8594, -0.5114,  5.2068],\n",
       "         [-0.4182,  0.9083,  5.3395]]),\n",
       " tensor([[0.7706],\n",
       "         [0.8023],\n",
       "         [0.7845],\n",
       "         [0.8590],\n",
       "         [0.8629]]),\n",
       " tensor([[-2.3130],\n",
       "         [-2.9093],\n",
       "         [-2.4896],\n",
       "         [-3.0000],\n",
       "         [-6.8610]]),\n",
       " tensor([[ 0.9773,  0.2117,  4.8995],\n",
       "         [ 0.7399,  0.6727,  5.6475],\n",
       "         [ 0.8981,  0.4399,  5.2136],\n",
       "         [ 0.9585, -0.2852,  4.9522],\n",
       "         [-0.6736,  0.7391,  6.1502]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 3]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1640.8992574911329"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step([action])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0775],\n",
       "         [-0.0670],\n",
       "         [-0.0675],\n",
       "         [-0.1390],\n",
       "         [-0.1310]], grad_fn=<SliceBackward0>),\n",
       " tensor([[-2.3769],\n",
       "         [-2.9874],\n",
       "         [-2.5562],\n",
       "         [-3.1047],\n",
       "         [-7.0408]], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(state, action):\n",
    "    #直接评估综合了state和action的value\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "\n",
    "    #[b, 4] -> [b, 1]\n",
    "    return model_value(input)\n",
    "\n",
    "\n",
    "def get_target(next_state, reward, over):\n",
    "    #对next_state的评估需要先把它对应的动作计算出来,这里用model_action_next来计算\n",
    "    #[b, 3] -> [b, 1]\n",
    "    action = model_action_next(next_state)\n",
    "\n",
    "    #和value的计算一样,action拼合进next_state里综合计算\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([next_state, action], dim=1)\n",
    "\n",
    "    #[b, 4] -> [b, 1]\n",
    "    target = model_value_next(input) * 0.98\n",
    "\n",
    "    #[b, 1] * [b, 1] -> [b, 1]\n",
    "    target *= (1 - over)\n",
    "\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_value(state, action)[:5], get_target(next_state, reward, over)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1264, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #首先把动作计算出来\n",
    "    #[b, 3] -> [b, 1]\n",
    "    action = model_action(state)\n",
    "\n",
    "    #像value计算那里一样,拼合state和action综合计算\n",
    "    #[b, 3+1] -> [b, 4]\n",
    "    input = torch.cat([state, action], dim=1)\n",
    "\n",
    "    #使用value网络评估动作的价值,价值是越高越好\n",
    "    #因为这里是在计算loss,loss是越小越好,所以符号取反\n",
    "    #[b, 4] -> [b, 1] -> [1]\n",
    "    loss = -model_value(input).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 -1242.8773068244013\n",
      "20 4400 -881.1079624252818\n",
      "40 8400 -408.8301354501383\n",
      "60 10000 -145.18861573121904\n",
      "80 10000 -124.33850014180787\n",
      "100 10000 -169.55078179211972\n",
      "120 10000 -199.78925005623233\n",
      "140 10000 -442.0419001449275\n",
      "160 10000 -192.75899711530064\n",
      "180 10000 -179.93575195946784\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model_action.train()\n",
    "    model_value.train()\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=5e-4)\n",
    "    optimizer_value = torch.optim.Adam(model_value.parameters(), lr=5e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(200):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算value和target\n",
    "            value = get_value(state, action)\n",
    "            target = get_target(next_state, reward, over)\n",
    "\n",
    "            #两者求差,计算loss,更新参数\n",
    "            loss_value = loss_fn(value, target)\n",
    "\n",
    "            optimizer_value.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "\n",
    "            #使用value网络评估action网络的loss,更新参数\n",
    "            loss_action = get_loss_action(state)\n",
    "\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #以一个小的比例更新\n",
    "            soft_update(model_action, model_action_next)\n",
    "            soft_update(model_value, model_value_next)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAPEUlEQVR4nO3df2zU933H8eeLCzb50agmPgjBZHirK4UkWxpZLFOitEozQZqqICQkV+3EH0j8w6RUm9TBqnaqtEjZ/qj6V1ShNprVdEWWGikoirIgmraqNoWY5kf5UYpbMnAh2FmUhFBwsHnvD3/aXow5H7473335vB6Sdd/7+Hvn98nw5HvfM2dFBGaWr0WtHsDMWssRMMucI2CWOUfALHOOgFnmHAGzzDUtApLWSzoqaUTSjmZ9HTOrj5rxcwKSSsCvgb8FRoFXgC9GxOGGfzEzq0uzjgTWAiMR8duI+BDYDWxo0tcyszpc16T7XQmcrLg+Cvz1lXbu7u6O1atXN2kUMwM4cODA2xFRnrnerAholrWPPO+QtA3YBnD77bczPDzcpFHMDEDS/8623qynA6PAqorrPcCpyh0iYldE9EdEf7l8WZzMbIE0KwKvAH2SeiV1AAPAniZ9LTOrQ1OeDkTEpKS/B/4LKAFPRcShZnwtM6tPs84JEBHPA8836/7NrDH8E4NmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlbs4ISHpK0pikgxVrSyXtlXQsXXZVfG6npBFJRyWta9bgZtYYtRwJ/AewfsbaDmBfRPQB+9J1JK0BBoA7022elFRq2LRm1nBzRiAifga8M2N5AzCYtgeBjRXruyNiIiKOAyPA2saMambNMN9zAssj4jRAulyW1lcCJyv2G01rl5G0TdKwpOHx8fF5jmFm9Wr0iUHNshaz7RgRuyKiPyL6y+Vyg8cws1rNNwJnJK0ASJdjaX0UWFWxXw9wav7jmVmzzTcCe4AtaXsL8GzF+oCkTkm9QB+wv74RzayZrptrB0k/BD4DdEsaBf4FeAIYkrQVOAFsBoiIQ5KGgMPAJLA9IqaaNLuZNcCcEYiIL17hU5+9wv6PA4/XM5SZLRz/xKBZ5hwBs8w5AmaZcwTMMjfniUEzgJic5PyJE3xw+DBT587RsWwZH7vrLhZ3dyPN9jNiVhSOgFUVEVw6f57TQ0O8/eKLTJ07BxFQKtFZLnPbl75E1wMPoJL/n1hROQJWVUxO8runn2b8+efh0qU/fWJqiom33uLEd74DpRJd99/vI4KC8jkBq+rsG2/w9gsvfDQAFabOnePU97/P5LvvLuxg1jCOgFX1fy+9RExOVt1n4q23OHvwYNV9rH05AlbVXAGY3imIqSkiZv0Po9bmHAFriJjyfxEpKkfAGsIRKC5HwBrCESguR8AawxEoLEfAGsJHAsXlCFhD1PQqgrUlR8AawkcCxeUIWEM4AsXlCFhDOALF5QhYQzgCxeUIWEM4AsXlCFhDXLpwodUj2Dw5AtYQ548fn36zESscR8AaIq7wfgPW/hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5hwBs8w5Aladf7XYNW/OCEhaJeklSUckHZL0WFpfKmmvpGPpsqviNjsljUg6KmldMx+ANdcNq1e3egRrslqOBCaBf4yIO4D7gO2S1gA7gH0R0QfsS9dJnxsA7gTWA09K8q+sLahFS5bUtuOlS1f8fYXW3uaMQEScjohfpO2zwBFgJbABGEy7DQIb0/YGYHdETETEcWAEWNvguW2hLKrtGWNE+NeQFdRVnROQtBr4FPAysDwiTsN0KIBlabeVwMmKm42mNSugmn/duI8ECqvmCEi6CfgR8JWIeL/arrOsXfZPhKRtkoYlDY+Pj9c6hi20qzkScAQKqabvsKTFTAfgBxHxTFo+I2lF+vwKYCytjwKrKm7eA5yaeZ8RsSsi+iOiv1wuz3d+a7KrOhLw04FCquXVAQHfA45ExLcqPrUH2JK2twDPVqwPSOqU1Av0AfsbN7ItqBqPBPCRQGFdV8M+9wN/B/xS0mtp7Z+BJ4AhSVuBE8BmgIg4JGkIOMz0KwvbI8LvQllQtR4JhM8JFNacEYiInzP783yAz17hNo8Dj9cxl7WLqzkS8NOBQvJPDFp1tZ4Y9JFAYTkCVtXVnBj0OYFicgSsuqt4idCvDhSTI2BV1XokMHX2LJPvvdfkaawZHAGrrsYjgUsTE1yamGjyMNYMjoBVVfM5ASssR8Cqq/UlQissf4etKjkC1zx/h606Px245jkCVpWPBK59/g5bdT4SuOY5AladjwSuef4OW1V+ifDaV8t/JbaczTgSeP/DD3nmxAnGL1xg3W23cXdXl0NRcI6AVVV5YvDsxYt849VX+fnY9JtIvTA6yr/eey9/s2zZlW5uBeCnA1Zdxb/yv/v97/nvsbE/Xn/v4kVePHXZO8dZwTgCVlXnrbfSkd4DsmPRIjpLH/0VEjcvXtyKsayBHAGravHSpXQ9+CAAvTfdxD/dfTfdnZ10lko8tGIFW/v6AOhYtoyOW29t5ag2Tz4nYFVJovzII7x/4ADn33yTR3t6uPeWWzg/OcnKG29kSamEOjpYvmkTi7u65r5Dazs+ErA5dZTL3L59O9f39qJFi7jthhv4i5tvZkmpxKIbbuDWzZvpfvhhv0pQUD4SsDlJ4sZPfpJPfP3rvPPTn3L2jTeYunCB61etYumnP82Nd9zBouv8R6mo/J2zmkiio7ub5Zs2sXzTpum3Ekv/8vsIoNgcAbsqf/wL77/41wyfEzDLnCNgljlHwCxzjoBZ5hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5uaMgKQlkvZLel3SIUnfTOtLJe2VdCxddlXcZqekEUlHJa1r5gMws/rUciQwATwUEX8F3AOsl3QfsAPYFxF9wL50HUlrgAHgTmA98KSk0mx3bGatN2cEYtoH6eri9BHABmAwrQ8CG9P2BmB3RExExHFgBFjbyKHNrHFqOicgqSTpNWAM2BsRLwPLI+I0QLr8w5vPrwROVtx8NK3NvM9tkoYlDY+Pj9fxEMysHjVFICKmIuIeoAdYK+muKrvP9m4TMct97oqI/ojoL6e3tDazhXdVrw5ExLvAT5h+rn9G0gqAdPmH30oxCqyquFkP4N9QYdamanl1oCzp42n7euBh4FfAHmBL2m0L8Gza3gMMSOqU1Av0AfsbPLeZNUgt7zG4AhhMZ/gXAUMR8Zyk/wGGJG0FTgCbASLikKQh4DAwCWyPiKnmjG9m9VLEZU/XF1x/f38MDw+3egyza5qkAxHRP3PdPzFoljlHwCxzjoBZ5hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5hwBs8w5AmaZcwTMMucImGXOETDLnCNgljlHwCxzNUdAUknSq5KeS9eXStor6Vi67KrYd6ekEUlHJa1rxuBm1hhXcyTwGHCk4voOYF9E9AH70nUkrQEGgDuB9cCTkkqNGdfMGq2mCEjqAR4FvluxvAEYTNuDwMaK9d0RMRERx4ERYG1DpjWzhqv1SODbwFeBSxVryyPiNEC6XJbWVwInK/YbTWtm1obmjICkzwNjEXGgxvvULGsxy/1ukzQsaXh8fLzGuzazRqvlSOB+4AuS3gR2Aw9Jeho4I2kFQLocS/uPAqsqbt8DnJp5pxGxKyL6I6K/XC7X8RDMrB5zRiAidkZET0SsZvqE348j4svAHmBL2m0L8Gza3gMMSOqU1Av0AfsbPrmZNcR1ddz2CWBI0lbgBLAZICIOSRoCDgOTwPaImKp7UjNrCkVc9nR9wfX398fw8HCrxzC7pkk6EBH9M9f9E4NmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc46AWeYcAbPMOQJmmXMEzDLnCJhlzhEwy5wjYJY5R8Asc4qIVs+ApHHgHPB2q2epUTfFmRWKNa9nbZ4/i4jyzMW2iACApOGI6G/1HLUo0qxQrHk968Lz0wGzzDkCZplrpwjsavUAV6FIs0Kx5vWsC6xtzgmYWWu005GAmbVAyyMgab2ko5JGJO1o9TwAkp6SNCbpYMXaUkl7JR1Ll10Vn9uZ5j8qad0Cz7pK0kuSjkg6JOmxdp1X0hJJ+yW9nmb9ZrvOWvH1S5JelfRcu886bxHRsg+gBPwG+HOgA3gdWNPKmdJcDwL3Agcr1v4d2JG2dwD/lrbXpLk7gd70eEoLOOsK4N60/THg12mmtpsXEHBT2l4MvAzc146zVsz8D8B/As+185+Dej5afSSwFhiJiN9GxIfAbmBDi2ciIn4GvDNjeQMwmLYHgY0V67sjYiIijgMjTD+uBRERpyPiF2n7LHAEWNmO88a0D9LVxekj2nFWAEk9wKPAdyuW23LWerQ6AiuBkxXXR9NaO1oeEadh+i8esCytt81jkLQa+BTT/8K25bzp8Po1YAzYGxFtOyvwbeCrwKWKtXaddd5aHQHNsla0lyva4jFIugn4EfCViHi/2q6zrC3YvBExFRH3AD3AWkl3Vdm9ZbNK+jwwFhEHar3JLGuF+LPc6giMAqsqrvcAp1o0y1zOSFoBkC7H0nrLH4OkxUwH4AcR8Uxabtt5ASLiXeAnwHrac9b7gS9IepPpp6kPSXq6TWetS6sj8ArQJ6lXUgcwAOxp8UxXsgfYkra3AM9WrA9I6pTUC/QB+xdqKEkCvgcciYhvtfO8ksqSPp62rwceBn7VjrNGxM6I6ImI1Uz/ufxxRHy5HWetW6vPTAKfY/qM9m+Ar7V6njTTD4HTwEWmC78VuAXYBxxLl0sr9v9amv8o8MgCz/oA04edbwCvpY/PteO8wF8Cr6ZZDwLfSOttN+uMuT/Dn14daOtZ5/Phnxg0y1yrnw6YWYs5AmaZcwTMMucImGXOETDLnCNgljlHwCxzjoBZ5v4fSGIJYGmt1i4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-234.9650549581404"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

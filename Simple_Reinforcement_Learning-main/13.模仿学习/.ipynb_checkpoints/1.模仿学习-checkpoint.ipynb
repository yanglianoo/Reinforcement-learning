{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUiUlEQVR4nO3dfaxcd53f8ffnXjsOiSPIw01qbKfJskYiQY2DrlKkVFUKlHhpVcMfVEYqyh+RjNQggRrRJqzUBamWthUP/QdoQxOtCyzBEiBclN2uyYIQ2m2C8xzngTjEJY6N7STNxiHe6/vw7R/3RBnsub7j+5CZc+f9kkZz5nvOmfn+ovEn5/7mzJxUFZKk9hjpdwOSpLNjcEtSyxjcktQyBrcktYzBLUktY3BLUsssW3An2ZLk6ST7k9y2XK8jScMmy3Eed5JR4FfAPwcOAr8EPlFVTyz5i0nSkFmuI+7rgP1V9euqOgncDWxdpteSpKGyapmedz3wfMfjg8A/nmvjSy65pK644oplakWS2ufAgQO8+OKL6bZuuYK724v93pxMku3AdoDLL7+cvXv3LlMrktQ+4+Pjc65brqmSg8DGjscbgEOdG1TVHVU1XlXjY2Njy9SGJK08yxXcvwQ2JbkyyTnANmD3Mr2WJA2VZZkqqaqpJJ8G/jcwCtxVVfuW47Ukadgs1xw3VXUPcM9yPb8kDSu/OSlJLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyyzq0mVJDgDHgWlgqqrGk1wEfA+4AjgA/Ouq+n+La1OS9IalOOL+Z1W1uarGm8e3AfdW1Sbg3uaxJGmJLMdUyVZgZ7O8E/joMryGJA2txQZ3AX+V5IEk25vaZVV1GKC5v3SRryFJ6rCoOW7g+qo6lORSYE+Sp3rdsQn67QCXX375ItuQpOGxqCPuqjrU3B8FfghcBxxJsg6guT86x753VNV4VY2PjY0tpg1JGioLDu4k5ye54I1l4MPA48Bu4KZms5uAHy22SUnSmxYzVXIZ8MMkbzzPn1fVXyb5JbAryc3Ab4CPL75NSdIbFhzcVfVr4Jou9ZeADy6mKUnS3PzmpCS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUsvMG9xJ7kpyNMnjHbWLkuxJ8kxzf2HHutuT7E/ydJIbl6txSRpWvRxx/xmw5ZTabcC9VbUJuLd5TJKrgG3A1c0+X08yumTdSpLmD+6q+jnw8inlrcDOZnkn8NGO+t1VNVFVzwH7geuWplVJEix8jvuyqjoM0Nxf2tTXA893bHewqZ0myfYke5PsPXbs2ALbkKThs9QfTqZLrbptWFV3VNV4VY2PjY0tcRuStHItNLiPJFkH0NwfbeoHgY0d220ADi28PUnSqRYa3LuBm5rlm4AfddS3JVmT5EpgE3D/4lqUJHVaNd8GSb4L3ABckuQg8CfAnwK7ktwM/Ab4OEBV7UuyC3gCmAJuqarpZepdkobSvMFdVZ+YY9UH59h+B7BjMU1JkubmNyclqWUMbklqGYNbklrG4JakljG4JallDG5JahmDW5JaxuCWpJYxuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqGYNbklrG4JakljG4JallDG5Japl5gzvJXUmOJnm8o/aFJC8kebi5faRj3e1J9id5OsmNy9W4JA2rXo64/wzY0qX+1ara3NzuAUhyFbANuLrZ5+tJRpeqWUlSD8FdVT8HXu7x+bYCd1fVRFU9B+wHrltEf5KkUyxmjvvTSR5tplIubGrrgec7tjnY1E6TZHuSvUn2Hjt2bBFtSNJwWWhwfwN4F7AZOAx8uamny7bV7Qmq6o6qGq+q8bGxsQW2IUnDZ0HBXVVHqmq6qmaAb/LmdMhBYGPHphuAQ4trUZLUaUHBnWRdx8OPAW+ccbIb2JZkTZIrgU3A/YtrUZLUadV8GyT5LnADcEmSg8CfADck2czsNMgB4FMAVbUvyS7gCWAKuKWqppelc0kaUvMGd1V9okv5zjNsvwPYsZimJElz85uTktQyBrcktYzBLUktY3BLUssY3JLUMvOeVSKtZJMnjnPi5RdOq686dy3nXbyhDx1J8zO4NbSqiuOHn+HZPf/ttHXvuGIzm278t33oSpqfUyUaajNTJ/vdgnTWDG4NtZmpiX63IJ01g1tDbWZqst8tSGfN4NZQc6pEbWRwa6gZ3Gojg1tDbfLEq13ro6vPfYs7kXpncGt4VfHab5/tuuqCdZve4mak3hncGnLdrqwXRlad85Z3IvXK4Ja6MLg1yAxuqQuDW4PM4Ja6MLg1yOYN7iQbk/w0yZNJ9iX5TFO/KMmeJM809xd27HN7kv1Jnk5y43IOQFpyMbg12Ho54p4Cbq2q9wDvB25JchVwG3BvVW0C7m0e06zbBlwNbAG+nmR0OZqXlsvIqtX9bkGa07zBXVWHq+rBZvk48CSwHtgK7Gw22wl8tFneCtxdVRNV9RywH7huifuWFq1mpqC6nVUCziJqkJ3VuzPJFcC1wH3AZVV1GGbDHbi02Ww98HzHbgeb2qnPtT3J3iR7jx07toDWpcWZmZ6iaqbfbUhnrefgTrIW+D7w2arq/nWzZtMutdMOa6rqjqoar6rxsbGxXtuQlkxNTxrcaqWegjvJamZD+ztV9YOmfCTJumb9OuBoUz8IbOzYfQNwaGnalZbOzPQUzBjcap9ezioJcCfwZFV9pWPVbuCmZvkm4Ecd9W1J1iS5EtgE3L90LUtLo6anqDnnuKXB1culy64HPgk8luThpvZ54E+BXUluBn4DfBygqvYl2QU8wewZKbdU1fRSNy4tlnPcaqt5g7uqfkH3eWuAD86xzw5gxyL6kpbdxPFjTJ88cVr9nPMvYnT1mj50JPXGc540tCZPvEpNn34FnHPWXsjIar+Ao8FlcEunGBldReI/DQ0u353SKTK6GgxuDTDfndIp4hG3BpzvTukUI6OrIXN9Hi/1n8EtnWJkdBUZ8Z+GBpfvTg2lqprzB6YyMsrcZ8BK/Wdwa2jNTJ1+KuAb4lSJBpjBrSFVzExN9LsJaUEMbg2nOvMRtzTIDG4NrZmpk/1uQVoQg1tDqpjp8nV3qQ0Mbg2lAmYm55jj9oNJDTiDW0OpZqY5fviZ0+oZGWXtP9jUh46k3hncGk5VXX8ZEMKqc857y9uRzobBLZ1iZNXqfrcgnZHBLXUKjKzyt7g12Axu6ffE4NbA6+ViwRuT/DTJk0n2JflMU/9CkheSPNzcPtKxz+1J9id5OsmNyzkAaSkFp0o0+Hq5WPAUcGtVPZjkAuCBJHuadV+tqi91bpzkKmAbcDXwTuAnSd7tBYPVCgkjox5xa7DNe8RdVYer6sFm+TjwJLD+DLtsBe6uqomqeg7YD1y3FM1KS6Vmpun+24D4k64aeGf1Dk1yBXAtcF9T+nSSR5PcleTCprYeeL5jt4OcOeilt9zM9CTUTL/bkBak5+BOshb4PvDZqnoV+AbwLmAzcBj48hubdtn9tIObJNuT7E2y99ixY2fbt7QoM1OTc/4etzToegruJKuZDe3vVNUPAKrqSFVNV9UM8E3enA45CGzs2H0DcOjU56yqO6pqvKrGx8bGFjMG6azV9CQ152SJNNh6OaskwJ3Ak1X1lY76uo7NPgY83izvBrYlWZPkSmATcP/StSwtnkfcarNeziq5Hvgk8FiSh5va54FPJNnM7DTIAeBTAFW1L8ku4Almz0i5xTNKNGhq2uBWe80b3FX1C7rPW99zhn12ADsW0Ze0rE6+/nfMzJx+PDG65nyIZ5VosPkO1VA68fILXX9k6ryLN/rNSQ08g1vqMLJqtRcK1sAzuKUOI6OrvZCCBp7BLXXI6Gq6f6QjDQ6DW+rgVInawOCWOoyscqpEg8/g1tCpM5y/PeJUiVrA4NZQmul6vUmAOFWigWdwa/hUnSG4pcFncGvoVM1QUwa32svg1hDyiFvtZnBr+FTN/jqg1FK9/DqgNPCmp6d56KGHOHny5LzbpqYZffFo13NHnn/+ef7vyb+Z9znOO+88rrnmGj/IVF8Y3FoRJiYm2Lp1K4cOnXbNjtO8/fw1/M/Pf4x1F1/we/W/PznFF/7Tl/jrB5+b9zne85738NhjjzE6OrrgnqWFMrg1dEZHRnj72nP5u8lLOHzyDwgzvHPNs2T6CMde+V2/25PmZXBrKL14ciO/OvlhJmsNAC9MvJs/HL2Hk5Ne80ODzw8nNXRO1rk8evwGJutcZr8lGSZm1vLI8Rt4/aRz1hp8BreGTlWY7vLH5uTMKiY84lYL9HKx4HOT3J/kkST7knyxqV+UZE+SZ5r7Czv2uT3J/iRPJ7lxOQcgna2RzLBm5PXT6mtynJOTniaowdfLEfcE8IGqugbYDGxJ8n7gNuDeqtoE3Ns8JslVwDbgamAL8PUkfvSugbE6E7zvgj2sHX2ZMEOY5u2rjnLN2p8wNTX/6YRSv/VyseACXmserm5uBWwFbmjqO4GfAf+hqd9dVRPAc0n2A9cBfzvXa0xOTvLb3/52YSOQgBMnTjA93ds0x+/+/iT//fs/Zoqf8dLkO0mKi1e/QE29xomJ3o64p6enOXLkCCMjzjZqeUye4a+/ns4qaY6YHwD+EPhaVd2X5LKqOgxQVYeTXNpsvh74Px27H2xqc3rppZf41re+1UsrUleTk5OcOHGip20nJqf5X3/zq+bRgwt6vVdeeYVvf/vbfgFHy+all16ac11PwV1V08DmJO8AfpjkvWfYvNs7+bQfQE6yHdgOcPnll/O5z32ul1akrl5//XW+9rWv8eqrr74lr3fxxRdz6623+gUcLZvvfe97c647q7/zquoVZqdEtgBHkqwDaO6PNpsdBDZ27LYBOO3rbFV1R1WNV9X42NjY2bQhSUOtl7NKxpojbZK8DfgQ8BSwG7ip2ewm4EfN8m5gW5I1Sa4ENgH3L3HfkjS0epkqWQfsbOa5R4BdVfXjJH8L7EpyM/Ab4OMAVbUvyS7gCWAKuKWZapEkLYFezip5FLi2S/0l4INz7LMD2LHo7iRJp/FcJklqGYNbklrGXwfUijA6OsqWLVt4+eWX35LX27hxo+dwq28Mbq0Ia9as4c477+x3G9JbwqkSSWoZg1uSWsbglqSWMbglqWUMbklqGYNbklrG4JakljG4JallDG5JahmDW5JaxuCWpJYxuCWpZQxuSWoZg1uSWqaXiwWfm+T+JI8k2Zfki039C0leSPJwc/tIxz63J9mf5OkkNy7nACRp2PTye9wTwAeq6rUkq4FfJPmLZt1Xq+pLnRsnuQrYBlwNvBP4SZJ3e8FgSVoa8x5x16zXmoerm1udYZetwN1VNVFVzwH7gesW3akkCehxjjvJaJKHgaPAnqq6r1n16SSPJrkryYVNbT3wfMfuB5uaJGkJ9BTcVTVdVZuBDcB1Sd4LfAN4F7AZOAx8udm824X4TjtCT7I9yd4ke48dO7aA1iVpOJ3VWSVV9QrwM2BLVR1pAn0G+CZvToccBDZ27LYBONTlue6oqvGqGh8bG1tI75I0lHo5q2QsyTua5bcBHwKeSrKuY7OPAY83y7uBbUnWJLkS2ATcv6RdS9IQ6+WsknXAziSjzAb9rqr6cZJvJdnM7DTIAeBTAFW1L8ku4AlgCrjFM0okaenMG9xV9ShwbZf6J8+wzw5gx+JakyR14zcnJallDG5JahmDW5JaxuCWpJYxuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqGYNbklrG4JakljG4JallDG5JahmDW5JaxuCWpJYxuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqmVRVv3sgyTHgd8CL/e5lGVyC42qblTo2x9Uu/7CqxrqtGIjgBkiyt6rG+93HUnNc7bNSx+a4Vg6nSiSpZQxuSWqZQQruO/rdwDJxXO2zUsfmuFaIgZnjliT1ZpCOuCVJPeh7cCfZkuTpJPuT3Nbvfs5WkruSHE3yeEftoiR7kjzT3F/Yse72ZqxPJ7mxP13PL8nGJD9N8mSSfUk+09RbPbYk5ya5P8kjzbi+2NRbPa43JBlN8lCSHzePV8q4DiR5LMnDSfY2tRUxtgWpqr7dgFHgWeAPgHOAR4Cr+tnTAsbwT4H3AY931P4LcFuzfBvwn5vlq5oxrgGubMY+2u8xzDGudcD7muULgF81/bd6bECAtc3yauA+4P1tH1fH+P4d8OfAj1fKe7Hp9wBwySm1FTG2hdz6fcR9HbC/qn5dVSeBu4Gtfe7prFTVz4GXTylvBXY2yzuBj3bU766qiap6DtjP7H+DgVNVh6vqwWb5OPAksJ6Wj61mvdY8XN3cipaPCyDJBuBfAP+jo9z6cZ3BSh7bGfU7uNcDz3c8PtjU2u6yqjoMswEIXNrUWzneJFcA1zJ7dNr6sTXTCQ8DR4E9VbUixgX8V+DfAzMdtZUwLpj9n+tfJXkgyfamtlLGdtZW9fn106W2kk9zad14k6wFvg98tqpeTboNYXbTLrWBHFtVTQObk7wD+GGS955h81aMK8m/BI5W1QNJbuhlly61gRtXh+ur6lCSS4E9SZ46w7ZtG9tZ6/cR90FgY8fjDcChPvWylI4kWQfQ3B9t6q0ab5LVzIb2d6rqB015RYwNoKpeAX4GbKH947oe+FdJDjA75fiBJN+m/eMCoKoONfdHgR8yO/WxIsa2EP0O7l8Cm5JcmeQcYBuwu889LYXdwE3N8k3Ajzrq25KsSXIlsAm4vw/9zSuzh9Z3Ak9W1Vc6VrV6bEnGmiNtkrwN+BDwFC0fV1XdXlUbquoKZv8d/XVV/RtaPi6AJOcnueCNZeDDwOOsgLEtWL8/HQU+wuwZC88Cf9zvfhbQ/3eBw8Aks/+nvxm4GLgXeKa5v6hj+z9uxvo08Ef97v8M4/onzP55+SjwcHP7SNvHBvwj4KFmXI8D/7Gpt3pcp4zxBt48q6T142L2rLNHmtu+N3JiJYxtoTe/OSlJLdPvqRJJ0lkyuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqGYNbklrm/wOzokbBtI3x9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 20.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        #定义模型\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 2),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.model_td = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer_td = torch.optim.Adam(self.model_td.parameters(),\n",
    "                                             lr=1e-2)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #得到一个动作\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).reshape(1, 4)\n",
    "        #[1, 4] -> [1, 2]\n",
    "        prob = self.model(state)\n",
    "\n",
    "        #根据概率选择一个动作\n",
    "        action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _get_advantages(self, deltas):\n",
    "        advantages = []\n",
    "\n",
    "        #反向遍历deltas\n",
    "        s = 0.0\n",
    "        for delta in deltas[::-1]:\n",
    "            s = 0.98 * 0.95 * s + delta\n",
    "            advantages.append(s)\n",
    "\n",
    "        #逆序\n",
    "        advantages.reverse()\n",
    "        return advantages\n",
    "\n",
    "    def train(self, states, rewards, actions, next_states, overs):\n",
    "        #states -> [b, 4]\n",
    "        #rewards -> [b, 1]\n",
    "        #actions -> [b, 1]\n",
    "        #next_states -> [b, 4]\n",
    "        #overs -> [b, 1]\n",
    "\n",
    "        #计算values和targets\n",
    "        #[b, 4] -> [b, 1]\n",
    "        values = self.model_td(states)\n",
    "\n",
    "        #[b, 4] -> [b, 1]\n",
    "        targets = self.model_td(next_states) * 0.98\n",
    "        targets *= (1 - overs)\n",
    "        targets += rewards\n",
    "\n",
    "        #[b, 1]\n",
    "        deltas = (targets - values).squeeze(dim=1).tolist()\n",
    "        advantages = self._get_advantages(deltas)\n",
    "        advantages = torch.FloatTensor(advantages).reshape(-1, 1)\n",
    "\n",
    "        #取出每一步的动作概率\n",
    "        #[b, 2] -> [b, 2] -> [b, 1]\n",
    "        old_probs = self.model(states)\n",
    "        old_probs = old_probs.gather(1, actions)\n",
    "        old_probs = old_probs.detach()\n",
    "\n",
    "        #每批数据反复训练10次\n",
    "        for _ in range(10):\n",
    "            #[b, 4] -> [b, 2]\n",
    "            new_probs = self.model(states)\n",
    "\n",
    "            #[b, 2] -> [b, 1]\n",
    "            new_probs = new_probs.gather(1, actions)\n",
    "            new_probs = new_probs\n",
    "\n",
    "            #[b, 1] - [b, 1] -> [b, 1]\n",
    "            ratios = new_probs / old_probs\n",
    "\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr1 = ratios * advantages\n",
    "\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            values = self.model_td(states)\n",
    "            loss_td = self.loss_fn(values, targets.detach())\n",
    "\n",
    "            #更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.optimizer_td.zero_grad()\n",
    "            loss_td.backward()\n",
    "            self.optimizer_td.step()\n",
    "\n",
    "    def get_data(self):\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        next_states = []\n",
    "        overs = []\n",
    "\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            overs.append(over)\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "        #[b, 4]\n",
    "        states = torch.FloatTensor(states).reshape(-1, 4)\n",
    "        #[b, 1]\n",
    "        rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
    "        #[b, 1]\n",
    "        actions = torch.LongTensor(actions).reshape(-1, 1)\n",
    "        #[b, 4]\n",
    "        next_states = torch.FloatTensor(next_states).reshape(-1, 4)\n",
    "        #[b, 1]\n",
    "        overs = torch.LongTensor(overs).reshape(-1, 1)\n",
    "\n",
    "        return states, rewards, actions, next_states, overs\n",
    "\n",
    "    def test(self, play):\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #记录反馈值的和,这个值越大越好\n",
    "        reward_sum = 0\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            state, reward, over, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "\n",
    "            #打印动画\n",
    "            if play and random.random() < 0.2:  #跳帧\n",
    "                display.clear_output(wait=True)\n",
    "                show()\n",
    "\n",
    "        return reward_sum\n",
    "\n",
    "\n",
    "teacher = PPO()\n",
    "\n",
    "teacher.train(*teacher.get_data())\n",
    "\n",
    "teacher.get_action([1, 2, 3, 4]), teacher.test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 18.0\n",
      "50 195.2\n",
      "100 200.0\n",
      "150 200.0\n",
      "200 187.0\n",
      "250 200.0\n",
      "300 200.0\n",
      "350 185.2\n",
      "400 200.0\n",
      "450 200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    teacher.train(*teacher.get_data())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        test_result = sum([teacher.test(play=False) for _ in range(10)]) / 10\n",
    "        print(i, test_result)\n",
    "\n",
    "teacher.test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 4]), torch.Size([200, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用训练好的模型获取一批教师数据\n",
    "teacher_states, _, teacher_actions, _, _ = teacher.get_data()\n",
    "\n",
    "#删除教师,只留下教师的数据就可以了\n",
    "del teacher\n",
    "\n",
    "teacher_states.shape, teacher_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PPO at 0x7f776be4dcf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化学生模型\n",
    "student = PPO()\n",
    "\n",
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5324],\n",
       "        [0.5758]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义鉴别器网络,它的任务是鉴定一批数据是出自teacher还是student\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        one_hot = torch.nn.functional.one_hot(actions.squeeze(dim=1),\n",
    "                                              num_classes=2)\n",
    "\n",
    "        cat = torch.cat([states, one_hot], dim=1)\n",
    "\n",
    "        return self.sequential(cat)\n",
    "\n",
    "\n",
    "discriminator = Discriminator()\n",
    "\n",
    "discriminator(torch.randn(2, 4), torch.ones(2, 1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30.4\n",
      "50 191.1\n",
      "100 187.5\n",
      "150 192.9\n",
      "200 200.0\n",
      "250 197.0\n",
      "300 200.0\n",
      "350 200.0\n",
      "400 200.0\n",
      "450 200.0\n"
     ]
    }
   ],
   "source": [
    "#模仿学习\n",
    "def copy_learn():\n",
    "    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "    for i in range(500):\n",
    "        #使用学生模型获取一局游戏的数据,不需要reward\n",
    "        states, _, actions, next_states, overs = student.get_data()\n",
    "\n",
    "        #使用鉴别器鉴定两批数据是来自教师的还是学生的\n",
    "        prob_teacher = discriminator(teacher_states, teacher_actions)\n",
    "        prob_student = discriminator(states, actions)\n",
    "\n",
    "        #老师的用0表示,学生的用1表示,计算二分类loss\n",
    "        loss_teacher = bce_loss(prob_teacher, torch.zeros_like(prob_teacher))\n",
    "        loss_student = bce_loss(prob_student, torch.ones_like(prob_student))\n",
    "        loss = loss_teacher + loss_student\n",
    "\n",
    "        #调整鉴别器的loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #使用一批数据来自学生的概率作为reward,取log,再符号取反\n",
    "        #因为鉴别器会把学生数据的概率贴近1,所以目标是让鉴别器无法分辨,这是一种对抗网络的思路\n",
    "        rewards = -prob_student.log().detach()\n",
    "\n",
    "        #更新学生模型参数,使用PPO模型本身的更新方式\n",
    "        student.train(states, rewards, actions, next_states, overs)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            test_result = sum([student.test(play=False)\n",
    "                               for _ in range(10)]) / 10\n",
    "            print(i, test_result)\n",
    "\n",
    "\n",
    "copy_learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUMElEQVR4nO3db4xdd53f8ffHY8fkD1kSPLGM7TQu65VIaNdBIxcpbUmBbrzZCsMDdo1U5AeRnAeJBOqWNtmVuvDAYlst0CcFKZQIi7IESxDFRWm7JgtFrGiM899O4o1DDBls4okpm4Q/jj3z7YM5bu7aM57x3DlzfWbeL+nqnvs9v3Pv9xfZnxz/7rn3pqqQJHXHskE3IEm6MAa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1TGvBnWRLkkNJDie5q63XkaSlJm1cx51kCPhb4F8Co8APgY9U1dPz/mKStMS0dca9GThcVT+qqteB+4CtLb2WJC0py1t63rXAiz2PR4F/Mt3gVatW1XXXXddSK5LUPUeOHOHll1/OVPvaCu6pXuzvrckk2QHsALj22mvZv39/S61IUveMjIxMu6+tpZJRYH3P43XA0d4BVXVPVY1U1cjw8HBLbUjS4tNWcP8Q2JhkQ5JLgG3AnpZeS5KWlFaWSqrqdJI7gf8FDAH3VtXBNl5Lkpaatta4qaoHgQfben5JWqr85KQkdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1jMEtSR1jcEtSxxjcktQxBrckdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHdPXT5clOQK8CowDp6tqJMnVwNeB64AjwB9W1f/tr01J0hnzccb9L6pqU1WNNI/vAh6qqo3AQ81jSdI8aWOpZCuwq9neBXywhdeQpCWr3+Au4K+SPJJkR1NbXVXHAJr7a/p8DUlSj77WuIGbqupokmuAvUmene2BTdDvALj22mv7bEOSlo6+zrir6mhzfxy4H9gMvJRkDUBzf3yaY++pqpGqGhkeHu6nDUlaUuYc3EkuT/LmM9vA7wEHgD3A9mbYduCBfpuUJL2hn6WS1cD9Sc48z19W1f9M8kNgd5LbgJ8AH+6/TUnSGXMO7qr6EfC7U9RPAO/rpylJ0vT85KQkdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1jMEtSR1jcEtSxxjcktQxBrckdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHWNwS1LHzBjcSe5NcjzJgZ7a1Un2Jnmuub+qZ9/dSQ4nOZTklrYal6SlajZn3F8GtpxVuwt4qKo2Ag81j0lyPbANuKE55vNJhuatW0nSzMFdVd8Dfn5WeSuwq9neBXywp35fVZ2sqheAw8Dm+WlVkgRzX+NeXVXHAJr7a5r6WuDFnnGjTe0cSXYk2Z9k/9jY2BzbkKSlZ77fnMwUtZpqYFXdU1UjVTUyPDw8z21I0uI11+B+KckagOb+eFMfBdb3jFsHHJ17e5Kks801uPcA25vt7cADPfVtSVYm2QBsBPb116IkqdfymQYk+RpwM7AqySjwZ8CfA7uT3Ab8BPgwQFUdTLIbeBo4DdxRVeMt9S5JS9KMwV1VH5lm1/umGb8T2NlPU5Kk6fnJSUnqGINbkjrG4JakjjG4JaljDG5J6hiDW5I6xuCWpI4xuCWpYwxuSeoYg1uSOsbglqSOMbglqWMMbknqGINbkjrG4JakjjG4JaljDG5J6hiDW5I6ZsbgTnJvkuNJDvTUPpnkp0keb2639uy7O8nhJIeS3NJW45K0VM3mjPvLwJYp6p+rqk3N7UGAJNcD24AbmmM+n2RovpqVJM0iuKvqe8DPZ/l8W4H7qupkVb0AHAY299GfJOks/axx35nkyWYp5aqmthZ4sWfMaFM7R5IdSfYn2T82NtZHG5K0tMw1uL8AvB3YBBwDPtPUM8XYmuoJquqeqhqpqpHh4eE5tiFJS8+cgruqXqqq8aqaAL7IG8sho8D6nqHrgKP9tShJ6jWn4E6ypufhh4AzV5zsAbYlWZlkA7AR2Ndfi5KkXstnGpDka8DNwKoko8CfATcn2cTkMsgR4HaAqjqYZDfwNHAauKOqxlvpXJKWqBmDu6o+MkX5S+cZvxPY2U9TkqTp+clJSeoYg1uSOsbglqSOMbglqWMMbknqmBmvKpE0vZOvjHHy1RPn1FdeuYqVb141gI60FBjcUh/GDv0Nxx598Jz620Y+wNve9QckU30LhNQfl0qkFtSEnztTewxuqQU1fpppvl9N6pvBLbWgJsbNbbXG4JZaMLlUYnKrHQa31IKaGKfK4FY7DG6pBb45qTYZ3FILasI3J9Ueg1tqgW9Oqk0Gt9SCCd+cVIsMbqkvU38yssbHjW21xuCW+nD5W9eToRXn1H/18o+p068PoCMtBTMGd5L1Sb6T5JkkB5N8rKlfnWRvkuea+6t6jrk7yeEkh5Lc0uYEpEEaWnkZybl/jcZP/cbLAdWa2Zxxnwb+uKreAbwbuCPJ9cBdwENVtRF4qHlMs28bcAOwBfh8kqE2mpcGLcuGplstkVozY3BX1bGqerTZfhV4BlgLbAV2NcN2AR9strcC91XVyap6ATgMbJ7nvqWLQpYtx+TWQrugNe4k1wE3Ag8Dq6vqGEyGO3BNM2wt8GLPYaNN7ezn2pFkf5L9Y2Njc2hdGrwMDfnVrVpwsw7uJFcA3wA+XlWvnG/oFLVzFvuq6p6qGqmqkeHh4dm2IV1UXAXUIMwquJOsYDK0v1pV32zKLyVZ0+xfAxxv6qPA+p7D1wFH56dd6eKSoSFcKtFCm81VJQG+BDxTVZ/t2bUH2N5sbwce6KlvS7IyyQZgI7Bv/lqWLh6+OalBmM1Pl90EfBR4KsnjTe1PgD8Hdie5DfgJ8GGAqjqYZDfwNJNXpNxRVX7jjhalLPOMWwtvxuCuqu8z/Z/M901zzE5gZx99SZ2QZUPGthacn5yU+jC5VGJ0a2EZ3FIflrlUogEwuKV+nOds2x9TUFsMbqkNZXCrPQa31IpqfgVHmn8Gt9SSGveMW+0wuKWWTLhUopYY3FILqlwqUXsMbqklLpWoLQa31BLPuNUWg1tqRXk5oFpjcEtt8Dputcjglvo15acni4nxUwveipYGg1vqQ5Yt54rVbz+nXhPjvPbS8wPoSEuBwS31aWjFyinrNe6bk2qHwS31IyFDs/k9Emn+GNxSH8KZX8GRFo7BLfUpyzzj1sKazY8Fr0/ynSTPJDmY5GNN/ZNJfprk8eZ2a88xdyc5nORQklvanIA0WGHZkGfcWlizOVU4DfxxVT2a5M3AI0n2Nvs+V1V/0Ts4yfXANuAG4G3At5P8jj8YrMXKpRIttBnPuKvqWFU92my/CjwDrD3PIVuB+6rqZFW9ABwGNs9Hs9JFJy6VaOFd0Bp3kuuAG4GHm9KdSZ5Mcm+Sq5raWuDFnsNGOX/QSx0Wz7i14GYd3EmuAL4BfLyqXgG+ALwd2AQcAz5zZugUh9cUz7cjyf4k+8fGxi60b+miEde4tcBmFdxJVjAZ2l+tqm8CVNVLVTVeVRPAF3ljOWQUWN9z+Drg6NnPWVX3VNVIVY0MDw/3MwdpoFwq0UKbzVUlAb4EPFNVn+2pr+kZ9iHgQLO9B9iWZGWSDcBGYN/8tSxdXJZNs1RSVVSd849NqW+zOVW4Cfgo8FSSx5vanwAfSbKJyWWQI8DtAFV1MMlu4Gkmr0i5wytKtFhlyi+YatQEk389zjNGmoMZg7uqvs/Uf/IePM8xO4GdffQldV5NjJvbaoWfnJRaUhMTLpWoFQa31JLJH1IwuDX/DG6pJZNLJQa35p/BLbWkJsYpz7jVAoNbaknVhGfcaoXBLbXEpRK1xeCWWuJSidpicEstqQmXStQOg1vq0+S3A577KZuJ8dcn17mleWZwS3269K3rGFp52Tn1X58YZfzkrwbQkRY7g1vq07Kh5STn/lWa/NSkSyWafwa31KdkCM73ZVPSPPOLhKVpPPXUU7z66qszDzz5CstPnTpnlbuqeOyxx6hLrpzxKZYvX86NN97IihUr5taslhSDW5rG7bffzg9+8IMZx6156xV88d9+gGuuuvzv1cfHx/mjP/wjXhx7ZcbnuPLKK3n++edZtWrVnPvV0mFwS30aH5/8wYRfjl/JT3+zkXGWs/qSI1y57GeDbk2LlMEt9en0xAR/d/pqnv273+dXE5PLIi/+5h2849L/Tfll3GqBb05KfTp9unjylffwq4nfYvJ67jBel3DgtX/GryeuGHR7WoQMbqlPpyeKUxPn/uN1guWUf8XUgtn8WPCbkuxL8kSSg0k+1dSvTrI3yXPN/VU9x9yd5HCSQ0luaXMC0qCNj4+zctm5V59ckl+zjNMD6EiL3WxOB04C762q3wU2AVuSvBu4C3ioqjYCDzWPSXI9sA24AdgCfD7J1D+DLS0C4xMT/KMrvsPVK44SxgkTXLrsFW688ttcuuyXg25Pi9Bsfiy4gNeahyuaWwFbgZub+i7gu8C/b+r3VdVJ4IUkh4HNwLTXVZ06dYqf/cx34HVxOXXq1KzGjY9P8OX//j1WvulRXj61joka4qoVP+Nvlr3Cz1/99ayeo6o4fvw4p097hq5J5/vzN6urSpoz5keA3wb+S1U9nGR1VR0DqKpjSa5phq8F/k/P4aNNbVonTpzgK1/5ymxakRbM2NjYrMYVsPeRHzWPnpzTa73++uvs3r2byy+/fObBWhJOnDgx7b5ZBXdVjQObkrwFuD/JO88zfKrrn875woYkO4AdANdeey2f+MQnZtOKtGDuv/9+fvzjHy/Ia61cuZI777zTD+Do//v6178+7b4Lesu7qn7B5JLIFuClJGsAmvvjzbBRYH3PYeuAo1M81z1VNVJVI8PDwxfShiQtabO5qmS4OdMmyaXA+4FngT3A9mbYduCBZnsPsC3JyiQbgI3AvnnuW5KWrNkslawBdjXr3MuA3VX1rSQ/AHYnuQ34CfBhgKo6mGQ38DRwGrijWWqRJM2D2VxV8iRw4xT1E8D7pjlmJ7Cz7+4kSefwY12S1DEGtyR1jN8OKE3jPe95D6tXr16Q17rsssu45JJLFuS11H0GtzSNT3/604NuQZqSSyWS1DEGtyR1jMEtSR1jcEtSxxjcktQxBrckdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1zGx+LPhNSfYleSLJwSSfauqfTPLTJI83t1t7jrk7yeEkh5Lc0uYEJGmpmc33cZ8E3ltVryVZAXw/yf9o9n2uqv6id3CS64FtwA3A24BvJ/kdfzBYkubHjGfcNem15uGK5lbnOWQrcF9VnayqF4DDwOa+O5UkAbNc404ylORx4Diwt6oebnbdmeTJJPcmuaqprQVe7Dl8tKlJkubBrIK7qsarahOwDtic5J3AF4C3A5uAY8BnmuGZ6inOLiTZkWR/kv1jY2NzaF2SlqYLuqqkqn4BfBfYUlUvNYE+AXyRN5ZDRoH1PYetA45O8Vz3VNVIVY0MDw/PpXdJWpJmc1XJcJK3NNuXAu8Hnk2ypmfYh4ADzfYeYFuSlUk2ABuBffPatSQtYbO5qmQNsCvJEJNBv7uqvpXkK0k2MbkMcgS4HaCqDibZDTwNnAbu8IoSSZo/MwZ3VT0J3DhF/aPnOWYnsLO/1iRJU/GTk5LUMQa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1jMEtSR1jcEtSxxjcktQxBrckdYzBLUkdY3BLUscY3JLUMQa3JHWMwS1JHWNwS1LHGNyS1DEGtyR1jMEtSR1jcEtSxxjcktQxBrckdUyqatA9kGQM+CXw8qB7acEqnFfXLNa5Oa9u+QdVNTzVjosiuAGS7K+qkUH3Md+cV/cs1rk5r8XDpRJJ6hiDW5I65mIK7nsG3UBLnFf3LNa5Oa9F4qJZ45Ykzc7FdMYtSZqFgQd3ki1JDiU5nOSuQfdzoZLcm+R4kgM9tauT7E3yXHN/Vc++u5u5Hkpyy2C6nlmS9Um+k+SZJAeTfKypd3puSd6UZF+SJ5p5faqpd3peZyQZSvJYkm81jxfLvI4keSrJ40n2N7VFMbc5qaqB3YAh4HngHwKXAE8A1w+ypznM4Z8D7wIO9NT+E3BXs30X8B+b7eubOa4ENjRzHxr0HKaZ1xrgXc32m4G/bfrv9NyAAFc02yuAh4F3d31ePfP7N8BfAt9aLH8Wm36PAKvOqi2Kuc3lNugz7s3A4ar6UVW9DtwHbB1wTxekqr4H/Pys8lZgV7O9C/hgT/2+qjpZVS8Ah5n8b3DRqapjVfVos/0q8Aywlo7PrSa91jxc0dyKjs8LIMk64A+A/9pT7vy8zmMxz+28Bh3ca4EXex6PNrWuW11Vx2AyAIFrmnon55vkOuBGJs9OOz+3ZjnhceA4sLeqFsW8gP8M/Dtgoqe2GOYFk/9z/askjyTZ0dQWy9wu2PIBv36mqC3my1w6N98kVwDfAD5eVa8kU01hcugUtYtyblU1DmxK8hbg/iTvPM/wTswryb8CjlfVI0luns0hU9Quunn1uKmqjia5Btib5NnzjO3a3C7YoM+4R4H1PY/XAUcH1Mt8einJGoDm/nhT79R8k6xgMrS/WlXfbMqLYm4AVfUL4LvAFro/r5uADyQ5wuSS43uT/De6Py8Aqupoc38cuJ/JpY9FMbe5GHRw/xDYmGRDkkuAbcCeAfc0H/YA25vt7cADPfVtSVYm2QBsBPYNoL8ZZfLU+kvAM1X12Z5dnZ5bkuHmTJsklwLvB56l4/Oqqrural1VXcfk36O/rqp/TcfnBZDk8iRvPrMN/B5wgEUwtzkb9LujwK1MXrHwPPCng+5nDv1/DTgGnGLy//S3AW8FHgKea+6v7hn/p81cDwG/P+j+zzOvf8rkPy+fBB5vbrd2fW7APwYea+Z1APgPTb3T8zprjjfzxlUlnZ8Xk1edPdHcDp7JicUwt7ne/OSkJHXMoJdKJEkXyOCWpI4xuCWpYwxuSeoYg1uSOsbglqSOMbglqWMMbknqmP8H7ekymLbQlXgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

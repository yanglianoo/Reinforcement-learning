{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\envs\\registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\ipykernel_launcher.py:141: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 9.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self):\n",
    "        #定义模型\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 2),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.model_td = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer_td = torch.optim.Adam(self.model_td.parameters(),\n",
    "                                             lr=1e-2)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #得到一个动作\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).reshape(1, 4)\n",
    "        #[1, 4] -> [1, 2]\n",
    "        prob = self.model(state)\n",
    "\n",
    "        #根据概率选择一个动作\n",
    "        action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _get_advantages(self, deltas):\n",
    "        advantages = []\n",
    "\n",
    "        #反向遍历deltas\n",
    "        s = 0.0\n",
    "        for delta in deltas[::-1]:\n",
    "            s = 0.98 * 0.95 * s + delta\n",
    "            advantages.append(s)\n",
    "\n",
    "        #逆序\n",
    "        advantages.reverse()\n",
    "        return advantages\n",
    "\n",
    "    def train(self, states, rewards, actions, next_states, overs):\n",
    "        #states -> [b, 4]\n",
    "        #rewards -> [b, 1]\n",
    "        #actions -> [b, 1]\n",
    "        #next_states -> [b, 4]\n",
    "        #overs -> [b, 1]\n",
    "\n",
    "        #计算values和targets\n",
    "        #[b, 4] -> [b, 1]\n",
    "        values = self.model_td(states)\n",
    "\n",
    "        #[b, 4] -> [b, 1]\n",
    "        targets = self.model_td(next_states) * 0.98\n",
    "        targets *= (1 - overs)\n",
    "        targets += rewards\n",
    "\n",
    "        #[b, 1]\n",
    "        deltas = (targets - values).squeeze(dim=1).tolist()\n",
    "        advantages = self._get_advantages(deltas)\n",
    "        advantages = torch.FloatTensor(advantages).reshape(-1, 1)\n",
    "\n",
    "        #取出每一步的动作概率\n",
    "        #[b, 2] -> [b, 2] -> [b, 1]\n",
    "        old_probs = self.model(states)\n",
    "        old_probs = old_probs.gather(1, actions)\n",
    "        old_probs = old_probs.detach()\n",
    "\n",
    "        #每批数据反复训练10次\n",
    "        for _ in range(10):\n",
    "            #[b, 4] -> [b, 2]\n",
    "            new_probs = self.model(states)\n",
    "\n",
    "            #[b, 2] -> [b, 1]\n",
    "            new_probs = new_probs.gather(1, actions)\n",
    "            new_probs = new_probs\n",
    "\n",
    "            #[b, 1] - [b, 1] -> [b, 1]\n",
    "            ratios = new_probs / old_probs\n",
    "\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr1 = ratios * advantages\n",
    "\n",
    "            #[b, 1] * [b, 1] -> [b, 1]\n",
    "            surr2 = torch.clamp(ratios, 0.8, 1.2) * advantages\n",
    "\n",
    "            loss = -torch.min(surr1, surr2)\n",
    "            loss = loss.mean()\n",
    "\n",
    "            values = self.model_td(states)\n",
    "            loss_td = self.loss_fn(values, targets.detach())\n",
    "\n",
    "            #更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.optimizer_td.zero_grad()\n",
    "            loss_td.backward()\n",
    "            self.optimizer_td.step()\n",
    "\n",
    "    def get_data(self):\n",
    "        states = []\n",
    "        rewards = []\n",
    "        actions = []\n",
    "        next_states = []\n",
    "        overs = []\n",
    "\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            states.append(state)\n",
    "            rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            next_states.append(next_state)\n",
    "            overs.append(over)\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "        #[b, 4]\n",
    "        states = torch.FloatTensor(states).reshape(-1, 4)\n",
    "        #[b, 1]\n",
    "        rewards = torch.FloatTensor(rewards).reshape(-1, 1)\n",
    "        #[b, 1]\n",
    "        actions = torch.LongTensor(actions).reshape(-1, 1)\n",
    "        #[b, 4]\n",
    "        next_states = torch.FloatTensor(next_states).reshape(-1, 4)\n",
    "        #[b, 1]\n",
    "        overs = torch.LongTensor(overs).reshape(-1, 1)\n",
    "\n",
    "        return states, rewards, actions, next_states, overs\n",
    "\n",
    "    def test(self, play):\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #记录反馈值的和,这个值越大越好\n",
    "        reward_sum = 0\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = self.get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            state, reward, over, _ = env.step(action)\n",
    "            reward_sum += reward\n",
    "\n",
    "            #打印动画\n",
    "            if play and random.random() < 0.2:  #跳帧\n",
    "                display.clear_output(wait=True)\n",
    "                show()\n",
    "\n",
    "        return reward_sum\n",
    "\n",
    "\n",
    "teacher = PPO()\n",
    "\n",
    "teacher.train(*teacher.get_data())\n",
    "\n",
    "teacher.get_action([1, 2, 3, 4]), teacher.test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.5\n",
      "50 157.4\n",
      "100 188.9\n",
      "150 200.0\n",
      "200 200.0\n",
      "250 195.2\n",
      "300 200.0\n",
      "350 200.0\n",
      "400 200.0\n",
      "450 200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    teacher.train(*teacher.get_data())\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        test_result = sum([teacher.test(play=False) for _ in range(10)]) / 10\n",
    "        print(i, test_result)\n",
    "\n",
    "teacher.test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([200, 4]), torch.Size([200, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用训练好的模型获取一批教师数据\n",
    "teacher_states, _, teacher_actions, _, _ = teacher.get_data()\n",
    "\n",
    "#删除教师,只留下教师的数据就可以了\n",
    "del teacher\n",
    "\n",
    "teacher_states.shape, teacher_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PPO at 0x2577eb35b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#初始化学生模型\n",
    "student = PPO()\n",
    "\n",
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4248],\n",
       "        [0.4310]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义鉴别器网络,它的任务是鉴定一批数据是出自teacher还是student\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(6, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        one_hot = torch.nn.functional.one_hot(actions.squeeze(dim=1),\n",
    "                                              num_classes=2)\n",
    "\n",
    "        cat = torch.cat([states, one_hot], dim=1)\n",
    "\n",
    "        return self.sequential(cat)\n",
    "\n",
    "\n",
    "discriminator = Discriminator()\n",
    "\n",
    "discriminator(torch.randn(2, 4), torch.ones(2, 1).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28.8\n",
      "50 172.2\n",
      "100 196.7\n",
      "150 200.0\n",
      "200 200.0\n",
      "250 200.0\n",
      "300 200.0\n",
      "350 200.0\n",
      "400 200.0\n",
      "450 200.0\n"
     ]
    }
   ],
   "source": [
    "#模仿学习\n",
    "def copy_learn():\n",
    "    optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "    for i in range(500):\n",
    "        #使用学生模型获取一局游戏的数据,不需要reward\n",
    "        states, _, actions, next_states, overs = student.get_data()\n",
    "\n",
    "        #使用鉴别器鉴定两批数据是来自教师的还是学生的\n",
    "        prob_teacher = discriminator(teacher_states, teacher_actions)\n",
    "        prob_student = discriminator(states, actions)\n",
    "\n",
    "        #老师的用0表示,学生的用1表示,计算二分类loss\n",
    "        loss_teacher = bce_loss(prob_teacher, torch.zeros_like(prob_teacher))\n",
    "        loss_student = bce_loss(prob_student, torch.ones_like(prob_student))\n",
    "        loss = loss_teacher + loss_student\n",
    "\n",
    "        #调整鉴别器的loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #使用一批数据来自学生的概率作为reward,取log,再符号取反\n",
    "        #因为鉴别器会把学生数据的概率贴近1,所以目标是让鉴别器无法分辨,这是一种对抗网络的思路\n",
    "        rewards = -prob_student.log().detach()\n",
    "\n",
    "        #更新学生模型参数,使用PPO模型本身的更新方式\n",
    "        student.train(states, rewards, actions, next_states, overs)\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            test_result = sum([student.test(play=False)\n",
    "                               for _ in range(10)]) / 10\n",
    "            print(i, test_result)\n",
    "\n",
    "\n",
    "copy_learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAATtklEQVR4nO3df6zddZ3n8eerLRRGjYBcOp22THHsBHGyFnMXMZosg3EGiVkY4xDYzUAMSWcTTDQx7sJsMmiyJEx2R3bNzpJlAorGEdhRQ2XZFQZJjJsIFi3Iz6FI2bZbaCkFYUBsy3v/uJ/iAdr7+/aez73PR/Lt+X4/38/3nPcnHF799nO+33NSVUiS+rFkvguQJE2NwS1JnTG4JakzBrckdcbglqTOGNyS1Jk5C+4k5yR5LMmWJJfP1etI0mKTubiOO8lS4B+BjwHbgZ8AF1XVw7P+YpK0yMzVGfcZwJaq+kVV/Rq4CThvjl5LkhaVZXP0vKuAbQPb24EPHq7ziSeeWGvXrp2jUiSpP1u3buXZZ5/NofbNVXBPKMkGYAPAySefzKZNm+arFEkaOqOjo4fdN1dTJTuANQPbq1vb66rquqoararRkZGROSpDkhaeuQrunwDrkpyS5GjgQmDjHL2WJC0qczJVUlX7k3wG+D6wFLihqh6ai9eSpMVmzua4q+p24Pa5en5JWqy8c1KSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmdm9NNlSbYCLwIHgP1VNZrkBOBmYC2wFbigqvbOrExJ0kGzccb9h1W1vqpG2/blwF1VtQ64q21LkmbJXEyVnAfc2NZvBM6fg9eQpEVrpsFdwB1J7kuyobWtqKqdbf1pYMUMX0OSNGBGc9zAR6pqR5KTgDuTPDq4s6oqSR3qwBb0GwBOPvnkGZYhSYvHjM64q2pHe9wFfBc4A3gmyUqA9rjrMMdeV1WjVTU6MjIykzIkaVGZdnAneVuSdxxcB/4IeBDYCFzSul0C3DrTIiVJvzGTqZIVwHeTHHyev6uq/53kJ8AtSS4FngIumHmZkqSDph3cVfUL4P2HaN8DfHQmRUmSDs87JyWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdcbglqTOTBjcSW5IsivJgwNtJyS5M8nj7fH41p4kX0myJckDST4wl8VL0mI0mTPurwHnvKntcuCuqloH3NW2AT4OrGvLBuDa2SlTknTQhMFdVT8EnntT83nAjW39RuD8gfav15gfA8clWTlLtUqSmP4c94qq2tnWnwZWtPVVwLaBfttb21sk2ZBkU5JNu3fvnmYZkrT4zPjDyaoqoKZx3HVVNVpVoyMjIzMtQ5IWjekG9zMHp0Da467WvgNYM9BvdWuTJM2S6Qb3RuCStn4JcOtA+8Xt6pIzgRcGplQkSbNg2UQdknwLOAs4Mcl24ErgauCWJJcCTwEXtO63A+cCW4CXgU/PQc2StKhNGNxVddFhdn30EH0LuGymRUmSDs87JyWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZwxuSeqMwS1JnTG4JakzBrckdWbC4E5yQ5JdSR4caPtikh1JNrfl3IF9VyTZkuSxJH88V4VL0mI1mTPurwHnHKL9mqpa35bbAZKcBlwIvK8d89+SLJ2tYiVJkwjuqvoh8Nwkn+884KaqerWqnmTs197PmEF9kqQ3mckc92eSPNCmUo5vbauAbQN9tre2t0iyIcmmJJt27949gzIkaXGZbnBfC/wesB7YCfz1VJ+gqq6rqtGqGh0ZGZlmGZK0+EwruKvqmao6UFWvAX/Lb6ZDdgBrBrqubm2SpFkyreBOsnJg80+Ag1ecbAQuTLI8ySnAOuDemZUoSRq0bKIOSb4FnAWcmGQ7cCVwVpL1QAFbgT8HqKqHktwCPAzsBy6rqgNzUrkkLVITBndVXXSI5uvH6X8VcNVMipIkHZ53TkpSZwxuSeqMwS1JnTG4JakzBrckdWbCq0qkxeDAvlf5p11PvqV9ybKjedtJp5BkHqqSDs3gloBfv7SXx/7nNVD1hvZjjvtt/uBPvwgGt4aIUyXSBIqauJN0BBnc0rjqLWfh0nwzuKXxFAa3ho7BLY2jBv6UhoXBLY2rKM+4NWQMbknqjMEtjcc5bg0hg1saV3k5oIaOwS1NxDNuDRmDW5qIwa0hY3BL4ymnSjR8JgzuJGuS3J3k4SQPJflsaz8hyZ1JHm+Px7f2JPlKki1JHkjygbkehDRXCjzj1tCZzBn3fuDzVXUacCZwWZLTgMuBu6pqHXBX2wb4OGO/7r4O2ABcO+tVS0dM4Q04GjYTBndV7ayqn7b1F4FHgFXAecCNrduNwPlt/Tzg6zXmx8BxSVbOduHSEWNua8hMaY47yVrgdOAeYEVV7Wy7ngZWtPVVwLaBw7a3tjc/14Ykm5Js2r1791Trlo6M8tsBNXwmHdxJ3g58G/hcVf1ycF+N3RM8pXd3VV1XVaNVNToyMjKVQ6UjyG8H1PCZVHAnOYqx0P5mVX2nNT9zcAqkPe5q7TuANQOHr25tUqcMbg2XyVxVEuB64JGq+vLAro3AJW39EuDWgfaL29UlZwIvDEypSN3xS6Y0bCbz02UfBv4M+HmSza3tL4CrgVuSXAo8BVzQ9t0OnAtsAV4GPj2bBUtHVDlVouEzYXBX1Y+Aw/3g3kcP0b+Ay2ZYlzQUjGwNI++clCbiGbeGjMEtjcsfUtDwMbil8dTrf0hDw+CWJuIZt4aMwS2Ny28H1PAxuKWJeMatIWNwS+MxtDWEDG5pHGO/FWx4a7gY3NKEDG4NF4NbGpe3vGv4GNzSeMbmSua7CukNDG5pAl4OqGFjcEvQvkbtUN+lVlCvHeFipPEZ3BKwbPlvcewJv/OW9v2/+ideee7/zUNF0uEZ3BJAlrBkyaG+5bio1w4c8XKk8RjcEgdnSg73tfPScDG4JWAsug1u9cHglgAC8YxbnZjMjwWvSXJ3koeTPJTks639i0l2JNnclnMHjrkiyZYkjyX547kcgDRrDG51YjI/Frwf+HxV/TTJO4D7ktzZ9l1TVf9psHOS04ALgfcBvwP8Q5Lfryo/4dEQc6pE/ZjwjLuqdlbVT9v6i8AjwKpxDjkPuKmqXq2qJxn7tfczZqNYaU6Z2+rElOa4k6wFTgfuaU2fSfJAkhuSHN/aVgHbBg7bzvhBLw2FmNzqxKSDO8nbgW8Dn6uqXwLXAr8HrAd2An89lRdOsiHJpiSbdu/ePZVDpdmXOMetbkwquJMcxVhof7OqvgNQVc9U1YGqeg34W34zHbIDWDNw+OrW9gZVdV1VjVbV6MjIyEzGIM2Y13GrJ5O5qiTA9cAjVfXlgfaVA93+BHiwrW8ELkyyPMkpwDrg3tkrWZoLcapE3ZjMVSUfBv4M+HmSza3tL4CLkqxn7IsvtwJ/DlBVDyW5BXiYsStSLvOKEnXBM251YsLgrqofcejP228f55irgKtmUJd0ZB322wGl4eOdkxIA8c5JdcPgll5ncKsPBrd0kLmtThjcEngdt7picEsc/KYSg1t9MLilgzzjVicMbgnw2wHVE4NbAn9IQV0xuKWDDG51wuCWAKdK1BODW2o84VYvDG4J8IxbPZnMtwNK3dq2bRvbtm2buCPF0j17Dnkm88QTT/D43qWTer33vve9HH/88RN3lGbA4NaC9tWvfpUrr7xywn5LloS/vPhfcO6Z696y7+qrr+bW//PYpF7ve9/7Hp/4xCemXKc0FQa3BFBQFPteO4odr/4+Lx94J+9c9gy/vfzJ+a5MeguDW2IstH99YDn3v/hRnt23miKE9/LC/pN4jR/Od3nSG/jhpNRsefl0du9bQ7EECMVS/u+v3sfTr757vkuT3sDglgAK9r22jDdfWVIs4UD5D1MNl8n8WPAxSe5Ncn+Sh5J8qbWfkuSeJFuS3Jzk6Na+vG1vafvXzvEYpBkrYHleamu/sYT9LF/yyrzUJB3OZM64XwXOrqr3A+uBc5KcCfwVcE1VvQfYC1za+l8K7G3t17R+0tBbe+z9/O4xD7E0vwaKo/IrTn3bPZx09FPzXZr0BpP5seACXmqbR7WlgLOBf9XabwS+CFwLnNfWAf4e+K9J0p7nkPbt28fTTz89jfKl8b300ksTd2q+f8+jPPrUf+S5fSt55bW3846le7l32S42b5n8e3Pv3r2+lzUr9u3bd9h9k5q8S7IUuA94D/A3wBPA81W1v3XZDqxq66uAbQBVtT/JC8C7gGcP9/x79uzhG9/4xmRKkabk/vvvn3TfzVuebiH98LRf7+677za4NSv27Nlz2H2TCu6qOgCsT3Ic8F3g1JkWlWQDsAHg5JNP5gtf+MJMn1J6i1deeYU77rjjiL3eJz/5SW/A0ay4+eabD7tvSleVVNXzwN3Ah4DjkhwM/tXAjra+A1gD0Pa/E3jLXx1VdV1VjVbV6MjIyFTKkKRFbTJXlYy0M22SHAt8DHiEsQD/VOt2CXBrW9/Ytmn7fzDe/LYkaWomM1WyErixzXMvAW6pqtuSPAzclOQ/AD8Drm/9rwe+kWQL8Bxw4RzULUmL1mSuKnkAOP0Q7b8AzjhE+6+AP52V6iRJb+Gdk5LUGYNbkjrjlzBoQTv11FM5//zzj9jrrVix4oi9lhYvg1sL2gUXXMAFF1ww32VIs8qpEknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUmcn8WPAxSe5Ncn+Sh5J8qbV/LcmTSTa3ZX1rT5KvJNmS5IEkH5jjMUjSojKZ7+N+FTi7ql5KchTwoyT/q+37QlX9/Zv6fxxY15YPAte2R0nSLJjwjLvGvNQ2j2pLjXPIecDX23E/Bo5LsnLmpUqSYJJz3EmWJtkM7ALurKp72q6r2nTINUmWt7ZVwLaBw7e3NknSLJhUcFfVgapaD6wGzkjyB8AVwKnAPwdOAP7dVF44yYYkm5Js2r1799SqlqRFbEpXlVTV88DdwDlVtbNNh7wKfBU4o3XbAawZOGx1a3vzc11XVaNVNToyMjKt4iVpMZrMVSUjSY5r68cCHwMePThvnSTA+cCD7ZCNwMXt6pIzgReqaucc1C5Ji9JkripZCdyYZCljQX9LVd2W5AdJRoAAm4F/0/rfDpwLbAFeBj4961VL0iI2YXBX1QPA6YdoP/sw/Qu4bOalSZIOxTsnJakzBrckdcbglqTOGNyS1BmDW5I6Y3BLUmcMbknqjMEtSZ0xuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6ozBLUmdMbglqTMGtyR1xuCWpM4Y3JLUGYNbkjpjcEtSZ1JV810DSV4EHpvvOubIicCz813EHFio44KFOzbH1ZffraqRQ+1YdqQrOYzHqmp0vouYC0k2LcSxLdRxwcIdm+NaOJwqkaTOGNyS1JlhCe7r5ruAObRQx7ZQxwULd2yOa4EYig8nJUmTNyxn3JKkSZr34E5yTpLHkmxJcvl81zNVSW5IsivJgwNtJyS5M8nj7fH41p4kX2ljfSDJB+av8vElWZPk7iQPJ3koyWdbe9djS3JMknuT3N/G9aXWfkqSe1r9Nyc5urUvb9tb2v618zqACSRZmuRnSW5r2wtlXFuT/DzJ5iSbWlvX78WZmNfgTrIU+Bvg48BpwEVJTpvPmqbha8A5b2q7HLirqtYBd7VtGBvnurZsAK49QjVOx37g81V1GnAmcFn7b9P72F4Fzq6q9wPrgXOSnAn8FXBNVb0H2Atc2vpfCuxt7de0fsPss8AjA9sLZVwAf1hV6wcu/ev9vTh9VTVvC/Ah4PsD21cAV8xnTdMcx1rgwYHtx4CVbX0lY9epA/x34KJD9Rv2BbgV+NhCGhvwW8BPgQ8ydgPHstb++vsS+D7woba+rPXLfNd+mPGsZizAzgZuA7IQxtVq3Aqc+Ka2BfNenOoy31Mlq4BtA9vbW1vvVlTVzrb+NLCirXc53vbP6NOBe1gAY2vTCZuBXcCdwBPA81W1v3UZrP31cbX9LwDvOqIFT95/Bv4t8FrbfhcLY1wABdyR5L4kG1pb9+/F6RqWOycXrKqqJN1eupPk7cC3gc9V1S+TvL6v17FV1QFgfZLjgO8Cp85vRTOX5BPArqq6L8lZ81zOXPhIVe1IchJwZ5JHB3f2+l6crvk+494BrBnYXt3aevdMkpUA7XFXa+9qvEmOYiy0v1lV32nNC2JsAFX1PHA3Y1MIxyU5eCIzWPvr42r73wnsObKVTsqHgX+ZZCtwE2PTJf+F/scFQFXtaI+7GPvL9gwW0HtxquY7uH8CrGuffB8NXAhsnOeaZsNG4JK2fglj88MH2y9un3qfCbww8E+9oZKxU+vrgUeq6ssDu7oeW5KRdqZNkmMZm7d/hLEA/1Tr9uZxHRzvp4AfVJs4HSZVdUVVra6qtYz9f/SDqvrXdD4ugCRvS/KOg+vAHwEP0vl7cUbme5IdOBf4R8bmGf/9fNczjfq/BewE9jE2l3YpY3OFdwGPA/8AnND6hrGraJ4Afg6Mznf944zrI4zNKz4AbG7Lub2PDfhnwM/auB4E/rK1vxu4F9gC/A9geWs/pm1vafvfPd9jmMQYzwJuWyjjamO4vy0PHcyJ3t+LM1m8c1KSOjPfUyWSpCkyuCWpMwa3JHXG4JakzhjcktQZg1uSOmNwS1JnDG5J6sz/Bxw3xzJ69rlqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第9章-策略梯度算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('RL_Simple')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6925958b004b98bc0512a6d71e5da00858331a32f66ed7f503cad179ff8aa782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

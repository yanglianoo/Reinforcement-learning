{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\gym\\core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=128, out_features=11, bias=True)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=3, out_features=128, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=128, out_features=11, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#计算动作的模型,也是真正要用的模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 11),\n",
    ")\n",
    "\n",
    "#经验网络,用于评估一个状态的分数\n",
    "next_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 11),\n",
    ")\n",
    "\n",
    "#把model的参数复制给next_model\n",
    "next_model.load_state_dict(model.state_dict())\n",
    "\n",
    "model, next_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, -1.2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    #走神经网络,得到一个动作\n",
    "    state = torch.FloatTensor(state).reshape(1, 3)\n",
    "    action = model(state).argmax().item()\n",
    "\n",
    "    if random.random() < 0.01:\n",
    "        action = random.choice(range(11))\n",
    "\n",
    "    #离散动作连续化\n",
    "    action_continuous = action\n",
    "    action_continuous /= 10\n",
    "    action_continuous *= 4\n",
    "    action_continuous -= 2\n",
    "\n",
    "    return action, action_continuous\n",
    "\n",
    "\n",
    "get_action([0.29292667, 0.9561349, 1.0957013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 0), 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action, action_continuous = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step([action_continuous])\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 5000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 5000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Conda\\envs\\RL_Simple\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-9.2810e-01,  3.7233e-01,  1.3928e+00],\n",
       "         [-9.7367e-01,  2.2796e-01,  1.5386e+00],\n",
       "         [-8.8369e-01,  4.6808e-01,  1.3003e-01],\n",
       "         [-9.1114e-01,  4.1210e-01,  9.7173e-01],\n",
       "         [-9.7445e-01,  2.2461e-01,  1.4201e+00],\n",
       "         [-8.2253e-01,  5.6873e-01,  3.0522e-01],\n",
       "         [-9.8746e-01,  1.5786e-01,  1.4138e+00],\n",
       "         [-9.8865e-01, -1.5023e-01,  4.3328e-01],\n",
       "         [-9.9963e-01,  2.7273e-02,  1.2317e+00],\n",
       "         [-9.9167e-01, -1.2878e-01,  7.0987e-01],\n",
       "         [-8.9400e-01,  4.4807e-01, -1.3145e+00],\n",
       "         [-9.7598e-01,  2.1786e-01, -1.2316e+00],\n",
       "         [-8.7086e-01,  4.9152e-01,  6.2119e-01],\n",
       "         [-8.4802e-01,  5.2996e-01, -1.6237e-01],\n",
       "         [-9.8878e-01, -1.4941e-01, -1.5728e-01],\n",
       "         [-9.5939e-01,  2.8209e-01, -1.1231e+00],\n",
       "         [-9.5232e-01,  3.0511e-01, -1.4167e+00],\n",
       "         [-9.9802e-01, -6.2958e-02, -6.8337e-01],\n",
       "         [-8.7831e-01,  4.7809e-01, -1.1300e+00],\n",
       "         [-8.1248e-01,  5.8298e-01, -2.7068e-01],\n",
       "         [-9.9933e-01, -3.6557e-02, -5.3186e-01],\n",
       "         [-9.9734e-01, -7.2840e-02,  4.1002e-01],\n",
       "         [-8.6194e-01,  5.0700e-01, -5.7492e-01],\n",
       "         [-9.9939e-01,  3.4902e-02, -1.1001e+00],\n",
       "         [-9.8278e-01,  1.8478e-01,  1.2449e+00],\n",
       "         [-9.9998e-01, -5.5194e-03,  8.3637e-01],\n",
       "         [-9.8824e-01,  1.5290e-01,  1.5296e+00],\n",
       "         [-8.4150e-01,  5.4025e-01, -9.6411e-01],\n",
       "         [-8.2030e-01,  5.7193e-01, -5.1963e-01],\n",
       "         [-9.2794e-01,  3.7273e-01,  7.3248e-01],\n",
       "         [-9.8757e-01, -1.5718e-01,  1.4061e-01],\n",
       "         [-9.9927e-01, -3.8122e-02,  6.5223e-01],\n",
       "         [-9.7152e-01,  2.3694e-01, -1.4144e+00],\n",
       "         [-9.9296e-01, -1.1842e-01, -1.6180e-01],\n",
       "         [-9.9746e-01, -7.1174e-02,  2.1843e-01],\n",
       "         [-9.9929e-01,  3.7705e-02, -1.1037e+00],\n",
       "         [-8.1295e-01,  5.8234e-01, -2.6014e-01],\n",
       "         [-9.4682e-01,  3.2177e-01, -1.4871e+00],\n",
       "         [-9.0090e-01,  4.3402e-01,  4.6212e-01],\n",
       "         [-9.9572e-01,  9.2386e-02, -1.0061e+00],\n",
       "         [-9.7153e-01,  2.3692e-01,  1.1206e+00],\n",
       "         [-8.5730e-01,  5.1482e-01,  3.5510e-01],\n",
       "         [-9.9965e-01, -2.6329e-02,  1.0722e+00],\n",
       "         [-9.7267e-01,  2.3217e-01, -1.5316e+00],\n",
       "         [-1.0000e+00,  3.9871e-04, -7.3928e-01],\n",
       "         [-9.9270e-01,  1.2064e-01, -9.7432e-01],\n",
       "         [-9.9756e-01,  6.9745e-02,  1.1175e+00],\n",
       "         [-9.4912e-01,  3.1492e-01,  1.1412e+00],\n",
       "         [-9.4674e-01,  3.2200e-01, -1.1925e+00],\n",
       "         [-8.3488e-01,  5.5043e-01, -7.5245e-01],\n",
       "         [-9.0730e-01,  4.2049e-01,  1.1553e+00],\n",
       "         [-9.5522e-01,  2.9588e-01,  1.3815e+00],\n",
       "         [-9.9681e-01,  7.9853e-02,  9.9186e-01],\n",
       "         [-8.8151e-01,  4.7217e-01,  9.8113e-01],\n",
       "         [-9.9505e-01,  9.9395e-02, -1.0642e+00],\n",
       "         [-8.5499e-01,  5.1864e-01, -9.6143e-01],\n",
       "         [-9.0391e-01,  4.2773e-01, -1.2708e+00],\n",
       "         [-8.9556e-01,  4.4493e-01, -5.3948e-01],\n",
       "         [-9.9496e-01,  1.0032e-01, -1.2554e+00],\n",
       "         [-9.9739e-01,  7.2151e-02, -8.4843e-01],\n",
       "         [-8.8279e-01,  4.6977e-01,  9.8564e-01],\n",
       "         [-9.9438e-01,  1.0589e-01, -1.3476e+00],\n",
       "         [-8.8682e-01,  4.6212e-01, -3.8578e-01],\n",
       "         [-9.5229e-01,  3.0518e-01,  1.1918e+00]]),\n",
       " tensor([[2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [7],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [7],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [7],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]),\n",
       " tensor([[-7.8135],\n",
       "         [-8.7157],\n",
       "         [-7.0494],\n",
       "         [-7.4771],\n",
       "         [-8.7006],\n",
       "         [-6.4453],\n",
       "         [-9.1000],\n",
       "         [-8.9650],\n",
       "         [-9.8521],\n",
       "         [-9.1267],\n",
       "         [-7.3405],\n",
       "         [-8.6910],\n",
       "         [-6.9451],\n",
       "         [-6.6754],\n",
       "         [-8.9537],\n",
       "         [-8.2821],\n",
       "         [-8.2198],\n",
       "         [-9.5259],\n",
       "         [-7.1152],\n",
       "         [-6.3543],\n",
       "         [-9.6709],\n",
       "         [-9.4351],\n",
       "         [-6.8460],\n",
       "         [-9.7739],\n",
       "         [-8.8928],\n",
       "         [-9.9063],\n",
       "         [-9.1641],\n",
       "         [-6.7037],\n",
       "         [-6.4432],\n",
       "         [-7.6707],\n",
       "         [-8.9062],\n",
       "         [-9.6755],\n",
       "         [-8.6253],\n",
       "         [-9.1419],\n",
       "         [-9.4333],\n",
       "         [-9.7573],\n",
       "         [-6.3578],\n",
       "         [-8.1411],\n",
       "         [-7.2731],\n",
       "         [-9.3995],\n",
       "         [-8.5509],\n",
       "         [-6.7782],\n",
       "         [-9.8212],\n",
       "         [-8.6883],\n",
       "         [-9.9232],\n",
       "         [-9.2208],\n",
       "         [-9.5622],\n",
       "         [-8.0910],\n",
       "         [-8.0608],\n",
       "         [-6.6051],\n",
       "         [-7.4660],\n",
       "         [-8.2648],\n",
       "         [-9.4735],\n",
       "         [-7.1194],\n",
       "         [-9.3687],\n",
       "         [-6.8348],\n",
       "         [-7.4508],\n",
       "         [-7.2156],\n",
       "         [-9.4074],\n",
       "         [-9.4945],\n",
       "         [-7.1347],\n",
       "         [-9.3973],\n",
       "         [-7.0983],\n",
       "         [-8.1607]]),\n",
       " tensor([[-9.5327e-01,  3.0212e-01,  1.4921e+00],\n",
       "         [-9.8824e-01,  1.5290e-01,  1.5296e+00],\n",
       "         [-8.9063e-01,  4.5472e-01,  3.0108e-01],\n",
       "         [-9.3243e-01,  3.6135e-01,  1.1008e+00],\n",
       "         [-9.8784e-01,  1.5549e-01,  1.4085e+00],\n",
       "         [-8.3790e-01,  5.4582e-01,  5.5176e-01],\n",
       "         [-9.9587e-01,  9.0790e-02,  1.3522e+00],\n",
       "         [-9.8757e-01, -1.5718e-01,  1.4061e-01],\n",
       "         [-9.9965e-01, -2.6329e-02,  1.0722e+00],\n",
       "         [-9.8865e-01, -1.5023e-01,  4.3328e-01],\n",
       "         [-8.6656e-01,  4.9907e-01, -1.1584e+00],\n",
       "         [-9.6049e-01,  2.7831e-01, -1.2482e+00],\n",
       "         [-8.9005e-01,  4.5587e-01,  8.0983e-01],\n",
       "         [-8.5730e-01,  5.1482e-01,  3.5510e-01],\n",
       "         [-9.9188e-01, -1.2716e-01, -4.4933e-01],\n",
       "         [-9.4257e-01,  3.3400e-01, -1.0915e+00],\n",
       "         [-9.2924e-01,  3.6947e-01, -1.3679e+00],\n",
       "         [-9.9985e-01, -1.7470e-02, -9.1059e-01],\n",
       "         [-8.5458e-01,  5.1932e-01, -9.5140e-01],\n",
       "         [-8.2075e-01,  5.7128e-01,  2.8656e-01],\n",
       "         [-1.0000e+00,  3.9871e-04, -7.3928e-01],\n",
       "         [-9.9667e-01, -8.1583e-02,  1.7539e-01],\n",
       "         [-8.5230e-01,  5.2306e-01, -3.7467e-01],\n",
       "         [-9.9524e-01,  9.7449e-02, -1.2539e+00],\n",
       "         [-9.9211e-01,  1.2534e-01,  1.2035e+00],\n",
       "         [-9.9927e-01, -3.8122e-02,  6.5223e-01],\n",
       "         [-9.9678e-01,  8.0199e-02,  1.4643e+00],\n",
       "         [-8.2097e-01,  5.7097e-01, -7.3892e-01],\n",
       "         [-8.1248e-01,  5.8298e-01, -2.7068e-01],\n",
       "         [-9.4264e-01,  3.3381e-01,  8.3202e-01],\n",
       "         [-9.8878e-01, -1.4941e-01, -1.5728e-01],\n",
       "         [-9.9818e-01, -6.0276e-02,  4.4364e-01],\n",
       "         [-9.5232e-01,  3.0511e-01, -1.4167e+00],\n",
       "         [-9.9528e-01, -9.7016e-02, -4.3061e-01],\n",
       "         [-9.9752e-01, -7.0429e-02, -1.4950e-02],\n",
       "         [-9.9496e-01,  1.0032e-01, -1.2554e+00],\n",
       "         [-8.2150e-01,  5.7022e-01,  2.9661e-01],\n",
       "         [-9.2149e-01,  3.8839e-01, -1.4258e+00],\n",
       "         [-9.1367e-01,  4.0645e-01,  6.0764e-01],\n",
       "         [-9.8902e-01,  1.4781e-01, -1.1168e+00],\n",
       "         [-9.8325e-01,  1.8226e-01,  1.1183e+00],\n",
       "         [-8.7140e-01,  4.9057e-01,  5.6122e-01],\n",
       "         [-9.9755e-01, -6.9896e-02,  8.7243e-01],\n",
       "         [-9.5197e-01,  3.0619e-01, -1.5375e+00],\n",
       "         [-9.9893e-01,  4.6331e-02, -9.1898e-01],\n",
       "         [-9.8488e-01,  1.7324e-01, -1.0638e+00],\n",
       "         [-9.9979e-01,  2.0311e-02,  9.8978e-01],\n",
       "         [-9.6626e-01,  2.5757e-01,  1.1973e+00],\n",
       "         [-9.2703e-01,  3.7500e-01, -1.1310e+00],\n",
       "         [-8.2030e-01,  5.7193e-01, -5.1963e-01],\n",
       "         [-9.3252e-01,  3.6111e-01,  1.2906e+00],\n",
       "         [-9.7385e-01,  2.2721e-01,  1.4234e+00],\n",
       "         [-9.9934e-01,  3.6343e-02,  8.7175e-01],\n",
       "         [-9.0730e-01,  4.2049e-01,  1.1553e+00],\n",
       "         [-9.8754e-01,  1.5739e-01, -1.1697e+00],\n",
       "         [-8.3488e-01,  5.5043e-01, -7.5245e-01],\n",
       "         [-8.7831e-01,  4.7809e-01, -1.1300e+00],\n",
       "         [-8.8682e-01,  4.6212e-01, -3.8578e-01],\n",
       "         [-9.8584e-01,  1.6770e-01, -1.3602e+00],\n",
       "         [-9.9270e-01,  1.2064e-01, -9.7432e-01],\n",
       "         [-9.0849e-01,  4.1790e-01,  1.1580e+00],\n",
       "         [-9.8411e-01,  1.7755e-01, -1.4482e+00],\n",
       "         [-8.8170e-01,  4.7181e-01, -2.1919e-01],\n",
       "         [-9.6938e-01,  2.4555e-01,  1.2407e+00]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 3]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state, action, reward, next_state, over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4603],\n",
       "        [0.5238],\n",
       "        [0.2989],\n",
       "        [0.3856],\n",
       "        [0.5033],\n",
       "        [0.2949],\n",
       "        [0.5160],\n",
       "        [0.4239],\n",
       "        [0.5154],\n",
       "        [0.4601],\n",
       "        [0.3963],\n",
       "        [0.4230],\n",
       "        [0.3335],\n",
       "        [0.2760],\n",
       "        [0.4035],\n",
       "        [0.3995],\n",
       "        [0.4371],\n",
       "        [0.4133],\n",
       "        [0.3611],\n",
       "        [0.2707],\n",
       "        [0.3968],\n",
       "        [0.4164],\n",
       "        [0.2817],\n",
       "        [0.4352],\n",
       "        [0.4833],\n",
       "        [0.4645],\n",
       "        [0.5387],\n",
       "        [0.3281],\n",
       "        [0.2638],\n",
       "        [0.3740],\n",
       "        [0.3977],\n",
       "        [0.4415],\n",
       "        [0.4474],\n",
       "        [0.3998],\n",
       "        [0.3943],\n",
       "        [0.4352],\n",
       "        [0.2716],\n",
       "        [0.4480],\n",
       "        [0.3353],\n",
       "        [0.4175],\n",
       "        [0.4527],\n",
       "        [0.3095],\n",
       "        [0.5029],\n",
       "        [0.4672],\n",
       "        [0.4069],\n",
       "        [0.4105],\n",
       "        [0.4913],\n",
       "        [0.4335],\n",
       "        [0.4008],\n",
       "        [0.2947],\n",
       "        [0.4032],\n",
       "        [0.4790],\n",
       "        [0.4730],\n",
       "        [0.3693],\n",
       "        [0.4219],\n",
       "        [0.3328],\n",
       "        [0.3922],\n",
       "        [0.2915],\n",
       "        [0.4435],\n",
       "        [0.4063],\n",
       "        [0.3704],\n",
       "        [0.4524],\n",
       "        [0.2761],\n",
       "        [0.4430]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(state, action):\n",
    "    #使用状态计算出动作的logits\n",
    "    #[b, 3] -> [b, 11]\n",
    "    value = model(state)\n",
    "\n",
    "    #根据实际使用的action取出每一个值\n",
    "    #这个值就是模型评估的在该状态下,执行动作的分数\n",
    "    #在执行动作前,显然并不知道会得到的反馈和next_state\n",
    "    #所以这里不能也不需要考虑next_state和reward\n",
    "    #[b, 11] -> [b, 1]\n",
    "    value = value.gather(dim=1, index=action)\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "get_value(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.3268],\n",
       "        [-8.1878],\n",
       "        [-6.7351],\n",
       "        [-7.0708],\n",
       "        [-8.1953],\n",
       "        [-6.1375],\n",
       "        [-8.5912],\n",
       "        [-8.5753],\n",
       "        [-9.3593],\n",
       "        [-8.7113],\n",
       "        [-6.9867],\n",
       "        [-8.2850],\n",
       "        [-6.5953],\n",
       "        [-6.3720],\n",
       "        [-8.5525],\n",
       "        [-7.9032],\n",
       "        [-7.8112],\n",
       "        [-9.1120],\n",
       "        [-6.7907],\n",
       "        [-6.0661],\n",
       "        [-9.2721],\n",
       "        [-9.0505],\n",
       "        [-6.5869],\n",
       "        [-9.3391],\n",
       "        [-8.4116],\n",
       "        [-9.4737],\n",
       "        [-8.6338],\n",
       "        [-6.4209],\n",
       "        [-6.1779],\n",
       "        [-7.2858],\n",
       "        [-8.5108],\n",
       "        [-9.2646],\n",
       "        [-8.1969],\n",
       "        [-8.7477],\n",
       "        [-9.0496],\n",
       "        [-9.3227],\n",
       "        [-6.0696],\n",
       "        [-7.7240],\n",
       "        [-6.9267],\n",
       "        [-8.9872],\n",
       "        [-8.0937],\n",
       "        [-6.4569],\n",
       "        [-9.3521],\n",
       "        [-8.2383],\n",
       "        [-9.5157],\n",
       "        [-8.8187],\n",
       "        [-9.0883],\n",
       "        [-7.6430],\n",
       "        [-7.6852],\n",
       "        [-6.3466],\n",
       "        [-7.0301],\n",
       "        [-7.7716],\n",
       "        [-9.0186],\n",
       "        [-6.7242],\n",
       "        [-8.9519],\n",
       "        [-6.5461],\n",
       "        [-7.0969],\n",
       "        [-6.9450],\n",
       "        [-8.9690],\n",
       "        [-9.0922],\n",
       "        [-6.7384],\n",
       "        [-8.9470],\n",
       "        [-6.8270],\n",
       "        [-7.7034]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #上面已经把模型认为的状态下执行动作的分数给评估出来了\n",
    "    #下面使用next_state和reward计算真实的分数\n",
    "    #针对一个状态,它到底应该多少分,可以使用以往模型积累的经验评估\n",
    "    #这也是没办法的办法,因为显然没有精确解,这里使用延迟更新的next_model评估\n",
    "\n",
    "    #使用next_state计算下一个状态的分数\n",
    "    #[b, 3] -> [b, 11]\n",
    "    with torch.no_grad():\n",
    "        target = next_model(next_state)\n",
    "    \"\"\"以下是主要的Double DQN和DQN的区别\"\"\"\n",
    "    #取所有动作中分数最大的\n",
    "    #[b, 11] -> [b]\n",
    "    #target = target.max(dim=1)[0]\n",
    "\n",
    "    #使用model计算下一个状态的分数\n",
    "    #[b, 3] -> [b, 11]\n",
    "    with torch.no_grad():\n",
    "        model_target = model(next_state)\n",
    "\n",
    "    #取分数最高的下标\n",
    "    #[b, 11] -> [b, 1]\n",
    "    model_target = model_target.max(dim=1)[1]\n",
    "    model_target = model_target.reshape(-1, 1)\n",
    "\n",
    "    #以这个下标取next_value当中的值\n",
    "    #[b, 11] -> [b]\n",
    "    target = target.gather(dim=1, index=model_target)\n",
    "    \"\"\"以上是主要的Double DQN和DQN的区别\"\"\"\n",
    "\n",
    "    #下一个状态的分数乘以一个系数,相当于权重\n",
    "    target *= 0.98\n",
    "\n",
    "    #如果next_state已经游戏结束,则next_state的分数是0\n",
    "    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.\n",
    "    #[b, 1] * [b, 1] -> [b, 1]\n",
    "    target *= (1 - over)\n",
    "\n",
    "    #加上reward就是最终的分数\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1330.8543591850073"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        _, action_continuous = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step([action_continuous])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 200 0 -1490.3204088293232\n",
      "20 4400 200 0 -1054.4662874974385\n",
      "40 5000 200 200 -987.6839263889742\n",
      "60 5000 200 200 -649.1422643423309\n",
      "80 5000 200 200 -673.882279516404\n",
      "100 5000 200 200 -166.931364361927\n",
      "120 5000 200 200 -321.4064149694941\n",
      "140 5000 200 200 -402.1436823088549\n",
      "160 5000 200 200 -352.85392089713656\n",
      "180 5000 200 200 -306.27528921167635\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(200):\n",
    "        #更新N条数据\n",
    "        update_count, drop_count = update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算一批样本的value和target\n",
    "            value = get_value(state, action)\n",
    "            target = get_target(reward, next_state, over)\n",
    "\n",
    "            #更新参数\n",
    "            loss = loss_fn(value, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #把model的参数复制给next_model\n",
    "            if (i + 1) % 50 == 0:\n",
    "                next_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(20)]) / 20\n",
    "            print(epoch, len(datas), update_count, drop_count, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-247.3974781797497"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('RL_Simple')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6925958b004b98bc0512a6d71e5da00858331a32f66ed7f503cad179ff8aa782"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUS0lEQVR4nO3db6xc9Z3f8ffnXoxNgN3gcqGObYo3ddRCUszqliaiqmiSBi+t1kRqKkdpxAMkB4lUibpKFnal3eSBpW2zSdoHTSLSoFjZbIilBGFFye6yNCiNkuAYwj9j/jhgwcUONmYRJICx73z74B7KxJ7rO75/PPfceb+kYc58zzlzvj8098PhN2dmUlVIktpjZNANSJJOjcEtSS1jcEtSyxjcktQyBrcktYzBLUkts2DBnWRjkseS7E1y00IdR5KGTRbiOu4ko8DjwL8DJoCfAx+uqkfm/WCSNGQW6oz7CmBvVT1ZVa8DtwGbFuhYkjRUzlig510NPNP1eAL4V9NtfP7559fFF1+8QK1IUvvs27eP559/Pr3WLVRw9zrYb83JJNkCbAG46KKL2LVr1wK1IkntMz4+Pu26hZoqmQDWdj1eA+zv3qCqbqmq8aoaHxsbW6A2JGnpWajg/jmwPsm6JGcCm4EdC3QsSRoqCzJVUlXHknwc+FtgFLi1qnYvxLEkadgs1Bw3VfV94PsL9fySNKz85KQktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLTOnny5Lsg94GZgEjlXVeJKVwLeBi4F9wH+qqn+YW5uSpDfMxxn3v62qDVU13jy+CbirqtYDdzWPJUnzZCGmSjYB25rlbcC1C3AMSRpacw3uAv4uyb1JtjS1C6vqAEBzf8EcjyFJ6jKnOW7gyqran+QC4M4kj/a7YxP0WwAuuuiiObYhScNjTmfcVbW/uT8I3A5cATyXZBVAc39wmn1vqarxqhofGxubSxuSNFRmHdxJzk5y7hvLwAeAh4EdwHXNZtcBd8y1SUnSm+YyVXIhcHuSN57nr6vqb5L8HNie5HrgaeBDc29TkvSGWQd3VT0JXNajfhh431yakiRNz09OSlLLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktcyMwZ3k1iQHkzzcVVuZ5M4kTzT353WtuznJ3iSPJbl6oRqXpGHVzxn314GNx9VuAu6qqvXAXc1jklwCbAYubfb5UpLReetWkjRzcFfVj4AXjitvArY1y9uAa7vqt1XVkap6CtgLXDE/rUqSYPZz3BdW1QGA5v6Cpr4aeKZru4mmdoIkW5LsSrLr0KFDs2xDkobPfL85mR616rVhVd1SVeNVNT42NjbPbUjS0jXb4H4uySqA5v5gU58A1nZttwbYP/v2JEnHm21w7wCua5avA+7oqm9OsjzJOmA9sHNuLUqSup0x0wZJvgVcBZyfZAL4c+AvgO1JrgeeBj4EUFW7k2wHHgGOATdW1eQC9S5JQ2nG4K6qD0+z6n3TbL8V2DqXpiRJ0/OTk5LUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1zIzBneTWJAeTPNxV+0ySZ5Pc39yu6Vp3c5K9SR5LcvVCNS5Jw6qfM+6vAxt71L9YVRua2/cBklwCbAYubfb5UpLR+WpWktRHcFfVj4AX+ny+TcBtVXWkqp4C9gJXzKE/SdJx5jLH/fEkDzZTKec1tdXAM13bTDS1EyTZkmRXkl2HDh2aQxuSNFxmG9xfBt4ObAAOAJ9v6umxbfV6gqq6parGq2p8bGxslm1I0vCZVXBX1XNVNVlVHeCrvDkdMgGs7dp0DbB/bi1KkrrNKriTrOp6+EHgjStOdgCbkyxPsg5YD+ycW4uSpG5nzLRBkm8BVwHnJ5kA/hy4KskGpqZB9gEfA6iq3Um2A48Ax4Abq2pyQTqXpCE1Y3BX1Yd7lL92ku23Alvn0pQkaXp+clKSWsbglqSWMbglqWUMbklqGYNbklpmxqtKpKWqqvjNwafoHHv9hHVnX7CO0WXLB9CVNDODW8Oriqfu/jqvvfir364nXPof/4y3rOz5NTvSwDlVIvVQk8cG3YI0LYNbOl5B59jRQXchTcvglnroTBrcWrwMbqmHMri1iBncUg+ecWsxM7ilHpzj1mJmcEsnKK8q0aJmcGt4BVa89R/3XPXqPzx7mpuR+mdwa4iFFb97Qc81R14+fJp7kfpncGuoZXTZoFuQTpnBraE2YnCrhWYM7iRrk/wwyZ4ku5N8oqmvTHJnkiea+/O69rk5yd4kjyW5eiEHIM3FyBkGt9qnnzPuY8AfVdU/B94N3JjkEuAm4K6qWg/c1TymWbcZuBTYCHwpyehCNC/NlVMlaqMZg7uqDlTVfc3yy8AeYDWwCdjWbLYNuLZZ3gTcVlVHquopYC9wxTz3Lc0Lp0rURqc0x53kYuBy4B7gwqo6AFPhDrzx9vxq4Jmu3Saa2vHPtSXJriS7Dh06NIvWpbkbGfWbjdU+fQd3knOA7wCfrKqXTrZpj1qdUKi6parGq2p8bGys3zakeXWyOe6qE1620qLQV3AnWcZUaH+zqr7blJ9LsqpZvwo42NQngLVdu68B9s9Pu9L8SQLTvP1S1TnN3Uj96+eqkgBfA/ZU1Re6Vu0ArmuWrwPu6KpvTrI8yTpgPbBz/lqWToNOx/DWotXPBN+VwEeBh5Lc39T+BPgLYHuS64GngQ8BVNXuJNuBR5i6IuXGqpqc78alhVSdSeh0YMQLorT4zBjcVfVjes9bA7xvmn22Alvn0Jc0UNWZZOp8w6tOtPj4yUmph+pMUh2nSrQ4GdxSD9WZBOe4tUgZ3FIPU1MlBrcWJ4Nb6qGq41SJFi2DW0PtzLN/lzNWnHNC/fWXD3Ps1ZcH0JE0M4NbQ210+dmMLFtxQn3y6GtMHnt9AB1JMzO4NdSSURL/DNQuvmI11DIyQkb8M1C7+IrVUMvIKHjGrZbxFauhlpFRz7jVOr5iNdQyMuIct1rHV6yGWkZ8c1Lt4ytWQy0ZBadK1DK+YjXcEjLdl1/6kXctUga3htrUr+D0XteZPHp6m5H6ZHBL0+gcM7i1OBnc0jTKM24tUga3NA2nSrRY9fNjwWuT/DDJniS7k3yiqX8mybNJ7m9u13Ttc3OSvUkeS3L1Qg5AWihOlWix6ufHgo8Bf1RV9yU5F7g3yZ3Nui9W1V92b5zkEmAzcCnwNuDvk7zDHwxW29TksUG3IPU04xl3VR2oqvua5ZeBPcDqk+yyCbitqo5U1VPAXuCK+WhWOp2cKtFidUpz3EkuBi4H7mlKH0/yYJJbk5zX1FYDz3TtNsHJg14aqNEz39KzfvTVl05zJ1J/+g7uJOcA3wE+WVUvAV8G3g5sAA4An39j0x67V4/n25JkV5Jdhw4dOtW+pXlz7qr1Peu/Ofjkae5E6k9fwZ1kGVOh/c2q+i5AVT1XVZM19YuqX+XN6ZAJYG3X7muA/cc/Z1XdUlXjVTU+NjY2lzFIc5LRZYNuQTol/VxVEuBrwJ6q+kJXfVXXZh8EHm6WdwCbkyxPsg5YD+ycv5al+TUy2s979NLi0c8r9krgo8BDSe5van8CfDjJBqamQfYBHwOoqt1JtgOPMHVFyo1eUaLFbMQzbrXMjMFdVT+m97z190+yz1Zg6xz6kk4bp0rUNn5yUkNv5AyDW+1icGvoOVWitjG4NfRONlVSdcKVrNLAGdwaetP+WHD9/39Ii4rBLU3zSwpVHarjr+Bo8TG4pWlUp0N1vJJVi4/BLU2jatLg1qJkcEvT6ThVosXJ4JamUdXBD/1qMTK4pWlMvTlpcGvxMbilafjmpBYrg1tDb+SMZYwsW35CvXP0CJOvvzqAjqST8/sstWQ9+eST/OpXv5p5w6OvMDpyFiMc+a3ysdde5qFdP6XzOxN9He9d73oX55577mxalU6Jwa0l63Of+xxf+cpXZtzuvHNX8D//yx/wzy46/4R1n/7jT/N/H3y6r+P95Cc/4T3vec8p9ymdKoNbQ2+yU0xOdjjSWcGzr72D1zpns3LZAS44s7/Alk43g1tDr9MpfnNsBfe99AFePHYBEJ5+7RLe/pZfUPztoNuTTuCbkxp6k50Ou1/6l7x47EKm/iRCMcovX7mcw6+/bdDtSScwuDX0JjvFa8fO4PgvmypG6TA6mKakk+jnx4JXJNmZ5IEku5N8tqmvTHJnkiea+/O69rk5yd4kjyW5eiEHIM1VZ7LDmbzE8V/hOprXWZYjvXeSBqifM+4jwHur6jJgA7AxybuBm4C7qmo9cFfzmCSXAJuBS4GNwJeSeNqiRWuyit9bcQ9vW76XUY4CxZl5lUvP/jHnLevjckLpNOvnx4IL+HXzcFlzK2ATcFVT3wbcDfxxU7+tqo4ATyXZC1wB/HS6Yxw9erS/622lU/DKK6/0tV0VfPfu+7nwoX0cPrqa1ztn8TtnPM89o4d5YuKFvo/3wgsv+DrWvDl69Oi06/q6qqQ5Y74X+KfA/6qqe5JcWFUHAKrqQJILms1XAz/r2n2iqU3r8OHDfOMb3+inFalvjz/+eN/b3rPnWeBZYPesj/eDH/yARx55ZNb7S90OHz487bq+grumviJtQ5K3ArcneedJNu/1cyIn/P5Tki3AFoCLLrqIT33qU/20IvXtySef5Gc/+9nMG86Tj3zkI34AR/Pm29/+9rTrTumqkqp6kakpkY3Ac0lWATT3B5vNJoC1XbutAfb3eK5bqmq8qsbHxsZOpQ1JGmr9XFUy1pxpk+Qs4P3Ao8AO4Lpms+uAO5rlHcDmJMuTrAPWAzvnuW9JGlr9TJWsArY189wjwPaq+l6SnwLbk1wPPA18CKCqdifZDjwCHANuLL+NXpLmTT9XlTwIXN6jfhh43zT7bAW2zrk7SdIJ/OSkJLWMwS1JLeO3A2rJuuyyy7j22mtP2/FWrlx52o6l4WZwa8m64YYbuOGGGwbdhjTvnCqRpJYxuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqGYNbklrG4JakljG4JallDG5JahmDW5JaxuCWpJYxuCWpZfr5seAVSXYmeSDJ7iSfbeqfSfJskvub2zVd+9ycZG+Sx5JcvZADkKRh08/3cR8B3ltVv06yDPhxkh80675YVX/ZvXGSS4DNwKXA24C/T/IOfzBYkubHjGfcNeXXzcNlza1Osssm4LaqOlJVTwF7gSvm3KkkCehzjjvJaJL7gYPAnVV1T7Pq40keTHJrkvOa2mrgma7dJ5qaJGke9BXcVTVZVRuANcAVSd4JfBl4O7ABOAB8vtk8vZ7i+EKSLUl2Jdl16NChWbQuScPplK4qqaoXgbuBjVX1XBPoHeCrvDkdMgGs7dptDbC/x3PdUlXjVTU+NjY2m94laSj1c1XJWJK3NstnAe8HHk2yqmuzDwIPN8s7gM1JlidZB6wHds5r15I0xPq5qmQVsC3JKFNBv72qvpfkG0k2MDUNsg/4GEBV7U6yHXgEOAbc6BUlkjR/ZgzuqnoQuLxH/aMn2WcrsHVurUmSevGTk5LUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktYzBLUktY3BLUssY3JLUMga3JLWMwS1JLWNwS1LLGNyS1DIGtyS1jMEtSS1jcEtSyxjcktQyBrcktUyqatA9kOQQ8Bvg+UH3sgDOx3G1zVIdm+Nql39SVWO9ViyK4AZIsquqxgfdx3xzXO2zVMfmuJYOp0okqWUMbklqmcUU3LcMuoEF4rjaZ6mOzXEtEYtmjluS1J/FdMYtSerDwIM7ycYkjyXZm+SmQfdzqpLcmuRgkoe7aiuT3Jnkieb+vK51NzdjfSzJ1YPpemZJ1ib5YZI9SXYn+URTb/XYkqxIsjPJA824PtvUWz2uNyQZTfKLJN9rHi+Vce1L8lCS+5PsampLYmyzUlUDuwGjwC+B3wPOBB4ALhlkT7MYw78Bfh94uKv234GbmuWbgP/WLF/SjHE5sK4Z++igxzDNuFYBv98snws83vTf6rEBAc5plpcB9wDvbvu4usb3X4G/Br63VF6LTb/7gPOPqy2Jsc3mNugz7iuAvVX1ZFW9DtwGbBpwT6ekqn4EvHBceROwrVneBlzbVb+tqo5U1VPAXqb+HSw6VXWgqu5rll8G9gCrafnYasqvm4fLmlvR8nEBJFkD/Hvgf3eVWz+uk1jKYzupQQf3auCZrscTTa3tLqyqAzAVgMAFTb2V401yMXA5U2enrR9bM51wP3AQuLOqlsS4gP8BfBrodNWWwrhg6j+uf5fk3iRbmtpSGdspO2PAx0+P2lK+zKV1401yDvAd4JNV9VLSawhTm/aoLcqxVdUksCHJW4Hbk7zzJJu3YlxJ/gNwsKruTXJVP7v0qC26cXW5sqr2J7kAuDPJoyfZtm1jO2WDPuOeANZ2PV4D7B9QL/PpuSSrAJr7g029VeNNsoyp0P5mVX23KS+JsQFU1YvA3cBG2j+uK4E/TLKPqSnH9yb5K9o/LgCqan9zfxC4nampjyUxttkYdHD/HFifZF2SM4HNwI4B9zQfdgDXNcvXAXd01TcnWZ5kHbAe2DmA/maUqVPrrwF7quoLXataPbYkY82ZNknOAt4PPErLx1VVN1fVmqq6mKm/o/9TVf+Zlo8LIMnZSc59Yxn4APAwS2Bsszbod0eBa5i6YuGXwJ8Oup9Z9P8t4ABwlKn/0l8P/CPgLuCJ5n5l1/Z/2oz1MeAPBt3/Scb1r5n638sHgfub2zVtHxvwL4BfNON6GPizpt7qcR03xqt486qS1o+LqavOHmhuu9/IiaUwttne/OSkJLXMoKdKJEmnyOCWpJYxuCWpZQxuSWoZg1uSWsbglqSWMbglqWUMbklqmf8Hu6IwEI8XSTgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个游戏的状态用4个数字表示,我也不知道这4个数字分别是什么意思,反正这4个数字就能描述游戏全部的状态\n",
      "state= [ 0.03946265 -0.01378644 -0.03145449 -0.01241964]\n",
      "这个游戏一共有2个动作,不是0就是1\n",
      "env.action_space= Discrete(2)\n",
      "随机一个动作\n",
      "action= 1\n",
      "执行一个动作,得到下一个状态,奖励,是否结束\n",
      "state= [ 0.03918692  0.18177216 -0.03170288 -0.31485853]\n",
      "reward= 1.0\n",
      "over= False\n"
     ]
    }
   ],
   "source": [
    "#测试游戏环境\n",
    "def test_env():\n",
    "    state = env.reset()\n",
    "    print('这个游戏的状态用4个数字表示,我也不知道这4个数字分别是什么意思,反正这4个数字就能描述游戏全部的状态')\n",
    "    print('state=', state)\n",
    "    #state= [ 0.03490619  0.04873464  0.04908862 -0.00375859]\n",
    "\n",
    "    print('这个游戏一共有2个动作,不是0就是1')\n",
    "    print('env.action_space=', env.action_space)\n",
    "    #env.action_space= Discrete(2)\n",
    "\n",
    "    print('随机一个动作')\n",
    "    action = env.action_space.sample()\n",
    "    print('action=', action)\n",
    "    #action= 1\n",
    "\n",
    "    print('执行一个动作,得到下一个状态,奖励,是否结束')\n",
    "    state, reward, over, _ = env.step(action)\n",
    "\n",
    "    print('state=', state)\n",
    "    #state= [ 0.02018229 -0.16441101  0.01547085  0.2661691 ]\n",
    "\n",
    "    print('reward=', reward)\n",
    "    #reward= 1.0\n",
    "\n",
    "    print('over=', over)\n",
    "    #over= False\n",
    "\n",
    "\n",
    "test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#计算动作的模型,也是真正要用的模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "#得到一个动作\n",
    "def get_action(state):\n",
    "    if random.random() < 0.01:\n",
    "        return random.choice([0, 1])\n",
    "\n",
    "    #走神经网络,得到一个动作\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "\n",
    "    return model(state).argmax().item()\n",
    "\n",
    "\n",
    "get_action([0.0013847, -0.01194451, 0.04260966, 0.00688801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 0), 208)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1403, -1.5900,  0.1024,  2.2623],\n",
       "         [-0.0193,  0.0074,  0.0325,  0.0051],\n",
       "         [-0.0843, -1.4061,  0.1112,  2.0697],\n",
       "         [ 0.0169, -0.5740, -0.0193,  0.8088],\n",
       "         [-0.0628, -0.9541,  0.0700,  1.5112]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " tensor([[-0.1721, -1.7859,  0.1477,  2.5847],\n",
       "         [-0.0192, -0.1882,  0.0326,  0.3079],\n",
       "         [-0.1124, -1.6021,  0.1526,  2.3946],\n",
       "         [ 0.0054, -0.7689, -0.0031,  1.0953],\n",
       "         [-0.0819, -1.1500,  0.1002,  1.8249]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples])\n",
    "    #[b]\n",
    "    action = torch.LongTensor([i[1] for i in samples])\n",
    "    #[b]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples])\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples])\n",
    "    #[b]\n",
    "    over = torch.LongTensor([i[4] for i in samples])\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action, reward, next_state[:5], over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1866, -0.0030, -0.1721, -0.0867, -0.1318, -0.1368, -0.1860, -0.0521,\n",
       "        -0.0928, -0.2029, -0.1845, -0.1608, -0.0451, -0.1302, -0.1117, -0.1741,\n",
       "        -0.1621, -0.0115, -0.2227, -0.0518, -0.1125, -0.1986, -0.1635, -0.1590,\n",
       "        -0.1749, -0.1699, -0.1413, -0.1478, -0.0684, -0.2062, -0.1351, -0.2119,\n",
       "        -0.1064, -0.0067, -0.0450, -0.1806, -0.0099, -0.1108, -0.0029, -0.1937,\n",
       "        -0.2198, -0.2203,  0.0058, -0.0088, -0.1348, -0.0481, -0.1831, -0.1888,\n",
       "        -0.1329, -0.0877, -0.0828, -0.1478, -0.1987, -0.0943, -0.2009, -0.0639,\n",
       "        -0.0113, -0.1290, -0.1587, -0.0151, -0.0658, -0.1523, -0.2234, -0.1966],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(state, action):\n",
    "    #使用状态计算出动作的logits\n",
    "    #[b, 4] -> [b, 2]\n",
    "    value = model(state)\n",
    "\n",
    "    #根据实际使用的action取出每一个值\n",
    "    #这个值就是模型评估的在该状态下,执行动作的分数\n",
    "    #在执行动作前,显然并不知道会得到的反馈和next_state\n",
    "    #所以这里不能也不需要考虑next_state和reward\n",
    "    #[b, 2] -> [b]\n",
    "    value = value[range(64), action]\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "get_value(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7923, 0.9600, 0.8054, 0.8917, 0.8459, 0.8388, 0.7920, 0.9290, 0.8881,\n",
       "        0.7770, 0.7942, 0.8171, 0.9330, 0.8552, 0.8679, 0.8031, 1.0057, 0.9492,\n",
       "        1.0000, 0.9301, 0.8697, 0.7811, 0.8167, 0.8206, 0.8027, 0.8076, 0.8424,\n",
       "        0.8327, 0.9123, 1.0000, 0.8444, 0.7698, 0.8759, 0.9516, 0.9334, 0.7980,\n",
       "        0.9535, 0.8728, 0.9599, 0.7846, 1.0000, 1.0000, 0.9595, 0.9566, 0.8398,\n",
       "        0.9288, 0.7957, 1.0000, 0.8508, 0.8891, 0.8957, 0.8312, 0.7817, 0.8856,\n",
       "        1.0000, 0.9188, 0.9569, 0.8520, 0.8210, 0.9534, 0.9158, 0.8269, 1.0000,\n",
       "        0.7832])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #上面已经把模型认为的状态下执行动作的分数给评估出来了\n",
    "    #下面使用next_state和reward计算真实的分数\n",
    "    #针对一个状态,它到底应该多少分,可以使用以往模型积累的经验评估\n",
    "    #这也是没办法的办法,因为显然没有精确解,这里使用延迟更新的next_model评估\n",
    "\n",
    "    #使用next_state计算下一个状态的分数\n",
    "    #[b, 4] -> [b, 2]\n",
    "    with torch.no_grad():\n",
    "        target = model(next_state)\n",
    "\n",
    "    #取所有动作中分数最大的\n",
    "    #[b, 2] -> [b]\n",
    "    target = target.max(dim=1)[0]\n",
    "\n",
    "    #如果next_state已经游戏结束,则next_state的分数是0\n",
    "    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.\n",
    "    #[b]\n",
    "    for i in range(64):\n",
    "        if over[i]:\n",
    "            target[i] = 0\n",
    "\n",
    "    #下一个状态的分数乘以一个系数,相当于权重\n",
    "    #[b] * [b] -> [b]\n",
    "    target *= 0.98\n",
    "\n",
    "    #加上reward就是最终的分数\n",
    "    #[b] + [b] -> [b]\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play:\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 415 207 0 12.35\n",
      "50 10000 241 241 113.55\n",
      "100 10000 200 200 164.7\n",
      "150 10000 200 200 182.95\n",
      "200 10000 340 340 193.9\n",
      "250 10000 200 200 165.05\n",
      "300 10000 282 282 170.7\n",
      "350 10000 200 200 178.4\n",
      "400 10000 200 200 186.65\n",
      "450 10000 200 200 198.35\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(500):\n",
    "        #更新N条数据\n",
    "        update_count, drop_count = update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算一批样本的value和target\n",
    "            value = get_value(state, action)\n",
    "            target = get_target(reward, next_state, over)\n",
    "\n",
    "            #更新参数\n",
    "            loss = loss_fn(value, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(20)]) / 20\n",
    "            print(epoch, len(datas), update_count, drop_count, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

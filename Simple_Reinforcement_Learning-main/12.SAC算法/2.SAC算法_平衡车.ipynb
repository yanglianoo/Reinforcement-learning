{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4225, 0.5775],\n",
       "        [0.4978, 0.5022]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_action = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5229,  0.0097],\n",
       "        [-0.3939, -0.1454]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value_next1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value_next2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value_next1.load_state_dict(model_value1.state_dict())\n",
    "model_value_next2.load_state_dict(model_value2.state_dict())\n",
    "\n",
    "model_value1(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    prob = model_action(state)\n",
    "\n",
    "    #根据概率选择一个动作\n",
    "    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204, 0), 204)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.3075e-02,  1.9651e-01,  1.8218e-01,  2.4880e-01],\n",
       "         [-2.5490e-02, -1.7621e-01, -2.6257e-04,  3.2060e-01],\n",
       "         [-1.2081e-02, -5.6254e-01,  4.3379e-02,  9.4328e-01],\n",
       "         [-6.9828e-02, -5.4780e-01,  4.4761e-02,  8.3885e-01],\n",
       "         [-1.5093e-03, -1.5274e-01, -4.8801e-05,  2.9281e-01]]),\n",
       " tensor([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[-0.0591,  0.3886,  0.1872,  0.0187],\n",
       "         [-0.0290,  0.0189,  0.0061,  0.0278],\n",
       "         [-0.0233, -0.3680,  0.0622,  0.6645],\n",
       "         [-0.0808, -0.3533,  0.0615,  0.5606],\n",
       "         [-0.0046, -0.3479,  0.0058,  0.5855]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052, requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#这也是一个可学习的参数\n",
    "alpha = torch.tensor(math.log(0.01))\n",
    "alpha.requires_grad = True\n",
    "\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #计算动作的概率\n",
    "    #[b, 4] -> [b, 2]\n",
    "    prob = model_action(next_state)\n",
    "\n",
    "    #计算动作的熵\n",
    "    #[b, 2]\n",
    "    entropy = prob * torch.log(prob + 1e-8)\n",
    "\n",
    "    #所有动作的熵求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4] -> [b, 2]\n",
    "    target1 = model_value_next1(next_state)\n",
    "    target2 = model_value_next2(next_state)\n",
    "\n",
    "    #取价值小的,这是出于稳定性考虑\n",
    "    #[b, 2]\n",
    "    target = torch.min(target1, target2)\n",
    "\n",
    "    #求target期望\n",
    "    #[b, 2] * [b, 2] -> [b, 2]\n",
    "    target = (prob * target)\n",
    "    #[b, 2] -> [b, 1]\n",
    "    target = target.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #exp和log互为反操作,这里是把alpha还原了\n",
    "    #这里的操作是在target上加上了动作的熵,alpha作为权重系数\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    target = target + alpha.exp() * entropy\n",
    "\n",
    "    #[b, 2]\n",
    "    target *= 0.98\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1557, grad_fn=<NegBackward0>),\n",
       " tensor([[0.6926],\n",
       "         [0.6931],\n",
       "         [0.6922],\n",
       "         [0.6924],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6929],\n",
       "         [0.6927],\n",
       "         [0.6878],\n",
       "         [0.6916],\n",
       "         [0.6927],\n",
       "         [0.6931],\n",
       "         [0.6916],\n",
       "         [0.6918],\n",
       "         [0.6823],\n",
       "         [0.6930],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6872],\n",
       "         [0.6930],\n",
       "         [0.6930],\n",
       "         [0.6919],\n",
       "         [0.6922],\n",
       "         [0.6930],\n",
       "         [0.6919],\n",
       "         [0.6923],\n",
       "         [0.6931],\n",
       "         [0.6902],\n",
       "         [0.6931],\n",
       "         [0.6918],\n",
       "         [0.6931],\n",
       "         [0.6719],\n",
       "         [0.6930],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6921],\n",
       "         [0.6931],\n",
       "         [0.6920],\n",
       "         [0.6930],\n",
       "         [0.6775],\n",
       "         [0.6905],\n",
       "         [0.6926],\n",
       "         [0.6929],\n",
       "         [0.6931],\n",
       "         [0.6931],\n",
       "         [0.6892],\n",
       "         [0.6930],\n",
       "         [0.6925],\n",
       "         [0.6929],\n",
       "         [0.6928],\n",
       "         [0.6931],\n",
       "         [0.6929],\n",
       "         [0.6923],\n",
       "         [0.6931],\n",
       "         [0.6766],\n",
       "         [0.6921],\n",
       "         [0.6923],\n",
       "         [0.6930],\n",
       "         [0.6931],\n",
       "         [0.6930],\n",
       "         [0.6888]], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算动作的概率\n",
    "    #[b, 4] -> [b, 2]\n",
    "    prob = model_action(state)\n",
    "\n",
    "    #计算动作的熵\n",
    "    #[b, 2]\n",
    "    entropy = prob * (prob + 1e-8).log()\n",
    "\n",
    "    #所有动作的熵求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #评估state的价值\n",
    "    #[b, 4] -> [b, 2]\n",
    "    value1 = model_value1(state)\n",
    "    value2 = model_value2(state)\n",
    "\n",
    "    #取价值小的,出于稳定性考虑\n",
    "    #[b, 2]\n",
    "    value = torch.min(value1, value2)\n",
    "\n",
    "    #按动作的概率对价值加权\n",
    "    #[b, 2] * [b, 2] -> [b, 2]\n",
    "    value *= prob\n",
    "\n",
    "    #所有动作的价值求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    value = value.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #这里的操作是在target上加上了动作的熵,这个值越大越好\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    loss_action = value + alpha.exp() * entropy\n",
    "\n",
    "    #因为是计算loss,所以对这个值符号取反\n",
    "    return -loss_action.mean(), entropy\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 417 0.0024493259843438864 25.5\n",
      "20 4510 2.781413058983162e-05 197.6\n",
      "40 9606 3.1722850053483853e-06 195.7\n",
      "60 10000 4.332260630235396e-07 199.8\n",
      "80 10000 6.155831044907245e-08 164.8\n",
      "100 10000 8.764728498533714e-09 94.3\n",
      "120 10000 1.3992318415034788e-09 200.0\n",
      "140 10000 3.429690975664812e-10 200.0\n",
      "160 10000 1.5074423653782532e-10 200.0\n",
      "180 10000 9.290643948611788e-11 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=1e-2)\n",
    "    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=1e-2)\n",
    "\n",
    "    #alpha也是要更新的参数,所以这里要定义优化器\n",
    "    optimizer_alpha = torch.optim.Adam([alpha], lr=1e-2)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(200):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            #计算两个value\n",
    "            value1 = model_value1(state).gather(dim=1, index=action)\n",
    "            value2 = model_value2(state).gather(dim=1, index=action)\n",
    "\n",
    "            #计算两个loss,两个value的目标都是要贴近target\n",
    "            loss_value1 = loss_fn(value1, target)\n",
    "            loss_value2 = loss_fn(value2, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value1.zero_grad()\n",
    "            loss_value1.backward()\n",
    "            optimizer_value1.step()\n",
    "\n",
    "            optimizer_value2.zero_grad()\n",
    "            loss_value2.backward()\n",
    "            optimizer_value2.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action, entropy = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #熵乘以alpha就是alpha的loss\n",
    "            #[b, 1] -> [1]\n",
    "            loss_alpha = (entropy + 1).detach() * alpha.exp()\n",
    "            loss_alpha = loss_alpha.mean()\n",
    "\n",
    "            #更新alpha值\n",
    "            optimizer_alpha.zero_grad()\n",
    "            loss_alpha.backward()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value1, model_value_next1)\n",
    "            soft_update(model_value2, model_value_next2)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), alpha.exp().item(), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

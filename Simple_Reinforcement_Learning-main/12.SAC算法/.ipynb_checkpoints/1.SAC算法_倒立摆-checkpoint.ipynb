{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n完整版和简化版的区别是有两个value模型\\n还有动态调整alpha\\n其他的和简化版一样'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "完整版和简化版的区别是有两个value模型\n",
    "还有动态调整alpha\n",
    "其他的和简化版一样\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4775],\n",
       "         [-0.5184]], grad_fn=<MulBackward0>),\n",
       " tensor([[0.8168],\n",
       "         [0.6553]], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ModelAction(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_state = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = torch.nn.Linear(128, 1)\n",
    "        self.fc_std = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        #[b, 3] -> [b, 128]\n",
    "        state = self.fc_state(state)\n",
    "\n",
    "        #[b, 128] -> [b, 1]\n",
    "        mu = self.fc_mu(state)\n",
    "\n",
    "        #[b, 128] -> [b, 1]\n",
    "        std = self.fc_std(state)\n",
    "\n",
    "        #根据mu和std定义b个正态分布\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        #采样b个样本\n",
    "        #这里用的是rsample,表示重采样,其实就是先从一个标准正态分布中采样,然后乘以标准差,加上均值\n",
    "        sample = dist.rsample()\n",
    "\n",
    "        #样本压缩到-1,1之间,求动作\n",
    "        action = torch.tanh(sample)\n",
    "\n",
    "        #求概率对数\n",
    "        log_prob = dist.log_prob(sample)\n",
    "\n",
    "        #这个值描述动作的熵\n",
    "        entropy = log_prob - (1 - action.tanh()**2 + 1e-7).log()\n",
    "        entropy = -entropy\n",
    "\n",
    "        return action * 2, entropy\n",
    "\n",
    "\n",
    "model_action = ModelAction()\n",
    "\n",
    "model_action(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1716],\n",
       "        [0.1247]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelValue(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #[b, 3+1] -> [b, 4]\n",
    "        state = torch.cat([state, action], dim=1)\n",
    "\n",
    "        #[b, 4] -> [b, 1]\n",
    "        return self.sequential(state)\n",
    "\n",
    "\n",
    "model_value1 = ModelValue()\n",
    "model_value2 = ModelValue()\n",
    "\n",
    "model_value_next1 = ModelValue()\n",
    "model_value_next2 = ModelValue()\n",
    "\n",
    "model_value_next1.load_state_dict(model_value1.state_dict())\n",
    "model_value_next2.load_state_dict(model_value2.state_dict())\n",
    "\n",
    "model_value1(torch.randn(2, 3), torch.randn(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6252444386482239"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 3)\n",
    "    action, _ = model_action(state)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "get_action([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " (array([-0.5734343 ,  0.81925154, -0.3065541 ], dtype=float32),\n",
       "  1.5948153734207153,\n",
       "  -4.770831076542803,\n",
       "  array([-0.59562784,  0.8032605 ,  0.54710686], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step([action])\n",
    "\n",
    "        #记录数据样本\n",
    "        datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 100000:\n",
    "        datas.pop(0)\n",
    "\n",
    "\n",
    "update_data()\n",
    "\n",
    "len(datas), datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9391,  0.3436, -1.7461],\n",
       "         [-0.9770, -0.2131,  2.4784],\n",
       "         [-0.8911, -0.4539, -2.5846],\n",
       "         [-0.6916, -0.7223,  1.3948],\n",
       "         [-0.9950, -0.0994,  4.1056]]),\n",
       " tensor([[1.2184],\n",
       "         [1.8985],\n",
       "         [1.1100],\n",
       "         [1.4616],\n",
       "         [0.6986]]),\n",
       " tensor([[ -8.0952],\n",
       "         [ -9.1841],\n",
       "         [ -7.8007],\n",
       "         [ -5.6463],\n",
       "         [-10.9398]]),\n",
       " tensor([[-0.9147,  0.4041, -1.3057],\n",
       "         [-0.9411, -0.3381,  2.6033],\n",
       "         [-0.9450, -0.3271, -2.7585],\n",
       "         [-0.6519, -0.7583,  1.0723],\n",
       "         [-0.9534, -0.3016,  4.1358]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 3]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1671.0303424055921"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step([action])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6052, requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#这也是一个可学习的参数\n",
    "alpha = torch.tensor(math.log(0.01))\n",
    "alpha.requires_grad = True\n",
    "\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #首先使用model_action计算动作和动作的熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(next_state)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    target1 = model_value_next1(next_state, action)\n",
    "    target2 = model_value_next2(next_state, action)\n",
    "\n",
    "    #取价值小的,这是出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    target = torch.min(target1, target2)\n",
    "\n",
    "    #exp和log互为反操作,这里是把alpha还原了\n",
    "    #这里的操作是在target上加上了动作的熵,alpha作为权重系数\n",
    "    #[b, 1] - [b, 1] -> [b, 1]\n",
    "    target += alpha.exp() * entropy\n",
    "\n",
    "    #[b, 1]\n",
    "    target *= 0.99\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0247, grad_fn=<MeanBackward0>),\n",
       " tensor([[0.4329],\n",
       "         [1.0428],\n",
       "         [0.7351],\n",
       "         [0.5723],\n",
       "         [0.7513],\n",
       "         [1.8897],\n",
       "         [1.0706],\n",
       "         [0.8916],\n",
       "         [0.5374],\n",
       "         [0.7748],\n",
       "         [1.0322],\n",
       "         [2.4203],\n",
       "         [1.3688],\n",
       "         [0.4537],\n",
       "         [0.7439],\n",
       "         [0.7293],\n",
       "         [1.4343],\n",
       "         [0.7655],\n",
       "         [0.7246],\n",
       "         [0.9576],\n",
       "         [0.3272],\n",
       "         [0.2929],\n",
       "         [0.6619],\n",
       "         [0.5069],\n",
       "         [1.3618],\n",
       "         [0.6433],\n",
       "         [0.2632],\n",
       "         [0.2513],\n",
       "         [0.7215],\n",
       "         [2.3594],\n",
       "         [0.9238],\n",
       "         [0.7462],\n",
       "         [0.6797],\n",
       "         [0.3886],\n",
       "         [1.1387],\n",
       "         [0.9908],\n",
       "         [0.9265],\n",
       "         [0.9182],\n",
       "         [0.6612],\n",
       "         [0.7070],\n",
       "         [1.2980],\n",
       "         [1.0725],\n",
       "         [0.7724],\n",
       "         [0.4195],\n",
       "         [1.0350],\n",
       "         [0.7553],\n",
       "         [3.2149],\n",
       "         [0.9208],\n",
       "         [0.9372],\n",
       "         [0.5459],\n",
       "         [1.6362],\n",
       "         [0.5472],\n",
       "         [0.9407],\n",
       "         [0.9058],\n",
       "         [0.3556],\n",
       "         [0.3882],\n",
       "         [0.2960],\n",
       "         [0.9076],\n",
       "         [0.4169],\n",
       "         [1.1163],\n",
       "         [0.8573],\n",
       "         [0.6273],\n",
       "         [0.4213],\n",
       "         [0.6652]], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算action和熵\n",
    "    #[b, 3] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(state)\n",
    "\n",
    "    #使用两个value网络评估action的价值\n",
    "    #[b, 3],[b, 1] -> [b, 1]\n",
    "    value1 = model_value1(state, action)\n",
    "    value2 = model_value2(state, action)\n",
    "\n",
    "    #取价值小的,出于稳定性考虑\n",
    "    #[b, 1]\n",
    "    value = torch.min(value1, value2)\n",
    "\n",
    "    #alpha还原后乘以熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "    #[1] - [b, 1] -> [b, 1]\n",
    "    loss_action = -alpha.exp() * entropy\n",
    "\n",
    "    #减去value,所以value越大越好,这样loss就会越小\n",
    "    loss_action -= value\n",
    "\n",
    "    return loss_action.mean(), entropy\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 0.00938995648175478 -1529.5596868328207\n",
      "10 2400 0.005891560111194849 -606.2095944839295\n",
      "20 4400 0.004272229969501495 -308.3045410765456\n",
      "30 6400 0.003395968349650502 -155.8718538878606\n",
      "40 8400 0.0030284016393125057 -197.55464418805403\n",
      "50 10400 0.003185431705787778 -161.84673734813708\n",
      "60 12400 0.0037035655695945024 -140.38025349464343\n",
      "70 14400 0.003973758779466152 -231.17426948605612\n",
      "80 16400 0.003422665176913142 -134.84076187807673\n",
      "90 18400 0.00224581197835505 -127.9872276885762\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=3e-4)\n",
    "    optimizer_value1 = torch.optim.Adam(model_value1.parameters(), lr=3e-3)\n",
    "    optimizer_value2 = torch.optim.Adam(model_value2.parameters(), lr=3e-3)\n",
    "\n",
    "    #alpha也是要更新的参数,所以这里要定义优化器\n",
    "    optimizer_alpha = torch.optim.Adam([alpha], lr=3e-4)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(100):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #对reward偏移,为了便于训练\n",
    "            reward = (reward + 8) / 8\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            #计算两个value\n",
    "            value1 = model_value1(state, action)\n",
    "            value2 = model_value2(state, action)\n",
    "\n",
    "            #计算两个loss,两个value的目标都是要贴近target\n",
    "            loss_value1 = loss_fn(value1, target)\n",
    "            loss_value2 = loss_fn(value2, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value1.zero_grad()\n",
    "            loss_value1.backward()\n",
    "            optimizer_value1.step()\n",
    "\n",
    "            optimizer_value2.zero_grad()\n",
    "            loss_value2.backward()\n",
    "            optimizer_value2.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action, entropy = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #熵乘以alpha就是alpha的loss\n",
    "            #[b, 1] -> [1]\n",
    "            loss_alpha = (entropy + 1).detach() * alpha.exp()\n",
    "            loss_alpha = loss_alpha.mean()\n",
    "\n",
    "            #更新alpha值\n",
    "            optimizer_alpha.zero_grad()\n",
    "            loss_alpha.backward()\n",
    "            optimizer_alpha.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value1, model_value_next1)\n",
    "            soft_update(model_value2, model_value_next2)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), alpha.exp().item(), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-356.6308145678895"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

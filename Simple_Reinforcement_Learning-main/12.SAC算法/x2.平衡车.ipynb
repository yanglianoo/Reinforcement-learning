{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3926, 0.6074],\n",
       "         [0.2995, 0.7005]], grad_fn=<SoftmaxBackward0>),)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_action = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    "    torch.nn.Softmax(dim=1),\n",
    ")\n",
    "\n",
    "model_action(torch.randn(2, 4)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1187, 0.1056],\n",
       "        [0.0394, 0.6344]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_value = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value_next = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 2),\n",
    ")\n",
    "\n",
    "model_value_next.load_state_dict(model_value.state_dict())\n",
    "\n",
    "model_value(torch.randn(2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "    prob = model_action(state)\n",
    "\n",
    "    #根据概率选择一个动作\n",
    "    action = random.choices(range(2), weights=prob[0].tolist(), k=1)[0]\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "get_action([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 0), 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-7.7739e-02,  1.8840e-01,  9.3669e-02, -1.0514e-01],\n",
       "         [ 8.5937e-02,  9.5666e-01, -1.8850e-01, -1.6122e+00],\n",
       "         [-1.4036e-03,  7.5256e-03,  4.1407e-02, -2.8605e-02],\n",
       "         [ 6.3814e-02,  4.1177e-01, -8.4156e-02, -8.0307e-01],\n",
       "         [-6.8117e-02, -4.0789e-01,  1.2939e-01,  1.0216e+00]]),\n",
       " tensor([[1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([[-7.3971e-02,  3.8206e-01,  9.1566e-02, -3.6686e-01],\n",
       "         [ 1.0507e-01,  7.6420e-01, -2.2075e-01, -1.3837e+00],\n",
       "         [-1.2531e-03, -1.8816e-01,  4.0835e-02,  2.7685e-01],\n",
       "         [ 7.2049e-02,  6.0794e-01, -1.0022e-01, -1.1210e+00],\n",
       "         [-7.6275e-02, -2.1470e-01,  1.4982e-01,  7.7213e-01]]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    action = torch.LongTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 4)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play:\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #计算动作的概率\n",
    "    #[b, 4] -> [b, 2]\n",
    "    prob = model_action(next_state)\n",
    "\n",
    "    #计算动作的熵\n",
    "    #[b, 2]\n",
    "    entropy = prob * torch.log(prob + 1e-8)\n",
    "\n",
    "    #所有动作的熵求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #计算next_state的价值\n",
    "    #[b, 4] -> [b, 2]\n",
    "    target = model_value_next(next_state)\n",
    "\n",
    "    #[b, 2] * [b, 2] -> [b, 2]\n",
    "    target = (prob * target)\n",
    "\n",
    "    #[b, 2] -> [b, 1]\n",
    "    target = target.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #这里的操作是在target上加上了动作的熵\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    target = target + 0.002 * entropy\n",
    "\n",
    "    #[b, 2]\n",
    "    target *= 0.98\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_value(state, action):\n",
    "#     #评估state的价值\n",
    "#     value = model_value(state)\n",
    "\n",
    "#     #取出对应动作的分数\n",
    "#     value = value.gather(1, action)\n",
    "\n",
    "#     return value\n",
    "\n",
    "\n",
    "# get_value(state, action).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0652, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算动作的概率\n",
    "    #[b, 4] -> [b, 2]\n",
    "    prob = model_action(state)\n",
    "\n",
    "    #计算动作的熵\n",
    "    #[b, 2]\n",
    "    entropy = prob * (prob + 1e-8).log()\n",
    "\n",
    "    #所有动作的熵求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    entropy = -entropy.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #评估state的价值\n",
    "    #[b, 4] -> [b, 2]\n",
    "    value = model_value(state)\n",
    "\n",
    "    #按动作的概率对价值加权\n",
    "    #[b, 2] * [b, 2] -> [b, 2]\n",
    "    value *= prob\n",
    "\n",
    "    #所有动作的价值求和\n",
    "    #[b, 2] -> [b, 1]\n",
    "    value = value.sum(dim=1, keepdim=True)\n",
    "\n",
    "    #这里的操作是在target上加上了动作的熵,这个值越大越好\n",
    "    #[b, 1] + [b, 1] -> [b, 1]\n",
    "    loss_action = value + 0.002 * entropy\n",
    "\n",
    "    #因为是计算loss,所以对这个值符号取反\n",
    "    return -loss_action.mean()\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 405 9.8\n",
      "20 4921 200.0\n",
      "40 8921 200.0\n",
      "60 10000 200.0\n",
      "80 10000 117.3\n",
      "100 10000 115.0\n",
      "120 10000 113.1\n",
      "140 10000 112.2\n",
      "160 10000 129.2\n",
      "180 10000 200.0\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=1e-3)\n",
    "    optimizer_value = torch.optim.Adam(model_value.parameters(), lr=1e-2)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(200):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            #计算value\n",
    "            value = get_value(state, action)\n",
    "\n",
    "            #计算loss,value的目标是要贴近target\n",
    "            loss_value = loss_fn(value, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value, model_value_next)\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

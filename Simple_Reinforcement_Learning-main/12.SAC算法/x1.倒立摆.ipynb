{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:26: UserWarning: \u001b[33mWARN: Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\u001b[0m\n",
      "  \"Gym minimally supports python 3.6 as the python foundation not longer supports the version, please update your version to 3.7+\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  \"The argument mode in render method is deprecated; \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('Pendulum-v1')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7027],\n",
       "         [-1.8450]], grad_fn=<MulBackward0>),\n",
       " tensor([[1.2818],\n",
       "         [1.3670]], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ModelAction(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_state = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = torch.nn.Linear(128, 1)\n",
    "        self.fc_std = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 1),\n",
    "            torch.nn.Softplus(),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        #[b, 3] -> [b, 128]\n",
    "        state = self.fc_state(state)\n",
    "\n",
    "        #[b, 128] -> [b, 1]\n",
    "        mu = self.fc_mu(state)\n",
    "\n",
    "        #[b, 128] -> [b, 1]\n",
    "        std = self.fc_std(state)\n",
    "\n",
    "        #根据mu和std定义b个正态分布\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "\n",
    "        #采样b个样本\n",
    "        #这里用的是rsample,表示重采样,其实就是先从一个标准正态分布中采样,然后乘以标准差,加上均值\n",
    "        sample = dist.rsample()\n",
    "\n",
    "        #样本压缩到-1,1之间,求动作\n",
    "        action = torch.tanh(sample)\n",
    "\n",
    "        #求概率对数\n",
    "        log_prob = dist.log_prob(sample)\n",
    "\n",
    "        #这个值描述动作的熵\n",
    "        entropy = log_prob - (1 - action.tanh()**2 + 1e-7).log()\n",
    "        entropy = -entropy\n",
    "\n",
    "        return action * 2, entropy\n",
    "\n",
    "\n",
    "model_action = ModelAction()\n",
    "\n",
    "model_action(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3400],\n",
       "        [0.1094]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelValue(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = torch.nn.Sequential(\n",
    "            torch.nn.Linear(4, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        #[b, 3+1] -> [b, 4]\n",
    "        state = torch.cat([state, action], dim=1)\n",
    "\n",
    "        #[b, 4] -> [b, 1]\n",
    "        return self.sequential(state)\n",
    "\n",
    "\n",
    "model_value = ModelValue()\n",
    "model_value_next = ModelValue()\n",
    "model_value_next.load_state_dict(model_value.state_dict())\n",
    "\n",
    "model_value(torch.randn(2, 3), torch.randn(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8171693086624146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_action(state):\n",
    "    state = torch.FloatTensor(state).reshape(1, 3)\n",
    "    action, _ = model_action(state)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "get_action([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " (array([-0.23095395, -0.9729647 ,  0.33660647], dtype=float32),\n",
       "  -1.3054171800613403,\n",
       "  -3.266925021254319,\n",
       "  array([-0.25950006, -0.96574306, -0.5889296 ], dtype=float32),\n",
       "  False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        next_state, reward, over, _ = env.step([action])\n",
    "\n",
    "        #记录数据样本\n",
    "        datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "        #更新游戏状态,开始下一个动作\n",
    "        state = next_state\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 100000:\n",
    "        datas.pop(0)\n",
    "\n",
    "\n",
    "update_data()\n",
    "\n",
    "len(datas), datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/cpu/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811701593/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0275, -0.9996, -8.0000],\n",
       "         [ 0.9718,  0.2358, -5.2443],\n",
       "         [-0.8741, -0.4858, -8.0000],\n",
       "         [-0.0412,  0.9992, -7.0880],\n",
       "         [ 0.9805,  0.1967, -2.9242]]),\n",
       " tensor([[-1.5577],\n",
       "         [-0.9106],\n",
       "         [-1.6672],\n",
       "         [-1.4562],\n",
       "         [-0.2474]]),\n",
       " tensor([[ -8.7841],\n",
       "         [ -2.8078],\n",
       "         [-13.3425],\n",
       "         [ -7.6247],\n",
       "         [ -0.8944]]),\n",
       " tensor([[-0.3639, -0.9314, -8.0000],\n",
       "         [ 0.9998, -0.0221, -5.2041],\n",
       "         [-0.9943, -0.1071, -8.0000],\n",
       "         [ 0.2827,  0.9592, -6.5570],\n",
       "         [ 0.9984,  0.0573, -2.8138]]),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 3]\n",
    "    state = torch.FloatTensor([i[0] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    action = torch.FloatTensor([i[1] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 1]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples]).reshape(-1, 1)\n",
    "    #[b, 3]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples]).reshape(-1, 3)\n",
    "    #[b, 1]\n",
    "    over = torch.LongTensor([i[4] for i in samples]).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "state[:5], action[:5], reward[:5], next_state[:5], over[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1377.5364123891325"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step([action])\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play and random.random() < 0.2:  #跳帧\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(model, model_next):\n",
    "    for param, param_next in zip(model.parameters(), model_next.parameters()):\n",
    "        #以一个小的比例更新\n",
    "        value = param_next.data * 0.995 + param.data * 0.005\n",
    "        param_next.data.copy_(value)\n",
    "\n",
    "\n",
    "soft_update(torch.nn.Linear(4, 64), torch.nn.Linear(4, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #首先使用model_action计算动作和熵\n",
    "    #[b, 4] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(next_state)\n",
    "\n",
    "    #评估next_state的价值\n",
    "    #[b, 4],[b, 1] -> [b, 1]\n",
    "    target = model_value_next(next_state, action)\n",
    "\n",
    "    #这里的操作是在target上加上了动作的熵\n",
    "    #[b, 1] - [b, 1] -> [b, 1]\n",
    "    target += 0.005 * entropy\n",
    "\n",
    "    #[b, 1]\n",
    "    target *= 0.99\n",
    "    target *= (1 - over)\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3104, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss_action(state):\n",
    "    #计算action和熵\n",
    "    #[b, 3] -> [b, 1],[b, 1]\n",
    "    action, entropy = model_action(state)\n",
    "\n",
    "    #使用value网络评估action的价值\n",
    "    #[b, 3],[b, 1] -> [b, 1]\n",
    "    value = model_value(state, action)\n",
    "\n",
    "    #熵,这个值期望的是越大越好,但是这里是计算loss,所以符号取反\n",
    "    #[1] - [b, 1] -> [b, 1]\n",
    "    loss_action = -0.005 * entropy\n",
    "\n",
    "    #减去value,所以value越大越好,这样loss就会越小\n",
    "    loss_action -= value\n",
    "\n",
    "    return loss_action.mean()\n",
    "\n",
    "\n",
    "get_loss_action(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 400 -1214.8860778558671\n",
      "10 2400 -1197.0920054200874\n",
      "20 4400 -1563.8853952024429\n",
      "30 6400 -1461.4888805964108\n",
      "40 8400 -566.090283126493\n",
      "50 10400 -758.8344351616859\n",
      "60 12400 -681.5523447064887\n",
      "70 14400 -513.7404362734312\n",
      "80 16400 -446.54963241564735\n",
      "90 18400 -304.40075456658604\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    optimizer_action = torch.optim.Adam(model_action.parameters(), lr=3e-4)\n",
    "    optimizer_value = torch.optim.Adam(model_value.parameters(), lr=3e-3)\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(100):\n",
    "        #更新N条数据\n",
    "        update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(200):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #对reward偏移,为了便于训练\n",
    "            reward = (reward + 8) / 8\n",
    "\n",
    "            #计算target,这个target里已经考虑了动作的熵\n",
    "            #[b, 1]\n",
    "            target = get_target(reward, next_state, over)\n",
    "            target = target.detach()\n",
    "\n",
    "            #计算value\n",
    "            value = model_value(state, action)\n",
    "\n",
    "            #计算loss,value的目标是要贴近target\n",
    "            loss_value = loss_fn(value, target)\n",
    "\n",
    "            #更新参数\n",
    "            optimizer_value.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "\n",
    "            #使用model_value计算model_action的loss\n",
    "            loss_action = get_loss_action(state)\n",
    "            optimizer_action.zero_grad()\n",
    "            loss_action.backward()\n",
    "            optimizer_action.step()\n",
    "\n",
    "            #增量更新next模型\n",
    "            soft_update(model_value, model_value_next)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(10)]) / 10\n",
    "            print(epoch, len(datas), test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-116.62285223388518"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
